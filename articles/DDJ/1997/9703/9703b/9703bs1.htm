<html>
<META NAME="year" CONTENT="1997">
<head>
<title>Dr. Dobb's Journal March 1997: Genetic Algorithms</title>
</head>

<body bgcolor="FFFFFF">
<!--Copyright &#169; Dr. Dobb's Journal-->
<p><i>Dr. Dobb's Journal</i> March 1997 </p>

<h1>Genetic Algorithms</h1>
</h1>
<HR>
<p>The field of genetic algorithms (GAs) addresses the same set of problems as reinforcement learning, but with a different methodology. Instead of learning a utility function, GAs learn the policy directly, representing it as an encoded bit string (or, in genetic programming, as a program in some computer language). The approach is to search the space of policies by generating a pool of policies and evaluating each one for a sequence of time steps in a simulated environment. A new pool of policies is constructed by combining the more successful existing policies with some random change. The algorithm iterates until a sufficiently good policy is found.</p>

<p>The name "genetic algorithms" comes from an analogy with biology: The bits representing a policy are like the bases in a strand of DNA, the creation of new policies is like the process of sexual reproduction, the random change is like mutation, and the propagation of successful policies is like natural selection.</p>

<p>The GA approach shares with RL the twin benefits of being based on training (not programming) and being responsive to changes in the environment through the possibility of online retraining. However, there are reasons to believe that RL makes better use of available computing resources. Consider, for example, a policy <i>A</i> that is in the pool of policies being evaluated by GAs. Further, assume that the long-term cumulative reward from policy <i>A</i> is very poor. Then GAs would simply discard policy <i>A</i> in favor of better policies in its pool without learning anything from the simulation of <i>A.</i></p>

<p>RL, on the other hand, has the potential of learning a lot from following policy <i>A.</i> Suppose policy <i>A</i> assigns action <i>a</i> to some state <i>s</i> and whenever action <i>a </i>is executed in state <i>s,</i> the resulting state always has high utility. RL will learn that action <i>a</i> is good in state <i>s</i> from executing policy <i>A,</i> even though the policy itself is bad. It can do this because it learns a state-based utility function. GAs, however, ignore the state-based structure of the problem and therefore waste information.</p>

<p>In summary, the GA approach needs a partial success in terms of total reward before it can make use of it, while RL only needs a partial success in terms of reaching a high-utility intermediate state, which is easier to achieve.</p>

<p>It is not clear exactly when GAs are the preferred approach over RL, but in general, RL seems to do better as the problem becomes more complex. GAs may do well when there is a constrained, small space of policies to be searched.</p>

<p> -- S.S., P.N., D.C.</p>
<P>
<a href="9703b.htm#rs1">Back to Article</A>
</P>
<HR><I>Copyright &copy; 1997, Dr. Dobb's Journal</I><BR>

</BODY>
</html>
