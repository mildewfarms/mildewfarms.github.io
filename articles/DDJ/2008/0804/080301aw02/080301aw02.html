<html>
<head>
<title>Optimizing Math-Intensive Applications with  Fixed-Point Arithmetic</title>
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/Layout.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/FontStyles.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/newarticle.css" type="text/css">
<script src="../../../../forms/popwindow.js"></script>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->
<p><i>Dr. Dobb's Journal</i> April 2008</p>

<h1>Optimizing Math-Intensive Applications with  Fixed-Point Arithmetic</h1>
<h2>Understanding and using fixed-point math can result in real performance gains</h2>


<h3>By Anthony Williams </h3>


<I>Anthony is founder and managing director of Just Software Solutions. He is also maintainer of the Boost thread library and a member of the BSI C++ Standards Panel. He can be contacted at <a href="mailto:anthony@justsoftwaresolutions.co.uk">anthony@justsoftwaresolutions.co.uk</a>.</I>

<hr>




<p>Recently, I was working on an application that helps helicopter maintenance engineers adjust their helicopters to fly better. This application analyzes data collected during flight, and provides engineers with information to help them adjust the helicopter. This is a very computationally expensive operation, with extensive use of trigonometric and exponential functions, so though the initial prototypes ran very fast on a PC, they were very slow on the target platform&#151;a handheld device running Windows CE on an ARM processor without hardware floating-point support. After several rounds of optimization focused on improving the algorithm, reducing the use of expensive operations, adjusting the memory-allocation patterns to avoid allocating, freeing memory in a loop, and so forth, the application was running considerably faster than the initial prototype. Unfortunately, it was still too slow, so it was time to pull out the "big guns" and use fixed-point math. This final push doubled the speed of the application, finally bringing the performance within acceptable limits. But it wasn't easy. In this article, I explain the fixed-point techniques we used to achieve this performance gain.</p>


<h3>Fixed-Point Math</h3>

<p>I had put off using fixed-point math because the application made extensive use of sines, cosines, and exponential functions that are nontrivial to implement. However, profiling (see sidebar) showed that the mathematical operations were consuming a very high percentage of CPU time, and I had exhausted all other avenues. This was relatively unsurprising, given that the target CPU didn't support floating-point operations in hardware, so all mathematical operations were performed by an emulation library.</p>

<p>Before proceeding, it was important to check the range of possible values&#151;the range for a <i>double</i> is far larger than can be represented in a simple fixed-point type. For this application, 64-bit fixed-point values (35 bits to the left of the point, 28 bits after, and one sign bit) were sufficient as a replacement.</p>

<p>Initially, I hunted the Internet for a free, open-source implementation of fixed-point math. The only implementation I found that provided all the required mathematical operations did speed up the application, as simple operations such as addition and subtraction were considerably improved. Unfortunately, the overall gains weren't as good as had been hoped for. However, seeing the na&iuml;ve implementations of the more complex functions (calculating sines and cosines from the Taylor series, for example) gave me hope&#151;by improving the implementation of these functions, there was potential for much greater improvements, so I pressed on with renewed vigor. Eventually, the whole fixed-point library was rewritten from scratch, but the final performance gain was enough to justify the time spent in doing so, and it was easier than anticipated. (The complete source code for the fixed-point library and profiling code is available online at <a href="http://www.ddj.com/code/">www.ddj.com/code/</a>.)</p>





<h3>Don't Forget the Constants!</h3>

<p>Having changed over to fixed point, the next easy gain came from replacing floating-point and integer constants involved in fixed-point calculations with fixed-point ones. It's easy just to use M_PI or 0 or 1 or even 0.5 in a calculation, and this is not so easy to spot when changing every use of "double" to "fixed." The profiler showed that these constants were responsible for quite a few calls to the converting constructor, so replacing them with fixed-point constants shaved another couple of percents off the runtime.</p>







<h3>CORDIC</h3>

<p>The single biggest gain was achieved by optimizing the trigonometric and exponential functions. For the trig functions, the CORDIC method was used. (For more information on CORDIC, which is short for "COordinate Rotation DIgital Computer," see "Implementing CORDIC Algorithms," by Pitt Jarvis; <a href="http://www.ddj.com/architect/184408428">www.ddj.com/architect/184408428</a>.) CORDIC gets its name because it uses properties of rotations on a plane to calculate sines and cosines. A vector can be rotated counterclockwise by an angle q using a rotation matrix.</p>

<img src="080301aw02_q1.jpg" width="138" height="70" border="0">

<p>Equivalently, if you have two angles <img src="alpha14.gif"> and <img src="beta14.gif"> such that <img src="theta14.gif" width="5" height="10" border="0"><i>=<img src="alpha14.gif">+<img src="beta14.gif"></i>, then we can first rotate by <img src="alpha14.gif">, and then by <img src="beta14.gif">, to achieve the same effect.</p>

<img src="080301aw02_q2.jpg" width="396" height="63" border="0">


<p>The rotation by <img src="beta14.gif"> can then itself be split into further rotations by smaller and smaller angles. The benefit here comes from the fact that you can then pick a set of angles <img src="alpha14.gif"><sub>i</sub> in advance, and precompute the appropriate sines and cosines. Rather than a time-consuming and potentially inaccurate power-series calculation, the desired sines and cosines can instead be calculated by a short series of multiplications and additions. Rather than using both sines and cosines of the angles <img src="alpha14.gif"><sub>i</sub>, the cosines can be factored out:</p>

<img src="080301aw02_q3.jpg" width="429" height="73" border="0">

<p>As written, this is fine for some fixed angle <i>theta</i>, but what about a general angle? It can be shown that provided that:</p>

<p><i><img src="alpha14.gif"><sub>i</sub></i> &lt; <i><img src="alpha14.gif"><sub>i-1</sub></i></p>

<p> and </p>



<p><i><img src="alpha14.gif"><sub>i</sub></i> &gt;= 0.5&#8226;<i><img src="alpha14.gif"><sub>i-1</sub></i></p>

<p>then any angle <img src="theta14.gif" width="5" height="10" border="0"> can be made by adding or subtracting each <img src="alpha14.gif"><sub>i</sub> exactly once up to a precision determined by the number of angles <i>n</i>. Since <i>cos(x)=cos(-x)</i> and <i>tan(x)=-tan(-x)</i>, the factored-out product of cosines can be stored as a constant multiplier once the angles have been determined, and the appropriate signs used for the tangents depending on the actual angle <img src="theta14.gif" width="5" height="10" border="0">.</p>

<p>As a final simplifying step, if you choose the angles <img src="alpha14.gif"><sub>i</sub> such that <i>tan(<img src="alpha14.gif"></i><sub>i<i></sub>)=2<sup>-j</i></sup><sup> </sup>for some integer <i>j,</i> then this greatly simplifies the multiplication, as it is now just a simple shift.</p>

<p>We only need to handle angles in the range <i>-<img src="pi14.gif">/2</i> to <img src="pi14.gif"><i>/2</i>, since the sines and cosines of larger angles only vary in sign (if at all), and restricting it to this range limits the number of iterations required.</p>

<p>The sine and cosine of an angle can thus be calculated simply by rotating the unit vector from the <i>x</i> axis by the required angle as described: The cosine and sine are the <i>x</i> and <i>y</i> coordinates of the rotated vector.</p>


<h3>Calculating arctan</h3>

<p>The inverse tangent can easily be calculated with the CORDIC rotation tables. This time, rather than rotating the unit vector, a vector with an <i>x</i> component of 1 and a <i>y</i> component of the value for which we want the arctan is rotated until it has a <i>y</i> value of 0.</p>











<h3>Shift and Add for Exponentials</h3>

<p>Having found out about using shifts and adds with CORDIC to implement sines and cosines, I wondered if the same could be done for exponentials and logarithms, too. For exponentials it's rather easy, as:</p>

<p><i>e<sup>x+y</sup></i><i>=e</i><sup>x</sup>&#8226;<i>e</i><i><sup>y</sup></i><i></i></p>

<p>So by choosing values of <i>x</i> such that <i>e<sup>x</sup>= 2<sup>n</i></sup> or <i>e<sup>x</sup>=1+2<sup>-n</i></sup> for some <i>n</i>, then exponentials can again be calculated just as a matter of shifts and adds. In each iteration, the current value of <i>x</i> is compared to the suggested value of <i>y</i>. If <i>x</i> is less than <i>y</i>, then that term is ignored, otherwise <i>x=x-y</i> and the result is multiplied by the appropriate value with a shift and add. By running from high values of <i>y</i> to smaller ones, the value of <i>x</i> approaches zero, and the result approaches the correct value for the exponential. For 64-bit fixed point (with 63 significant bits), this requires 63 table entries.</p>

<p>For negative values of <i>x</i>, the principle is the same, but the table values are for <i>e<sup>x</sup>=2<sup>-n</i></sup> or <i>e<sup>x</sup> =1/(1-2<sup>-n</sup>)</i>. Here we can use the fact that <i>log(2<sup>-n</sup>)=-log(2<sup>n</sup>)</i> to avoid duplicate table entries.</p>


<h3>Logarithms</h3>

<p>Logarithms are slightly more complex to get right, though conceptually just as easy, as they are just a reverse lookup using the same tables as for calculating the exponentials. In fact, the implementation is even easier because logarithms are always positive. The key here is to find where to start the calculation, and you do that by shifting the parameter left until there is a 1 in the most significant bit. By counting the shifts, you know what initial value to use for the result. For example, in the 35.28 fixed-point system being used here, if you had to shift-left 30 times, the original value was <i>2</i><sup>5<i></sup>*y</i> for some <i>y</i>, so you start with a result of <i>log(2</i><sup>5<i></sup>)</i> and work from there. Similarly, if you had to shift-left 42 times, then the original value was <i>2<sup>-7</sup>*y</i>, so you start with a result of <i>log(2<sup>-7</sup>)</i>.</p>

<p>Once you have the starting result for <i>log(x)</i>, it's smooth sailing, as you always have a value with the MSB set to 1. For increasing values of <i>n</i>, check to see if the current value of <i>x&gt;=1+(x*2<sup>-n</sup>)</i>. If it is, then subtract <i>(x*2<sup>-n</sup>)</i> from the current value of <i>x</i>, and add <i>log(1/(1-2<sup>-n</sup>))</i> to the result. Repeat until <i>x==1</i> or until you're out of bits.</p>

<p>This uses the identity that:</p>

<p><i>log(x*y)=log(x)+log(y)</i></p>

<p>so</p>

<img src="080301aw02_q4.jpg" width="236" height="53" border="0">
<p>so </p>

<img src="080301aw02_q5.jpg" width="529" height="49" border="0">

<p>We've already got the values of <i>log(1/(1-2<sup>-n</sup>))</i> in a lookup table, so you can reuse them here. It's important to go for increasing values of <i>n</i>, so that the large factors are taken out first.</p>












<h3>Square Roots</h3>

<p>With the shift-and-add implementations of sines, cosines, exponentials, and logarithms, the runtime was much improved. However, another function was now taking a lot of the time&#151;the square-root function. This had been na&iuml;vely implemented using a Newton-Raphson approximation, and was taking a very long time to converge in some cases. What was needed was a systematic method of calculating a square root that had a bounded iteration count.</p>

<p>Yet again, you can use powers of two to your benefit. If <i>y=<img src="sqrt.gif" width="13" height="15" border="0">x</i> then <i>y</i><sup>2<i></sup>=x</i>. If you break <i>y</i> down by extracting the highest power of 2, so <i>y=2<sup>n</sup>+r</i>, then <i>y</i><sup>2<i></sup>=2<sup>2n</sup>+r*2<sup>(n+1)</sup>+r</i><sup>2</sup>. Consequently, you can find <i>n</i> from the highest even power of two in <i>x</i>, then it's just a matter of finding <i>r</i>, which you can do by trial and error, hunting for each bit in turn.</p>

<p>If you start with <i>a=2<sup>n</i></sup>, and <i>z=x-a</i><sup>2</sup>, then you can iterate through the remaining powers of two less than <i>n</i> to find the result. In each iteration you try <i>y=a+2<sup>m</sup>+r1</i>, for decreasing powers of <i>m</i>. Now, you know that <i>y</i><sup>2<i></sup>=a</i><sup>2<i></sup>+2*a*2<sup>m</sup>+2<sup>2m</sup>+r2</i>, and the ideal is that <i>r1</i> (and thus <i>r2</i>) is zero, so <i>y=<img src="sqrt.gif" width="13" height="15" border="0">x</i>. However, in each iteration you've got <i>z=x-a</i><sup>2</sup>, so you need to compare <i>z</i> against <i>2*a*2<sup>m</sup>+2<sup>2m</i></sup>. Since we're going in decreasing values of <i>m</i>, if <i>z&gt;=2*a*2<sup>m</sup>+2<sup>2m</i></sup>, then you know that <i>2<sup>m</i></sup> is a component of <i>y</i>, since <i>r2</i> will always be positive. Because a given power of 2 can only be present once, you can now move to the next smaller value of <i>m</i>, updating the new value of <i>a</i> and <i>z</i>. Since you've got a fixed-point value, once you run out of bits you're done, so there is a hard limit on the number of iterations.</p>


<h3>Multiply and Divide</h3>

<p>After investing the effort in optimizing the complex operations, plain old multiply and divide were now top of the heap. These functions are more than just simple integer operations, because they need to take care of the fixed-point offset. Not only that, but the target platform is a 32-bit platform, so the compiler-supplied 64-bit integer operations are multiple instructions anyway, thus it is important to ensure that every instruction does something useful. For multiplication, this means splitting the top 32 bits and the bottom 32 bits&#151;there's no point doing unnecessary 32-bit times 64-bit multiplications just for completeness. Doing the split like this also lets the appropriate bit shifts be incorporated directly without having to worry about loss of precision.</p>

<p>Division is more complicated, as to get the final answer correct to the available precision, you need more than 64 bits in some cases. To circumvent this problem, when the numerator is more than the denominator, then the denominator is scaled up until it is at least as big as half the numerator. This then makes for division of values of similar size, so the shift-and-add method used for the actual calculation is most effective, and doesn't lose any precision&#151;basically, each bit is calculated in turn, starting with the most significant bit of the result.</p>


<h3>Conclusion</h3>

<p>Using fixed-point math gave a considerable performance boost to this application, and I anticipate that there are many other applications that could benefit from the techniques described here. It is important to check that the required range of values for an application can comfortably be represented within the chosen fixed-point representation.</p>




<table height="0" border="0" cellpadding="10" cellspacing="5" vspace="15">
  <tr> 
    <td valign="top" bgcolor="CCCC99">
    <p><b>Measure, Measure, Measure</b></p> 

<p>When trying to optimize code, it's important to profile both before and after an optimization. Unfortunately, on the embedded target there was no profiler available, and the profile on the PC was radically different due to the different CPU. This left only one sensible option&#151;write a profiler. Actually writing a full-blown profiler that can integrate with the compiler and get full line-by-line performance data is a huge task, and well outside the scope of this project, so I opted for the next best thing&#151;a code-based profiler. Because  the application is written in C++, I could make use of the magic of macros and deterministic destruction to get timing data from every block of code. It works really simply: Replace the opening brace of every block of code to be tested with "{INSTRUMENT_BLOCK;". In traced builds, INSTRUMENT_BLOCK expands to an instance of a class which records profiling data. In nontraced builds, it expands to an empty string. This then allows the recording of call-count and execution time for every block of code in the application, which can then be dumped to a file on exit. Profiling of individual blocks can be adjusted by adding and removing the INSTRUMENT_BLOCK from the start; individual lines can be profiled by enclosing them in their own block, and adding the INSTRUMENT_BLOCK invocation to that.</p>

<p>     Though the profiling does make the application run slower (about 10 times slower in fact, which made test runs VERY tedious, often taking over half an hour per run), the results can generally by adjusted to account for the overhead of the profiling, and it does highlight the parts of the application that are most in need of optimization&#151;the functions which consume the most total runtime. Some of these are functions that are run millions of times, and others are run a few times but take a long time for each execution.</p>

<p>&#151;A.W.</p>
	</td>
  </tr>
</table>















</body>
</html>