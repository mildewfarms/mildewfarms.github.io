<HTML>
<META NAME="year" CONTENT="1991">
<HEAD>
<TITLE>JUL91: LETTERS</TITLE></HEAD>
<BODY BGCOLOR="#ffffff">
<h1>LETTERS<a name="017f_0001"></h1><P>
<h3></h3><P>
<h3><a name="017f_0002">Absolute vs. Relative Performance<a name="017f_0002"></h3><P>
Dear DDJ,<P>
I'd like to thank Ken Kashmarek (&quot;Letters,&quot; DDJ, February 1991) for taking the time to write concerning my article, &quot;Optimal Determination of Object Extents&quot; (DDJ, October 1990).  I'm glad to hear he enjoyed it.  I completely agree with Ken about the column order reversal. Speedup, however, is defined as a positive quantity; otherwise the &quot;up&quot; portion of the word wouldn't make much sense.  When I wrote the article, I thought computing percentages would be obvious, so I didn't spell out the details.  Anyway, what's really important is speed improvement.  Anybody can come up with a slower algorithm, its the faster ones that are difficult.<P>
For the most part, Ken's comments detailed &quot;absolute performance.&quot;  My article, however, was about &quot;relative performance&quot;--that is, one algorithm relative to another.  Whatever hardware, compiler, switches, or floating-point element size you use, as long as you are consistent with comparisons the results will always be about the same (the MIN/MAX algorithm will be faster, since it performs less work to reach the conclusion, as long as you must perform comparisons of items to find min and max!).<P>
Also notice that the data sample used makes a difference.  For consistency, I used the same data sample over all machines.  All times were the user CPU time since that's how long the computer took to execute the program.  I believe I used an array of 1000 items and ran a loop finding min and max of these items 5000 times (this varied from one machine to another to get accurate time value).  For timing, I used a program I wrote that's similar to the Unix time command.<P>
As for Ken's comments about the Cray, remember that when you spend over $10 million on a computer, you get one of the best parallelizing compilers available (otherwise the hardware is kind of hard to use).  This compiler completely unfolds the loops and runs the code straight. But even the Cray vector processor can't avoid doing comparisons; therefore it, too, will show relative performance improvement.  Yes, the Cray does matrix math faster, but that's because it can do an addition and a multiplication in one cycle (just like Ken's IBM 6000), and because it runs on a 250-MHz clock (vs. the 25-MHz IBM 6000).<P>
As for Ken's other comments:<P>
<OL>
<li>Math coprocessors.  Most workstations come with math coprocessors;.only cost-sensitive PCs make FPUs optional anymore--for the time being.  PCs based on the 68040 and 80486 (and others) will have built-in math coprocessors.  Whether or not you have a FPU makes a difference in absolute measurement--but not in relative timing.</li>
<li>Compilers.  Compilers often have a mode to output assembly code so that you can see exactly what's going on and how well the compiler is optimizing.</li>
<li>Array size.  The science of algorithms splits algorithms into two groups: one in which all code and data fit into main memory and another where they don't.  These two groups are compared with other algorithms within the group; otherwise they don't always lose.  But for relative purposes, it still doesn't matter.  Thanks to Ken for the data on the 3090 and Apple IIGS--and for caring enough to write.  His input is appreciated.  For more information on these topics, interested readers might refer to &quot;Vector Pipelining, Chaining, and Speed on the IBM 3090 and Cray X-MP&quot; (IEEE Computer, September 1989).  It's a really good, down-and-dirty comparison of the two machines and their vector capabilities.</li>
</OL>
Victor Duvanenko<P>
Indianapolis, Indiana<P>
<h3><a name="017f_0003">Power/MS- Basic Dispute Continues<a name="017f_0003"></h3><P>
Dear DDJ,<P>
This is a reply to Bruce Tonkin's response (DDJ, March 1991) to my original letter concerning his review of Power Basic:<P>
Where Bruce uses numbers in his reply, the numbers are essentially correct, but his conclusions about the numbers are not.  The main problem seems to be that Bruce assumes: 1. That programmers will be willing to convert their (possibly) large volume of source code to another format and syntax just to gain some small advantage, when it is not known how long that particular version of the language will be supported; where MS-DOS Basic will likely dominate on PCs for years to come, and 2. that other programmers will mostly use the same (inferior) methods of coding that Bruce describes in his reply.  To compare a few reply points:<P>
<OL>
<li><P>a. Basic 7.x allows multiple 64-KByte string heaps.</P>
    <P>b. It is much faster to sort in a pre-allocated string (memory) buffer and not in string arrays.</P>
    <P>c. A few large string buffers can be &quot;concatenated&quot; using simple algorithms to simulate a very large memory block, much as one would do in C.</P></LI>
<LI>The argument about fixed-length strings in MS-Basic is essentially invalid, since the larger string buffers need only be written to and searched using dynamic pointers, and not actually shuffled around.  Does Bruce think that the two times slower speed of fixed-length strings vs. dynamic strings is significant for MS-Basic, while the 3.4 times difference in Basics is not?</LI>
<LI>You'll have to write your own sort?  Well, I'll give anyone who wants it a super fast merge/insertion sort which uses a large string as a memory block and avoids all of the problems using string arrays.  The sort is generic and is in a callable subprogram, and it will return all of the memory it uses when it is finished running.</LI>
<LI>At my present job, we have 200 Basic programs comprising 700,000 lines of code, and it has not been my experience that Basic slows down substantially when it approaches maximum memory usage.  I would suggest to programmers who are looking for good performance as well as some measure of portability to other modern languages, that they learn simple memory management and avoid the anything-goes coding style which plagues many Basic programs today, and which causes much embarrassment when the code is compared to C.</LI>
</OL>
In conclusion, I believe most programmers will want to know how to get good performance from whatever language they are now using or are committed to, and not be led to believe that the only solution is to switch compilers and have to convert a lot of code.  I'm convinced that MS-Basic can run right up there with Power Basic, no matter what the application, and when people who pay programmers are no longer convinced of same, they will most likely jump to C rather than switch to another Basic compiler.<P>
Dale Thorn<P>
Cleveland, Tennessee<P>
<h3><a name="017f_0004">Blipless Curves<a name="017f_0004"></h3><P>
Dear DDJ,<P>
In the October 1990 issue of DDJ, I read a letter by Michael P. Lindner.  It was titled &quot;Still Going in Circles&quot; and was feedback on Tim Paterson's article, &quot;Circles and the Differential Analyzer&quot; (July 1990).  I was impressed by Paterson's article.  There are a few suggestions that I would like to make.<P>
I implemented the algorithm as it was on DOS, Lindner's Example 3.  It is duplicated here as my <a href="#017f_0005">Example 1</A>.<P>
<h4><a name="017f_0005">Example 1: plot8 is the function which drows the point in the eight octants</h4><P>
<pre>
  x = e = 0
  y = r
  while (x &lt;= y)
  {
    plot8 (x,y);
    e =+ (x&lt;&lt;1)+1;
    if (e &gt; 0)
    {
        e -= (y&lt;&lt;1) -1;
        y --;
    }
    x ++;
  }
</pre><P>
<P>
The resulting curve had a blip, as shown in my <a href="19910181.htm">Figure 1</A>, which is extremely noticeable on large circles.<P>
The reason for that is that if the curve passes through a pixel, the &quot;e&quot; for that point is positive and so the pixel below it is chosen.  What results is the plotting of pixels just inside the circle, not on it.  The first pixel is the start and is on the circle -- hence the blip.<P>
In this algorithm the &quot;e&quot; for all the points would be zero or negative.  A better algorithm would make sure that &quot;e&quot; is kept around zero.  A measure of this &quot;closeness&quot; could be the sum of &quot;e&quot; for all the points.  I'll call it E.  My <a href="#017f_0006">Table 1</A> shows E for the fragment in <a href="#017f_0005">Example 1</A> for various radii.<P>
<h4><a name="017f_0006">Table 1</h4><P>
<pre>
  Radius   E
  150      -14705
  175      -19942
  200      -27301
</pre><P>
<P>
I was trying to write a small fast version in assembler, so adding more variables or complicating any expression was out of the question.  By changing the condition in the if statement, a better result can be obtained.  I tried two other simple conditions (e&gt;x) and (e&gt;y).  The corresponding E<SUP>1</SUP> and E<SUP>2</SUP> are in <a href="#017f_0007">Table 2</A> for the same radii.<P>
<h4><a name="017f_0007">Table 2</h4><P>
<pre>
  Radius   E<SUB>1</SUB>     E<SUB>2</SUB>
  150      -10998   689
  175      -14895   -558
  200      -17244   1090
</pre><P>
<P>
It's obvious from <a href="#017f_0007">Table 2</A> that E<SUP>2</SUP> is the best bet, i.e., if (e&gt;y).  /*line7*/, where E is around zero.<P>
I found out E for radii from 1 to 1000 using this new condition.  E was positive for 724 of the cases and negative for the remaining 276 cases.  This also eliminates the blip.  The resulting circle is closer to the true circle and smooth and continuous.  Just for statistics, I'd like to mention that in the assembly language version I had kept &quot;e,&quot; &quot;x,&quot; and &quot;y&quot; in registers.  Making this change meant a reduction of the code size by 1 byte.  The resulting code would be just a wee bit faster, as the immediate operand (0 in this case) need not be fetched.<P>
V. Venkataraman<P>
Pune, India<P>
<h3><a name="017f_0008">The Return of Big-O<a name="017f_0008"></h3><P>
Dear DDJ,<P>
As many others probably already have, I regret to inform you of Edward Allburn's misconclusions in his article entitled &quot;Graph Decomposition: Imposing order on chaos,&quot; in your January 1991 issue.  Mr. Allburn claims -- because it is apparent -- that his algorithm is in 0(n<SUP>2</SUP>) in worst case.  Without going into details about graph theory, given a graph G with n vertices, the most number of edges e is n(n- 1)/2.  This is common knowledge in graph theory. A graph that satisfies this property is called a complete graph.  Understanding this yields to show that the algorithm which Mr. Allburn claims as 0(n<SUP>2</SUP>) is actually 0(n<SUP>3</SUP>).  A complete graph is the worst possible case for Mr. Allburn's algorithm.  Given a complete graph g with n vertices and e edges given to the algorithm in the order (1,2), (1,3), ... (1,n), (2,3), ... (n - 1, n) the algorithm must loop through step #1 in Mr. Allburn's algorithm (n-1)n/2 times. This step alone is in 0(n<SUP>2</SUP>).  Now the loop in step #4.d.1 (see Allburn's Example 2) adds another degree of complexity, making the algorithm run in 0(n<SUP>3</SUP>).<P>
I tested the algorithm to see hard results.  I implemented the algorithm in Turbo C++ (see Listing One, page 14) and timed its execution, providing complete graphs from N = 4 to 1000 step 4.  I then took the time T(N) it took to run the program on N vertices and found its ratio to N<SUP>2</SUP>, N<SUP>3</SUP>, and N<SUP>4</SUP> and plotted them on a graph (see <a href="19910182.htm">Figure 2</A>).  This graph shows how the algorithm grows with respect to the tree growth functions N<SUP>2</SUP>, N<SUP>3</SUP>, and N<SUP>4</SUP>.  From the graph it can be seen that the graph of the ratio of N<SUP>2</SUP> to T(N) is decreasing.  This says that T(N) is growing faster than N<SUP>2</SUP>.  Likewise, looking at the ratio N<SUP>4</SUP> to T(N) the curve is increasing, saying that T(N) does not grow as fast as N<SUP>4</SUP>.  Finally, the graph of the ratio of N<SUP>3</SUP> to T(N) is nearly constant (slightly increasing).  This says that T(N) is growing as fast, but not faster than N<SUP>3</SUP>, which is the definition of Big-Oh.<P>
There are many ways in which general graph theory algorithms can be tailored to individual problems to speed up algorithm execution.  Knowing more information than the general algorithm requires is always an asset.  I did enjoy reading Mr. Allburn's article and liked the development of the algorithm using a new data structure.  But I also wonder if Mr. Allburn really tested his algorithm thoroughly before making his claim and if his understanding of Big-Oh notation is such to make that claim.<P>
Martin Schlapfer<P>
Scotts Valley, California<P>
Edward responds:<P>
In his test, Mr. Schlapfer used a &quot;complete&quot; graph.  For a directed graph that translates to N* (N- 1) edges, and 1/2 that number for undirected graphs (where N = Number of Vertices). The graph I used in my article was a &quot;sparse&quot; graph which contained the minimum number of edges needed to specify that all of the vertices are connected.  This works out to be N- 1 edges for an undirected graph.<P>
In Big-O notation, &quot;N&quot; is intended to represent the amount of input into the algorithm.  For a graph, the units of input are traditionally either edges or vertices.  When the number of edges is about the same as the number of vertices (as was the case in my example graph), then &quot;N&quot; can safely represent either edges or vertices.  When faced with &quot;complete&quot; graphs, however, then number of edges is an order of magnitude greater than the number of vertices. Because of this &quot;N&quot; must now represent the number of edges.<P>
After reviewing my test results, I realized that this should be the case even when the number of edges is less than the number of vertices.  For example, in the case where a graph is be comprised of 1 million vertices and only a handful of edges, the GAD algorithm is finished as soon as the last edge has been read .  Because GAD's performance is tied to the number of edges rather than vertices, &quot;N&quot; should represent edges in the analysis of this algorithm performance.  Thus, GAD is still O(N<SUP>2</SUP>) in the worst case.  I thank Mr. Schlapfer for bringing this to my attention.<P>
<h3><a name="017f_0009">Rumanian Rumination<a name="017f_0009"></h3><P>
Dear DDJ,<P>
I am the representative of the Consulting and Engineering Agency, a new private Rumanian company of consulting engineers, and I am writing to ask your readers for help.<P>
After the Rumanian revolution in December 1989, we realized that it was vital for Rumania to try and turn its economy around from the bankrupt socialist system into a market-driven economy.  One area which needs desperate and particular attention is computers and computing techniques.<P>
Twenty of our best software and hardware engineers from Constanta -- drawn from computer centers, universities, and other enterprises -- set up this agency.  The first major problem we face is a severe lack of scientific books, manuals, magazines, and catalogs.<P>
We realize that one of the most effective means of ridding our country of the terrible memory of Communism is not simply by appealing for clothes and food, but by asking for help in improving our education.  In this way we can improve our standards in industry and commerce, and help our people to make the things they need themselves.<P>
We would be very grateful if any of your readers could donate books and software, which is particularly expensive for us.  Perhaps even donate a subscription to their favorite magazine, or even send us old issues that they no longer need.<P>
Donations should be addressed to either of the following:<P>
Aurel Cartu, Manager Consulting &amp; Engineering Agency<P>
Aleea Brindujelor Nr 2,<P>
Bloc L9, Sc C,<P>
Ap 45 RO-8700<P>
Constanta<P>
Rumania<P>
Guilain Devillers<P>
A.I.D.E.<P>
P.O. Box 54<P>
L-8001 Strassen Luxembourg<P>


<HR><P>Copyright &copy; 1991, <I>Dr. Dobb's Journal</I></P></BODY></HTML>
