<HTML>
<META NAME="year" CONTENT="1991">
<HEAD>
<TITLE>FEB91: LOOKING INTO THE FUTURE OF MICROPROCESSORS</TITLE></HEAD>
<BODY BGCOLOR="#ffffff">
<h1>LOOKING INTO THE FUTURE OF MICROPROCESSORS<a name="0077_0003"></h1><P>
<h2><a name="0077_0001"><a name="0077_0000">A report from the Microprocessor Forum</h2><P>
<h3>Ray Duncan</h3><P>
<p><i><a name="0077_0002"><a name="0077_0000">Ray is a DDJ contributing editor and can be contacted at LMI, P.O. Box 10430, Marina Del Rey, CA 90295.</i></p><hr><P>
Over the past three years, the most important indicator of the microprocessor industry's future directions -- particularly in the areas of general-purpose microprocessors, embedded microprocessors, digital signal processors (DSPs), and multichip modules -- has become the Microprocessor Forum, sponsored by the Microprocessor Report.  The most recent conference was no exception, attracting several hundred hard-core engineers and systems designers, software developers, journalists, and -- in a rather ominous development -- lawyers.<P>
In his keynote speech entitle &quot;The Impact of Free Silicon on Microprocessor Design and Application,&quot; Andrew Rappaport of the Technology Research Group (Boston, Mass.) presented a simple premise: Microprocessor implementation and fabrication technology rides on the coattails of Dynamic Random Access Memory (DRAM) technology.  But DRAM technology is advancing so quickly that it will soon far outstrip our ability to make effective use of the number of transistors that we can cram onto a single CPU chip.<P>
To illustrate this point, consider that the 80386 was basically implemented with 256-Kbit DRAM technology; it has on the order of 300,000 transistors.  By the time the 80486 came along, 1-Mbit DRAMs were in full production.  The 80486 designers, though, were more interested in maintaining full compatibility with the 80386 than anything else, and really had very little need for the extra 700,000 transistors that 1-Mbit DRAM technology made available.  They ultimately managed to use up their one million transistor budget by putting the 80387 onto the same chip with the 80486, along with an 8-Kbyte cache.  And that was in 1988!<P>
4-Mbit DRAM production is coming online now, 16-Mbit DRAMs should be along by 1993 or 1994, and the year 2000 will, unless fabrication technology runs into some unexpected potholes, bring DRAMs with 128 Mbits or more on a single chip.  Over the same period, the manufacturing cost per DRAM bit should decline from the present 0.0005 cents (500 microcents) to around 0.00001 cents (10 microcents), and by the year 2010 will decrease further to around 0.0000008 cents (0.8 microcents).  At the same time, the estimated manufacturing cost per die of a one million-transistor microprocessor (such as the 80486) should decline from its 1989 value of $8.33 to around $0.29 (this does not include the cost of testing, packaging, and so on).<P>
The implications?  In the year 2000, it will be perfectly feasible and economical to put the equivalent of an 80386 in a tiny corner of every single DRAM chip!  The capabilities of the high-end microprocessors we are familiar with today will become essentially &quot;free&quot; -- and we will be able to put such processors literally everywhere.  As Rappaport says, what can be built will be governed by what can be designed and by our ability to find ways to utilize all this processing power.<P>
A counterpoint to Rappaport's prognostications was provided by Paul Saffo, InfoWorld columnist and chief guru of the Institute for the Future, in a talk entitled &quot;The Zen of Change.&quot;  The relationship of Saffo's arguments to Zen was lost on me (if indeed there was any; Zen seems to have become a popular marketing buzzword like &quot;object oriented&quot; or &quot;superscalar&quot;), but he did make some interesting points.  Saffo's theme was the natural history of the adoption of any new technology, and he drew some interesting parallels between the years following the invention of printing by Gutenberg and the current state of the computer industry.<P>
According to Saffo, the acceptance of a completely new technology takes place in three stages, each requiring about ten years.  During the first decade, the implications of the new technology are poorly understood, and it is frequently (mis)used to solve old problems to which it is only vaguely related.  Saffo calls this the &quot;paving the cowpaths&quot; period.  In the second decade, entrepreneurs experiment with applications of the new technology, taking it in wildly different directions (many of which inevitably turn out to be dead ends).  In the third decade, natural selection winnows out the suboptimal solutions, standards for use of the new technology emerge, mass production lowers costs drastically, and the technology is integrated seamlessly into the background noise of daily life.<P>
When Gutenberg printed his first books, nobody foresaw the newspapers, magazines, paperback racks, and bookstores of today; instead, monks and scribes decorated the printed books with colorful pictures and designs so they'd look more like the traditional hand-copied volumes. In the following years, the technology of printing spread rapidly throughout Europe, but there was as yet no standard design for books; for example, any given book might or might not have running heads, page numbers, a title page, or a table of contents.  It took several decades before the structure of a book as we know it today emerged and became recognized as the &quot;right way&quot; to publish a manuscript.<P>
A hundred years from now, the state of computer technology in our time may be perceived as the era of wild and woolly experimentation, or (who knows?)  even as the period of &quot;paving the cow paths.&quot;  We can't possibly visualize the uses to which Rappaport's &quot;free silicon&quot; will be put in a decade or two, any more than Gutenberg could imagine the satellite communications, high-speed printing presses, and distribution network that puts identical copies of USA Today (for better or worse) on every street corner in America each morning.  It's amusing to speculate whether the rapid spread of GUIs over the last few years really represents the best fruit of natural selection and the arrival of standardization.  Personally, I suspect that GUIs will fade away without a trace when the &quot;real&quot; user interfaces arrive -- perhaps real-time voice recognition and synthesis a la Star Trek, or direct mind-to-computer linkages in Cyberspace a la William Gibson.<P>
<h3><a name="0077_0004">New Chips Now<a name="0077_0004"></h3><P>
The most important new chip announcements at these sessions were probably the Motorola 88100 (the next generation of Motorola's 88000 RISC processor), the AMD 29050 (the next generation of its 29000 RISC processor), and the Inmos H1 transputer (the next generation of its current T425 and T805 transputers).  The least important and most useless announcement was undoubtedly Motorola's unveiling of the 68EC030 microprocessor.  The 68EC030, which was billed as a low cost implementation of the 68030 especially designed for embedded systems, turned out to be merely an ordinary 68030 with an untested Memory Management Unit (MMU) in a plastic package.<P>
For users of IBM-compatible PCs, as opposed to designers, the most interesting news was the first public description of AMD's AM286ZX/LX microprocessors and Intel's new two-chip set for notebook computers.  The AM286ZX/LX products are essentially PC AT motherboards on a single chip, including a CMOS 80286-compatible CPU, clock and bus controller, programmable counter/timer, DMA controller, DRAM controller, EMS address mapper, interrupt controller, real-time clock, and CMOS RAM.  The manufacturer of a laptop or notebook controller needs only to add some RAM, EPROMs for the ROM BIOS, keyboard controller, a mass storage device, and a display to have a complete functioning system.<P>
The new Intel chip set, called the &quot;SL SuperSet,&quot; is even more ambitious than AMD's.  The two components in the set are the 386SL CPU chip, which contains approximately 855,000 transistors, and the 8236OSL companion I/O chip, composed of approximately 226,000 transistors -- together replacing approximately eight VLSI chips or over 100 conventional integrated circuits.  The 386SL CPU is based on the 80386, but is a fully static implementation with considerable added logic (and a whole new execution &quot;mode&quot;) for power management.  Intel validated the SL SuperSet design before committing it to silicon by simulating the bootup of a DOS system from Ctrl-Alt-Del all the way to the C&gt; prompt.  This simulation took several days to execute on an IBM 3090 mainframe, but had an excellent payoff: The first chips off the assembly line were fully functional, and it was rumored at the conference that Compaq and Toshiba are already building prototype 80386 notebook computers using SL SuperSet engineering samples.<P>
<h3><a name="0077_0005">RISC vs. CISC<a name="0077_0005"></h3><P>
At the 1989 Microprocessor Forum, the RISC vs. CISC debates were bitter and, with nearly a dozen different RISC or RISC-wanna-be designs contending for attention, the RISC marketplace proper was still tremendously confusing.  At this year's Forum, the RISC proponents were noticeably less strident, perhaps partly because the RISC processors -- with the addition of floating point units, support for superscalar operation, and other complexities -- are diverging further and further from their original, Spartan ideals.  The RISC marketplace also seems to be shaking out somewhat; SPARC and MIPS appear to be the best choices if you are designing a workstation, while the AMD 29050 and the Intel 80960 look like the main contenders for the embedded systems market.  The Intel i860 and Motorola 88000 still have a few fans, but their importance is fading rapidly, and the IBM RISC processor is ignored by everyone except captive IBM accounts.<P>
<h3><a name="0077_0006">And Into the Future ...<a name="0077_0006"></h3><P>
The last session of the conference, called &quot;Architectural Issues for the 1990s,&quot; provided some interesting glimpses into the future.  Mike Johnson of AMD presented a tutorial on Superscalar and Superpipelined microprocessors; both these buzzwords represent advanced techniques to allow the execution of more than one instruction per machine cycle.  Gregory Papadopoulos of MIT described the current state of the art in data-flow processors, a peculiar species of CPU which may yet have its day when the technology of conventional processors has been pushed to its limit.  And Monica Lam of Stanford gave a talk on compiler-directed parallelism, showing how CPU designers and compiler designers can work together to achieve performance that neither can attain separately.<P>
The &quot;Architectural Issues&quot; session ended with a panel discussion where David Patterson of UC Berkeley, John Mashey of MIPS, Patrick Gelsinger of Intel, and other illustrious computer architects each presented their view of where microprocessors are going in the 1990s and beyond.  Predictably, there were some fundamental differences of opinion between the RISC proponents (such as Patterson, one of the designers of the RISC I, which evolved into the Sun SPARC) and the CISC advocates (such as Gelsinger, one of the architects of the 80486), although the RISC team generously acknowledged the 80486 as a technically excellent implementation of a bankrupt architecture.  But everyone on the panel agreed on one point: 32-bits of address space are not enough, and the first true 64-bit microprocessors will be appearing within a very short time.<P>


<HR><P>Copyright &copy; 1991, <I>Dr. Dobb's Journal</I></P></BODY></HTML>
