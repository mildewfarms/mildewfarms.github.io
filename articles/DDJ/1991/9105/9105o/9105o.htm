<HTML>
<META NAME="year" CONTENT="1991">
<HEAD>
<TITLE>MAY91: LETTERS</TITLE></HEAD>
<BODY BGCOLOR="#ffffff">
<h1>LETTERS<a name="010e_0001"></h1><P>
<h3></h3><P>
<h3><a name="010e_0002">A Reader Over Your Shoulder<a name="010e_0002"></h3><P>
Dear DDJ,<P>
In his February 1991 &quot;Programming Paradigms&quot; column &quot;A Programmer Over Your Shoulder,&quot; Michael Swaine presents several examples to illustrate Antonetta van Gasteren's ideas in streamlined mathematical arguments.  I'd like to comment on some of them.<P>
In the maximization example, defining the sequence elements instead as elements of two &quot;sets&quot; (the mathematical equivalent of &quot;bags&quot;) would clarify the problem somewhat, although subscripts and permutations would still be required.  Also, I wonder whether Hardy, Littlewood, and Polya were really &quot;deluded&quot; by their own notation.  In mathematics, looking at a problem from a different perspective (recasting a geometric problem in an equivalent algebraic form, say) is a common, often enlightening practice.  Perhaps they knew what they were doing.<P>
The point about retaining symmetry whenever possible is well taken; however, in the game involving bit strings, restating the proposition in terms of x and y accomplishes nothing. The critical fact is that the leftmost bit never changes; once that observation has been made, the conclusion follows directly from mathematical induction (which Swaine uses implicitly in his argument for termination).  The only way to see that the leftmost bit never changes, though, is to refer back to the original transformations.  I believe a better example could have been found to illustrate the point.<P>
Steve Medvedoff<P>
Santa Maria, California<P>
Dear DDJ,<P>
Our work as programmers often leads us to confuse computations and concepts.  Michael Swaine, LISPer-extraordinaire, endorses recursion in place of the concept.  In his February 1991 review of a book by Antonetta van Gasteren, he gives a problem simply and correctly: Match up pairs of natural numbers from two finite sets of the same size and maximize the sum of the products of the pairs.  Then he defers to van Gasteren's recursive statement of the problem and claims that her recursive solution stands out among &quot;generally messy&quot; proofs.<P>
Hardly.  The solution is to sort the two sets of numbers the same way (ascending or descending) and pair them.  A neat proof says: Starting from this solution, suppose we exchange the members of two pairs.  Do a little algebra (omitted here) by writing the larger numbers as the smaller ones plus nonnegative differences.  Compute and compare the required sums.  The correct matching is greater by the nonnegative product of the difference terms, so no alternative can be larger.<P>
The statement of the problem by van Gasteren introduces unnecessary recursion.  I maintain that the above proof, even written out with a few variables, is clearer than a recursive one.<P>
Charles Pine<P>
Oakland, California<P>
<h3><a name="010e_0003">Multimedia Muscles<a name="010e_0003"></h3><P>
Dear DDJ,<P>
I read with interest the &quot;Editorial&quot; in the February 1991 issue where the cost/performance aspects of various multimedia platforms are considered.  The conclusion is that the Amiga is the sensible choice.  The Amiga's especially versatile, sophisticated, and compact preemptive multitasking operating system makes it ideal for multimedia.  The editorial goes on to say that the Amiga has never made it to the broad-based market.  Although there is an Amiga for every two Macs, neither of those machines is broad-based in the sense that MS-DOS machines are.<P>
Accurate assessments of the Amiga, especially those dealing with multimedia, coupled with such powerful programs as AmigaVision and hardware/ software exclusive to the Amiga such as the Video Toaster, should continue to propel it into the &quot;mainstream.&quot;<P>
Mainstream publications, such as the March 1991 Video and Popular Science magazines, mention the Amiga more often than computer magazines seem to.  The city of Atlanta won its bid to host the Olympics on the strength of an Amiga running the multimedia presentation.  A Mac II was originally tried, but was relegated to a supporting role when it was found inadequate to the task -- due to an inability to multitask, perhaps?  Now I'm beginning to sound like Michael Swaine.<P>
Jeff Johnson<P>
Cincinnati, Ohio<P>
<h3><a name="010e_0004">In A Snap<a name="010e_0004"></h3><P>
Dear DDJ,<P>
I'm writing to register my vote for the best article in the February 1991 issue: &quot;Screen Capturing for Windows 3.0&quot; by Jim Conger.  While possibly one of the shortest code-related articles in DDJ's history, it came at just the time that I needed such a utility.  Thanks!<P>
While the program worked perfectly, there was one small bug (which Conger fixed via a kludge).  While initializing the wndclass structure, the icon should have been loaded as follows (substituting hInstance for NULL):<P>
<pre>wndclass.hIcon - LoadIcon (hInstance,   szAppName);</pre><P>
Had this been done, the entire if (IsIconic (h Wnd)) clause within the WM_PAINT case could have been eliminated, and the matching else clause just made part of the straight line WM_PAINT code.<P>
Brian R. Anderson<P>
Burnaby, British Columbia<P>
Jim responds: Thanks.  Note that this ties the icon to the window class, so that any window created from this class will minimize with that icon.  That is fine in a small application like Snap3, but probably not what you want in a larger program with multiple calls to CreateWindow( ).<P>
<h3><a name="010e_0005">Protecting Protected Mode<a name="010e_0005"></h3><P>
Dear DDJ,<P>
I read &quot;Accessing Hardware from 80386 Protected Mode, Part I,&quot; by Stephen Fried (May 1990) with great interest, mostly because I've spent the past few years programming for Intel's iRMX, which runs in protected mode.  While I generally enjoyed Mr. Fried's article, there are two subjects upon which I must comment.<P>
First, 64 terrabytes is 2**46, not 2**48.  The size of the virtual address space is (2**14 selectors) * (2**32 maximum segment size in bytes).  The lower two bits of the selector contain the requestor's privilege level (RPL), which is the reason there are not 2**16 possible selectors.<P>
Second, I do not like the convention of overlapping code and data segments in a flat address model, especially when executing in ring 0.  I specifically refer to Figure 1, in which selectors Och (user code) and 14h (user data) overlap, sharing a base at 100000h with a 2ffh offset limit.  Since a task running at privilege level 0 is at supervisor level with respect to the page-level protection mechanisms, it is not subject to the page R/W restrictions of user-level tasks, either.  Such a combination defeats an important protection mechanism: preventing a program from overwriting its own code.  I do not know of any compiler that will generate such code (although I've seen library implementations of int86() that modify code inline), so any time compiled code writes to the code segment, it is due to indirection through an invalid pointer, i.e., a programming error.<P>
Stephen Fried disagrees with me.  He believes programs should run at ring 3 only if the memory mapping described in his Figure 1 is used.  In other words, a program should have read, write, and execute privileges for every byte of allocated memory.  Perhaps he could satisfy himself and me both by allowing two different types of loading.  By including offset fixup data in the executable (a cross between relocation data in a standard DOS executable and FIXUPP records in OMF-86 object modules), the loader could set descriptor bases either in the Fried-preferred method or in a manner such that code and data do not overlap.<P>
I think that programmers benefit by having invalid memory references trapped by the hardware.  Protection should be utilized as a developer's tool.  Let's not cripple the 286/386/486 chips to satisfy our DOS programming prejudices.<P>
Charles Scott Nichol<P>
Philadelphia, Pennsylvania<P>
<h3><a name="010e_0006">Persistence Pays Off<a name="010e_0006"></h3><P>
Dear DDJ,<P>
Here's a simpler way to have persistent objects other than that described by Scott Ladd in his article &quot;Persistent Objects in Turbo Pascal&quot; (September 1990).  I was writing a program that had to store several objects of different types in a file, and I read the Turbo Pascal manual to find out the details of the storage of objects in memory.  The format makes it easy to store and retrieve objects from disk.<P>
Objects are stored exactly like a record except for a VMT (Virtual Method Table) if there are any virtual methods.  The VMT is 2 bytes, and it should not be stored on disk. Furthermore, the problem with SizeOf( &lt;object&gt; ) does not apply when the object has a VMT. The VMT points to the actual size of the object and not the size of the base class.  This makes it easy to create a base Storable class:<P>
<pre>  Type
  Storable = object
        Procedure Load( var F : File );
        Procedure Save( var F : File );
        Destructor Done; virtual;
  end;
  Procedure Storable.Load;
         Type
              ObjectData = Array [0..2] of Byte;
         Begin
              BlockRead (F, ObjectData(Self)[2],
                                             Sizeof(Self) - 2);
         End;
  Procedure Storable.Save;
         Type
             ObjectData = Array [0..2] of Byte;
         Begin
              BlockWrite (F, ObjectData(Self)[2],
                                             Sizeof(Self) - 2);
          End;
  Destructor Storable.Done
          Begin;
          End;</pre><P>
Any other object that should be persistent can be inherited from Storable.  Storable's Load and Save methods will automatically find the descendant type's size and store it.  The descendant types do not have to write each individual field as they do in Mr. Ladd's program.<P>
Amit J. Patel<P>
Houston, Texas<P>
<h3><a name="010e_0007">Graph Decomposition Redux<a name="010e_0007"></h3><P>
Dear DDJ,<P>
I read Edward Allburn's article &quot;Graph Decomposition&quot; (January 1991) with some interest. Although my work has nothing to do with graphs, I am fond of algorithms and I also use the Phar Lap DOS Extender with MetaWare's High-C compiler.  Mr. Allburn wrote that the most expensive section of the algorithm was in determining if both vertices are already in the same set.  A technique for reducing this effort comes readily to mind.<P>
An additional data item will be needed for each element of an array.  The size of this item in bits is up to you.  More bits will reduce the effort required.  (You could either steal some of the high-order bits of each entry, or you could make each entry take more space.)<P>
Each time an adjacency list is created, the additional data items will be set to a given value.  This value is based on incrementing a counter modulo 2 to the N where N is the number of bits you have available in the additional udata item.<P>
Whenever an additional element is added, it gets its data item from the list it is being added to.  If both vertices have been seen before, you first compare their additional data items.  If they are different, then there is no chance that they are currently in the same adjacency list.  If they are the same, then you must use your current technique for finding this out.  Whenever adjacency lists are merged, you can either set all the combined elements additional data items to a new value, or you can pick one of the lists, and set all of the other entries to the first's value.<P>
This reduces the number of times that you must find out the hard way if two vertices are in the same set.  The amount that your effort is reduced is proportional to the number of bits you can allocate for the additional data item.  Just four bits would reduce the number of times you would have to work things out the hard way by an order of 1 over 16.<P>
Neal Somos<P>
Seven Hills, Ohio<P>
Dear DDJ,<P>
I was intrigued by Edward Allburn's article &quot;Graph Decomposition&quot; (January 1991).  I do not have a sufficient background in graph theory to really follow his argument, so I set myself to the challenge of trying to discover an algorithm which would solve the problem as I understand it.<P>
<B>Given</B>: 1. A set of integers 1..Ncount 2. A set of pairs P(a,b), where a &amp; b are in 1..Ncount. Pcount = number of pairs.<P>
Then:<P>
Assign each element in 1..Ncount to a subset so that all the subsets are disjoint and P(a,b) ==&gt; (a in Subset &lt;==&gt; b in Subset).<P>
I am a software developer specializing in graphics and calculations for manufacturing and industrial customers.  I consider my main strength to be in the area of algorithm design (incidentally, I agree with Mr. Allburn's assessment of Sedgewick).  Over the past year, I have worked with a small team which developed a 3-D modeling and rendering package, and alone I developed a time management package for large assembly lines.  I got my math training as a physics major, and consequently I'm weak on graph theory.<P>
I'm wondering if Mr. Allburn could take a look at the algorithm that follows.  I am really curious to know if my &quot;flash of insight&quot; amounted to anything.<P>
Graph Decomposition II<P>
Let each connected component be referred to by the number of its minimum vertex.  In Figure 2c (see DDJ, January 1991, page 90) we have<P>
<pre>   component 1 (2,3,5,6)
   component 4 (7,8)
   component 9
   component 10
   component 11 (12)</pre><P>
For each vertex i, let G[i] store the minimum vertex.  We want an algorithm to produce G as follows:<P>
<pre>    G[1] = 1         G[7] = 4
    G[2] = 1         G[8] = 4
    G[3] = 1         G[9] = 9
    G[4] = 4         G[10] = 10
    G[5] = 1         G[11] = 11
    G[6] = 1        G[12] = 12</pre><P>
Then determining if a path exists between a and b is equivalent to checking if G[a] == G[b]. In pseudocode:<P>
<pre>   Step 1: for each i, G[i] = 1
   Step 2: for each pair (a,b)
               find min and max of G[a], G[b]
               G[max] = G[a] = G[b] = min
   Step 3: for each i, G[i] = G[G[i]]</pre><P>
Step 1: Initialize G by marking each i as connected to itself.<P>
Step 2: When a new pair is introduced, we actually have four vertices:<P>
<pre>  Two members of the pair: a &amp; b
  Their &quot;minimum&quot; connections (to date) G[a] &amp; G[b]</pre><P>
<B>Question</B>: Which of these is the absolute minimum: Once we've found this, label the other three with it.  Since G[i] &lt;= i for all i, it suffices to just check the Gs.  For example, imagine that at some point G is:<P>
<pre>   G[1] = 1        G[4] = 4
   G[2] = 1        G[5] = 4
   G[3] = 1        G[6] = 4</pre><P>
Now comes the pair P(6,3).  Max = G[6] = 4, min = G[3] = 1.  G changes to:<P>
<pre>   G[1] = 1
   G[2] = 1
   G[3] = 1 &lt;- 1     G[b] = min
   G[4] = 4 &lt;- 1     G[max] = min
   G[5] = 4
   G[6] = 4 &lt;- 1   G[a] = min</pre><P>
I call P(6,3) a &quot;late arrival&quot; because until it arrived, it looked like we had two separate components, notice that this step has &quot;informed everyone in the group&quot; except #5.  Step 3 handles that.  Step 3: Final assignments: G[i] = G[G[i]]; in this example:<P>
<pre>   G[1] = 1 &lt;- 1
   G[4] = 1 &lt;- 1
   G[2] = 1 &lt;- 1
   G[5] = 4 &lt;- 1
   G[3] = 1 &lt;- 1
   G[6] = 1 &lt;- 1</pre><P>
This is really a check to see if G[i] has been reassigned without i's knowledge.  If G[i] is not equal to G[G[i]], such as G[5] = 4 in the previous example, then G[5] has been reassigned.  In this case, vertex 4 (=G[5]) was reassigned by P(6,3) without #5's knowledge.  If G[i] already equals G[G[i]], then G[i] has not been reassigned, and this step is redundant but does no harm.<P>
Bill Polson<P>
Saginaw, Michigan<P>
Ed responds: Neal's suggestion is similar to one that a colleague of mine posed several months ago.  Basically, the idea is to assign each of the connected components an ID number, and to have each vertex carry the ID number of the connected component the vertex belongs to.  Thus, when determining if two vertices are already in the same connected component, the first step would be to compare the ID numbers attached to each vertex.  It would then only be necessary to traverse one of the connected components if the ID numbers happened to match.  Thus, one would theoretically save a significant amount of time when merging a pair of connected components.  The drawback to this is that the ID numbers of one of the connected components have to all be updated to match the other.  When I experimented with this idea, however, I found that this process of updating all of the ID numbers consumed all of the time that the ID numbers saved.<P>
Bill Polson's algorithm is right on the money.  In some respects, his algorithm is similar to Neal's suggestion of using ID numbers on each connected component.  In Bill's algorithm, the smallest vertex number of the connected component basically serves this purpose.  I implemented both his algorithm and GAD using Watcom C 386 8.0.  When I compared them side by side, I found that Bill's algorithm consistently ran twice as fast as GAD when counting connected components in worst-case graphs.<P>
The main drawback of Bill's algorithm compared to GAD is that it takes significantly more time to determine what vertices are in a particular connected component.  I recently had the opportunity to write a DIFF program that is used to compare large graphs at my workplace. When a difference is found, all of the vertices involved in the difference must be logged into a report.  Since GAD represents each connected component as a linked list of vertices, one simply follows this list to find all of the vertices.  With Bill's algorithm, it would be necessary to scan the entire array to find all of the vertices.  Bill, too, has released his algorithm into the public domain.  It deserves serious consideration by anyone who is working with graph data structures.<P>


<HR><P>Copyright &copy; 1991, <I>Dr. Dobb's Journal</I></P></BODY></HTML>
