<html>
<head>
<title>Spin Buffers</title>
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/Layout.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/FontStyles.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/newarticle.css" type="text/css">
<script src="../../../../forms/popwindow.js"></script>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->
<p><i>Dr. Dobb's Journal</i> July 2007</p>

<h1>Spin Buffers</h1>
<h2>Eliminating performance bottlenecks</h2>


<h3>By Prashanth Hirematada</h3>


<I>Prashanth is the chief architect for Gamantra. He can be contacted at prash@gamantra.com.</I>

<hr>




<p>Given any software, there is always a need for sharing resources or passing objects between two or more modules. When two threads are exchanging data as in Figure 1, the "producer" thread puts the resource into a common, shared area (a "shared FIFO buffer"), and the "consumer" thread takes the resource out of the shared area. </p>


<div>
    
<!--  -->    
<img class="illowide" src="070601ph01_f1.gif" style="width:480">

<div class="caption" style="width:482">
Figure 1: Two threads are exchanging data. 
</div>
</div>


<p>However, producer-consumer patterns are often performance bottlenecks. Why? Because one thread has to lock the other one out when both are accessing the shared resource area. This causes the operating system to put one thread on the wait list, while the other accesses the shared area. Of course, when this code becomes an application's hot spot (a common situation), there are alternatives. One approach is to minimize synchronization as much as possible. But no matter how small you make the synchronization functionality, the problem remains and is amplified when the data/resource exchange happens a thousand times a second. Consequently, you should consider using Spin buffers if you are writing high-performance applications because they eliminate the need for synchronization. They don't even need to employ low-level atomic instructions (such as Compare &amp; Swap) found in advanced processors.</p>










<h3>Ring Buffers</h3>

<p>The most common way to implement the shared buffer is a Ring buffer (Listing One) with a certain size (<i>SIZE</i>). With Ring buffers, you maintain two pointers&#151;one to read (<i>m_rptr</i>) and one to write (<i>m_wptr</i>). The producer inserts a new item into the buffer at <i>m_wptr</i>, then increments it by 1; for instance, <i>(m_wptr+1)%size</i>. Similarly, the consumer inserts a read from <i>m_rptr</i>, then increments it by 1. If the special condition <i>m_rptr == m_wptr</i> is True, the buffer is empty. </p>

<p>One thing you should avoid is letting the read pointer get ahead of the write pointer. To ensure this doesn't happen, you could maintain the current size (<i>m_size</i>) so that the read pointer is not incremented. In this case, <i>m_size</i> is 0 (buffer is empty, nothing to read) and the write pointer should not be incremented if <i>m_size</i> is equal to the buffer size (buffer is full, write fails).</p>

<p>Let's package all this into a class and provide access to methods <i>put</i> and <i>get</i>. Note that I try to have the smallest amount of synchronized code; namely, the method <i>updateSize()</i>. In most cases, this is okay because it is straightforward. But if you are producing a high-performance application, you need to take a closer look at the producer-consumer patterns in the applications. For example, I work on game server engines that require more than 100,000 messages be processed every second, then broadcast back to the clients.</p>


<h3>Spin Buffers</h3>

<p>Spin buffers contain three internal "ordered" buffers (see Figure 2), but expose a simple API similar to Ring buffers. The buffers are ordered as 0, 1, and 2. The buffer of 0 is 1, then the buffer of 1 is 2, and finally, the buffer of 2 is 0. Two pointers are maintained&#151;one that points to a buffer that a reader reads from, and another that a writer writes to. Referring to Figure 2, at any time there can be an assigned read buffer (<i>R</i>), an assigned write buffer (<i>w</i>), and free one (<i>f</i>). The buffers can be implemented as fixed-sized arrays.</p>


<div>
    
<!-- http://i.cmpnet.com/ddj/images/article/2007/07/ -->    
<img class="illowide" src="070601ph01_f2.gif" style="width:480">

<div class="caption" style="width:482">
Figure 2: Spin buffers contain three internal "ordered" buffers.
</div>
</div>


<p>The <i>put</i> method is as follows:</p>

<ol>
    <li>Put the item into the write buffer.</li>
  <li>If the next buffer is free, make the free buffer become the write buffer, and the current write buffer becomes free.</li>
  </ol>

<p>The <i>get</i> method is as follows:</p>

<ol>
    <li>Read the item from the read buffer.</li>
  <li>If the current read buffer is empty and the next buffer is free, make the next buffer the read buffer, and the current read buffer becomes free.</li>
  </ol>

<p>Listing Two is the Spin buffer implementation in Java (it should be straightforward to write it in other programming languages).</p>










<h3>Caveats</h3>

<p>There are things to watch out for when implementing Spin buffers. For instance, the reader and writer on the Spin buffer must be called all the time; otherwise, you could run into a situation where exiting items in the buffer cannot be retrieved by the reader. This happens when writers have nothing to write; if <i>put</i> is not called, the items in the current write buffer can never be read by readers. One way around this is to call <i>put</i> with null, which forces the write buffer to advance to the next buffer, thereby letting the reader thread access the items in the buffer.</p>

<p>Spin buffers are also sensitive to impedance mismatch, which means slow readers slow down the writer and visa versa. The performance peaks when reads/writes are called at matching frequency.</p>

<p>The current implementation works when there is only one reader and one writer. If multiple readers and writers are needed, the <i>put</i> method must be synchronized; similarly, if there are multiple writer threads, the <i>put</i> method must be synchronized. However, the code for reading/writing need not be synchronized.</p>


<h3>Performance Analysis</h3>

<p>I did the comparative performance analysis on these three implementations: </p>

<ul>
  <li>Ring Buffer.</li>
  <li>Java <i>ConcurrentLinkedQueue</i> (CLQ) (based on "Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms," by Maged M. Michael and Michael L. Scott; www .research.ibm.com/people/m/michael/podc-1996.pdf).</li>
  <li>Spin Buffer.</li>
  </ul>

<p>I conducted the performance tests on a hyperthreaded 3.0-GHz Pentium machine with 1 MB of RAM. I ran the tests multiple times for a minimum period of one hour on each of the different algorithm implementations. Table 1 presents the results. (In Table 1, I use the term "bandwidth" to describe how fast a buffer enables the data transfer from producer to consumer. The higher the bandwidth of the shared buffer implementation, the higher the number of items that can be transferred through&#151;and the better the overall performance of the applications.) The performance test code is available <a href="http://www.ddj.com/code/">www.ddj.com/code/</a>.</p>


<div>
<table cellpadding=3 cellspacing=3>
  <tr>
    <td><b>Implementation</td>
    <td>Ring buffer</td>
    <td>CLQ</td>
    <td>Spin buffer</b></td>
  </tr>
  <tr>
    <td><b>Bandwidth  (millions/sec)</b></td>
    <td>1.17</td>
    <td>1.77</td>
    <td>5.05</td>
  </tr>
</table>

<div class="caption">
Table 1: Test results.
</div>
</div>



<h3>Conclusion</h3>

<p>Spin buffers are simple to implement, expose a straightforward API, and don't require special hardware or advanced processors. In spite of their shortcomings, the performance gains that Spin buffers deliver make an ideal choice for a wide range of applications such as game server engines, graphics, networking, and memory managers, among other high-performance applications.</p>

















</body>
</html>