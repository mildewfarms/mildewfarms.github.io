<html>
<head>
<title>RapidMind: C++ Meets Multicore</title>
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/Layout.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/FontStyles.css" type="text/css">
<link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/newarticle.css" type="text/css">
<script src="../../../../forms/popwindow.js"></script>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->
<p><i>Dr. Dobb's Journal</i> July 2007</p>

<h1>RapidMind: C++ Meets Multicore</h1>
<h2>Making the most of multiple cores</h2>


<h3>By Stefanus Du Toit and Michael McCool</h3>


<I>Stefanus is the founder of and chief architect at RapidMind and 
Michael is chief  scientist at RapidMind. They can be contacted at www.rapidmind.net.</I>

<hr>




<p>You can't go far in the software industry these days without hearing about the inevitability of multicore processors. Herb Sutter may have put it best in his Dr. Dobb's Journal article when he said, "The free lunch is over" (www.ddj.com/dept/architect/184405990). It is telling that this warning was quoted by Intel's Tim Mattson and James Reinders in a web-based presentation that explained why multicore is a reality, and is here to stay. Attempting to scale processor speed using traditional means such as increased clock frequency and deeper pipelines is running into physical limitations, putting an end to the "free lunch" in performance that developers&#151;and users&#151;have become accustomed to. Luckily, the answer is straightforward: Instead of attempting to make more complex single cores, processor makers are putting multiple simpler cores on the same die.</p>

<p>Multicore designs have been used in other processors for some time. Less restricted by traditional architectural designs, graphics processing units (GPUs), and the Cell Broadband Engine (Cell BE) processor (www.ddj.com/dept/64bit/197801624) by Sony, Toshiba, and IBM, have demonstrated tremendous performance improvements employing massively parallel approaches to processor architecture. These processors provide opportunities for high-performance applications. But the multicore revolution isn't limited to these processors. With multicore designs being adopted by CPU vendors such as AMD and Intel, parallel programming is a necessity for all developers.</p>

<h3>The Problem with Multicore</h3>

<p>Although multicore clearly provides potential for high performance, software needs to explicitly take advantage of the multiple cores to fulfill this potential. Sadly, no compiler turns your serial algorithms expressed using C++ into perfectly parallelized programs that scale to an arbitrary number of cores. Traditional approaches such as multithreading force you to spend time worrying about thread management, instead of designing scalable parallel algorithms. With the number of threads growing as the number of cores does, dealing with bugs caused by deadlocks and race conditions hampers developer productivity significantly. At four or more cores, the complexity of threading becomes a serious problem.</p>

<p>So are there high-level approaches to parallel computing that map well to multicore architectures? After all, parallel computing is nothing new. Architectures such as the Thinking Machines Connection Machine CM-2 sported as many as 65,000 processors in the late '80s. However, past high-level approaches to parallel computing have often been focused on new languages or nonstandard extensions to existing languages. This approach allows a great deal of flexibility in terms of syntax, but means that developers are forced down a heavy migration path by dropping established tools and languages. The lack of good approaches to parallel programming that embrace the existing toolchain was one of the factors that led to the sluggish adoption of parallel computing in the past.</p>









<h3>Heterogeneous Cores</h3>

<p>Parallelization is only half the battle. The move to multicore is an opportunity for chip vendors to embrace other interesting architectural changes in an attempt to reach teraflop performance within this decade. We illustrate this by describing the architecture of the Cell BE processor. Each Cell BE contains a total of nine cores. The Power Processing Unit (PPU) is a fairly traditional CPU core, with a PowerPC instruction set architecture and a reasonably sized cache. The other eight cores are referred to as Synergistic Processing Units (SPUs), and use a nontraditional architecture to achieve high performance in a small chip area. Instead of caches, each SPU sports 256 KB of on-chip, locally addressable memory (the local store), in addition to 128 128-bit registers. The SPUs use a vector instruction set that allows, for example, operating on groups of four floating-point numbers at once. Each SPU also features its own Memory Flow Controller (MFC), which can independently issue DMA transfers between main memory and the SPU local stores. The SPU's predictable, dual-issue pipeline allows complete certainty about the optimality of a particular sequence of assembly instructions. This lets a single Cell BE processor using all eight SPUs perform large matrix multiplications at over 200 Gflops, compared to about 12 Gflops for a single traditional CPU core.</p>

<p>The same features that make the Cell such a high-performance chip also make it hard for compilers for languages such as C/C++ to take advantage of. The assumptions these languages make about memory organization and instruction sets simply do not map well to heterogeneous processors such as the Cell. And if you think this kind of architecture is restricted to the domain of game consoles and special-purpose equipment, consider what vendors such as AMD and Intel are saying about their upcoming architecture. AMD's Fusion project aims to merge GPU-like cores with traditional x86-style cores, which, given the similarities between SPUs and GPU processors, will result in an architecture much like that of the Cell. At its 2006 developer forum, Intel showed off an 80-core processor prototype capable of more than a teraflop of compute power. Those 80 cores are not traditional x86 cores&#151;this is an entirely different processor architecture. Intel also recently announced the Larrabee project, an attempt at converging GPUs and multicore CPU architectures.</p>

<p>So wouldn't it be nice to be able to program with the familiar tools and languages without compromising on performance? </p>









<h3>The RapidMind Platform</h3>

<p>The RapidMind Development Platform, developed by RapidMind (which Stefanus cofounded), is just such a framework for expressing data-parallel computations from within C++ and executing them efficiently on multicore processors. It lets you specify any computation you want to leverage multiple cores within existing C++ applications, without changing your compiler or workflow. Figure 1 illustrates the RapidMind architecture. </p>


<div>
    
<div class="smallcap">&#91;Click image to view at full size&#93;</div>
<!--  -->    
<img class="illowide" src="070601sd01_f1.gif" onclick = "popimage(this,'www.ddj.com - RapidMind: C++ Meets Multicore - Figure 1')">

<div class="caption">
Figure 1: The RapidMind architecture.
</div>
</div>


<p>The Developer Edition of the RapidMind platform (download for free at www.rapidmind.net) is currently available for a number of Linux and Windows versions, and is supported on 32- and 64-bit systems. </p>

<p>There are three basic types you need to be aware of when programming with RapidMind: </p>



<ul>
  <li><i>Value</i>s are small pieces of data, similar to the primitive types such as <i>float</i> and <i>int</i> in C++. </li>
  <li><i>Array</i>s represent arrays of these values, just like C arrays or C++ vectors. </li>
  <li><i>Program</i>s encapsulate computation, in the same way that a C++ function does. All of these types, and a number of less important classes, are contained in the <i>rapidmind</i> namespace, which is made available by including a single header:</li>
</ul>

<pre class="code">

#include &lt;rapidmind/platform.hpp&gt;

</pre>








<h3>Values</h3>

<p>In terms of the <i>Value</i> type, you can declare two values, initialize them, then add them up and place the result in a third value:</p>

<pre class="code">

Value1f x(1.0f), y(2.0f);
Value1f z = x + y;

</pre>
<p>The <i>1</i> and <i>f</i> portions of the <i>value</i> types are used here. The <i>1</i> means that this value contains a single scalar. <i>Value</i>s can contain any fixed number of scalars, but they usually contain between one and four elements. The <i>f</i> stands for <i>float</i>. Values can contain any standard C++ type, as well as some nonstandard ones (most notably, half-precision floating-point types). <i>Value</i>s are templated; <i>Value1f</i> is actually a <i>typedef</i> for <i>Value&lt;1</i>, <i>float</i>&gt;. <i>Typedef</i>s for up to four elements of all basic types are provided by the platform.</p>


<h3>Programs</h3>

<p>The previous example of using <i>value</i>s might not make them seem particularly interesting. The computation, if inserted into a C++ function, would simply happen immediately in the same thread the rest of the function is executing in.<i> Value</i>s become more interesting when they are combined with the <i>Program</i> type. This code illustrates a program definition using RapidMind:</p>

<pre class="code">

Program add_two_numbers = 
      RM_BEGIN {
  In&lt;Value1f&gt; x, y;
  Out&lt;Value1f&gt; z;
  z = x + y;
} RM_END;

</pre>
<p>This trivial program captures the same computation we performed directly on <i>value</i>s above. However, the computation does not execute immediately. Instead, it is stored in the <i>program</i> object <i>add_two_numbers</i>, and can later be used to compute the sum of two numbers. When a <i>program</i> object is defined, every computation on <i>RapidMind</i> types between the RM_BEGIN and RM_END statements is collected and stored within the object. This process happens at runtime and does not require any special preprocessing or compiler modifications. <i>Program</i>s can be defined in any function, but typically a <i>program</i> is defined in a constructor of a class encapsulating some computation.</p>


<h3>Arrays</h3>

<p>Adding two numbers is not a very parallel operation. Adding two arrays of numbers, however, can be parallelized assuming the arrays are large enough. RapidMind lets program objects be called on entire arrays at once:</p>

<pre class="code">

Array&lt;1, Value1f&gt; a(10000),
     b(10000), c;
for (int i = 0; i &lt; 10000; i++) {
  a.write_data()[i] = ...;
  b.write_data()[i] = ...;
}
c = add_two_numbers(a, b);


</pre>
<p>The first line declares three arrays. The arrays <i>a</i> and <i>b</i> are initialized to make space for 10,000 elements each, whereas <i>c</i> is initially empty. Just like <i>Value</i>, <i>Array</i> is a simple class template. The first template parameter specifies the dimensionality of the array (one, two, or three), and the second parameter specifies the element type of which the array holds a collection.</p>

<p>In the next four lines we simply initialize our input arrays with some data. The <i>write_data()</i> function obtains a plain C++ pointer to the data&#151;in this case, a<i> float*</i>&#151;that can be used to modify the array. A similar <i>read_data() </i>function can be used for read-only accesses. Distinguishing between the two helps the platform understand when it needs to move data around; for example, to do a copy-on-write operation.</p>

<p>The last line of the snippet calls upon our program object, <i>add_two_numbers</i>, to perform the computation. Note how the program object is used just as though it were a C++ function. Now, even though this program takes two values as its input, and produces another value, you can call it on the entire array. This is effectively the same as calling the program once for each entry in the array; for example, by looping through the array. However, by applying the program to entire arrays at once, the parallelism is explicit. The platform knows that this computation can be executed independently for each element it computes, and therefore knows that the computation can be split across an arbitrary number of cores.</p>










<h3>Backends and  Runtime Compilation</h3>

<p>When this last line runs, the platform actually does a little more than just execute the <i>program</i> object. First, it decides which "backend" should be responsible for the program executions. Backends form the connection between the RapidMind platform and a particular piece of target hardware. For example, RapidMind currently ships with backends for the Cell BE, OpenGL-based GPUs, and a fallback backend that generates and compiles C code on the fly. A multicore x86 backend will be released later this year, and other backends are in the works.</p>

<p>Once a suitable backend has been chosen (a process that is generally instantaneous), it is asked to execute the program under the given conditions. The first time this is done generally causes the program object to be compiled for that particular backend, similar to the way a JIT environment behaves. Once a program has been compiled, it is not recompiled. Compiles can also be forced to happen using the <i>compile()</i> function.</p>

<p>This runtime compilation mechanism is powerful, as the generated code is optimized for the exact conditions it is being run under. This is one reason why RapidMind-generated code outperforms plain C or even hand-tuned assembly code in many cases.</p>


<h3>A More Interesting Example</h3>

<p>The previous program was a trivial example. <i>Program</i> objects typically contain more statements, and include function calls, random-access array reads, control flow, and the like. The following program, which computes a Julia set fractal, is more complicated, but still fairly basic:</p>

<pre class="code">

Program julia2d = RM_BEGIN {
  In&lt;Value2i&gt; index;
  Out&lt;Value1f&gt; shade;
  std::complex&lt;Value1f&gt; 
    z(index[0]/(Value1f)width - 0.5f, 
    index[1]/(Value1f)height - 0.5f);
  z *= scale;
  Value1ui i;
  RM_FOR (i = 0, i &lt; max_iterations 
    &amp;&amp; std::norm(z) &lt; 4.0f, i++) {
    z = z * z + c;
  } RM_ENDFOR;
  shade = i/(Value1f)max_iterations;
} RM_END;

</pre>
<p>This program (available at <a href="http://www.ddj.com/code/">www.ddj.com/code/</a>) takes as its input the location of a pixel in an image, then computes a shade for that particular pixel by iterating a complex arithmetic expression and checking whether it diverges; see Figure 2. To perform the complex arithmetic, we use the standard C++ class <i>std::complex</i> with a <i>RapidMind</i> <i>value</i> type. Because <i>RapidMind</i> <i>value</i> types act like numeric types in C++, this works out of the box. It is often possible to convert a piece of existing C++ code to RapidMind by simply replacing basic C++ types with RapidMind types.</p>


<div>
    
<!-- http://i.cmpnet.com/ddj/images/article/2007/07/ -->    
<img class="illowide" src="070601sd01_f2.gif" style="width:360">

<div class="caption" style="width:362">
Figure 2: Image produced by sample program.
</div>
</div>









<h3>Control Flow</h3>

<p>The Julia set program also illustrates control flow. The main computation encapsulated in the <i>program</i> object, a complex multiplication and addition, is repeated until a maximum number of iterations is reached or the norm of the complex number grows greater than four. Because the latter condition depends on a computation being performed in the program object, we cannot simply include a C++ <i>for</i> loop here. The <i>for</i> loop would attempt to determine the value of <i>std::norm(z) &lt; 4.0f</i> as a <i>bool</i>, but the value of <i>z</i> is not yet determined because the program is merely being collected, not executed. You can use C++ <i>for</i> loops in program definitions, but their conditions need to be based on nonRapidMind types. For example, it's possible to run a loop that iterates a constant number of times and that unrolls the RapidMind computations it contains.</p>

<p>RapidMind does allow dynamic control flow in programs using constructs such as RM_FOR, RM_IF, and RM_WHILE. These constructs generally behave like their C++ counterparts, but are collected into the program. By letting you use control constructs, RapidMind exposes a Single Program Multiple Data (SPMD) programming model, as opposed to a Single Instruction Multiple Data (SIMD) model. This is one of the reasons why RapidMind has an explicit <i>Program</i> object. In systems where basic operations would be executed on arrays and those operations collected into <i>program</i> objects implicitly, control flow is difficult or impossible to express, reducing the generality of the programming model as well as posing performance problems.</p>

<p>The platform provides a number of other operations that let you express almost any kind of computation. In addition to functions and operators that can be used on values, the platform provides operations that affect entire arrays. For example, array contents can be rearranged using "access patterns" and "collective computations" over an entire array, so that finding the sum of all entries can be accomplished. Table 1 lists some of the  types and functions.</p>



<div>
<table cellpadding=3 cellspacing=3>
  <tr>
    <td><b>Feature</b></td>
    <td><b>Description</b></td>
  </tr>
  <tr>
    <td>Types</td>
    <td>Value, Array, Program, Bundle, Matrix, Exception, ...</td>
  </tr>
  <tr>
    <td>Arithmetic</td>
    <td>+, -, *, /, %, ...</td>
  </tr>
  <tr>
    <td>Trigonometric &amp; Exponential</td>
    <td>sin, atan2, exp10, pow, ...</td>
  </tr>
  <tr>
    <td>Geometry</td>
    <td>dot, cross, distance, ...</td>
  </tr>
  <tr>
    <td>Logical</td>
    <td>&lt;, &gt;, &amp;&amp;, ||, cond, all, any, ...</td>
  </tr>
  <tr>
    <td>Miscellaneous</td>
    <td>lerp, join, cast, ...</td>
  </tr>
  <tr>
    <td>Control Flow</td>
    <td>RM_FOR, RM_IF, RM_WHILE, RM_DO, ...</td>
  </tr>
  <tr>
    <td>Array Access</td>
    <td>[], (), read_data, boundary, adopt_array, ...</td>
  </tr>
  <tr>
    <td>Access Patterns</td>
    <td>take, offset, shift, slice, ...</td>
  </tr>
  <tr>
    <td>Virtual Arrays</td>
    <td>grid, dice, ...</td>
  </tr>
  <tr>
    <td>Collective</td>
    <td>reduce, sum, min, max, count, exists, ...</td>
  </tr>
</table>

<div class="caption">
Table 1: Some types and functions.
</div>
</div>










<h3>Real Applications</h3>

<p>A number of applications written using the RapidMind platform illustrate the performance gains. One such example is RTT AG (www.rtt.ag), a provider of automotive visualization software. RTT's RealTrace software was built using RapidMind, and is the world's first workstation real-time raytracer. It lets accurate reflections and refractions be displayed on interactive models; see Figure 3. By leveraging GPUs to perform the complicated computations involved in raytracing, RTT was able to put a product on the market that surpasses the state-of-the-art in raytracing performance. The same code was ported to the Cell BE processor within three weeks, and shown as a demonstration in IBM's SIGGRAPH 2006 booth.</p>

<div>


<img class="illowide" src="070601sd01_f3.gif" style="width:336">

<div class="caption" style="width:338">
Figure 3: RTT's RealTrace application produces real-time, ray-traced images.
</div>
</div>


<p>With quad-core CPUs now available (and larger numbers of cores on the horizon), we are preparing for the pervasiveness of multicore systems. To this end, we recently demonstrated a prototype of a RapidMind x86 multicore backend on a financial modeling application; see Figure 4. Compared to hand-tuned C code using the Intel C++ Compiler, the RapidMind version of the algorithm was able to achieve twice the performance on a single core, scaling to eight times the performance on four cores, with no additional application effort. The increased performance on a single core may come as a surprise. It stems from the fact that the semantics of the platform let our code generators perform much better analysis and optimization of the application code. We have been careful to design the system so that we are not plagued by issues (such as pointer aliasing) that inhibit effective optimization on C or C++. At the same time, our system integrates so cleanly with C++ that all the modularity constructs of the language are available to the developer, generally at no performance cost.</p>


<div>
    
<div class="smallcap">&#91;Click image to view at full size&#93;</div>
<!-- http://i.cmpnet.com/ddj/images/article/2007/07/ -->    
<img class="illowide" src="070601sd01_f4.gif" style="width:480">

<div class="caption" style="width:482">
Figure 4: Performance comparison between a tuned C++ implementation and a RapidMind-enabled version running on an x86 multicore backend prototype.
</div>
</div>


<p>Likewise, Hewlett-Packard and RapidMind performed comparisons between CPUs and GPUs for a scientific algorithms, such as the Fast Fourier Transform (FFT) and the Basic Linear Algebra Subroutines (BLAS) single-precision matrix-multiply (SGEMM). The results showed that the RapidMind implementations on a GPU were between 2.4 and 32.2 times as fast as the same algorithm running on a CPU core. For the CPU comparisons, extremely tuned numerical libraries were used.</p>



















</body>
</html>