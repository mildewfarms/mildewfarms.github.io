<html>
<META NAME="year" CONTENT="1998">
<head>
<title>Algorithm Alley</title>
</head>

<body bgcolor="FFFFFF">
<!--Copyright &#169; Dr. Dobb's Journal-->
<h1>Sorting and Searching Linked Lists in Java</h1>

<p><i>Dr. Dobb's Journal</i> May 1998
</p>
<h3>By John Boyer</h3>

<I>John is a software-development manager at UWI.Com -- The Internet Forms Company. John is also a part-time theoretical computer-science graduate student at the University of Victoria. He can be contacted at jboyer@uwi.com. </I>

<hr>

<p>Generic collection classes, such as those in the C++ Standard Template Library and Java Developer's Kit, are nice to have around, in part because they encapsulate standard algorithms that are useful in many situations. But such libraries are hard to write -- how do you know, for instance, that the particular algorithm you chose will work well across so many applications? As you'll see upon reading John's article, even experts  such as those writing the JDK can sometimes overlook the best algorithm choices.</p>

<p>I found John's discussion of linked list searching especially interesting. Like many people, I failed to distinguish between the cost of walking a link and the cost of a key comparison. Understanding that distinction makes a big difference.  </p>

<p> -- Tim Kientzle</p>

<p>Some common misconceptions in computing are that arrays can be sorted more quickly than linked lists; that the quicksort is the fastest comparison sort on average; and that binary searching a linked list provides no benefit over a sequential search because both are <i>O(n)</i>.</p>

<p>If you agree with any of the aforementioned views, you're in good company. The list sort and search algorithms in the recently released Java Developer's Kit (JDK) 1.2 beta 2 are based on these misconceptions, which results in suboptimal  performance.</p>

<p>In this article, I'll present Java implementations (available electronically, see "Resource Center," page 3) of more-efficient list sort and search algorithms, which are based, in part, on my undergraduate thesis work in 1988. These algorithms also have the advantage that they work on both singly and doubly linked lists (although the sorts require a simple post-processing loop for doubly linked lists to reconnect all of the references to previous nodes).</p>

<h3>Performance</h3>

<p>Although I collected empirical results on a 64-MB 200-MHz Pentium running Windows 95, I also ran the tests on other machines. While the numbers change, the conclusions are the same.</p>

<p>The test program allows you to specify the number of randomly generated ten-character strings to sort or search. As you type your number, the program will not show you what you are typing. This is a bug in Java's <i>Kbd.readLine()</i> function. However, if you simply type your number on faith and press Enter, your input will be properly recognized. You can then choose a sort or search test. The sort test runs five different sort functions: the array quicksort in <i>Arrays.sort()</i>, the linked-list quicksort in <i>Collections.sort()</i>, a new linked-list quicksort, a new linked-list merge sort, and a nonrecursive version of the linked-list merge sort. The search test compares the new binary search to the sequential search in <i>Collections.binarySearch()</i>. The search tests simply use the search to find every string in the array or list.</p>

<p><a name="rf1"><a href="9805pf1.htm">Figure 1</A> shows the results of the sort tests on 16,000 strings. The limitations of <i>System.currentTimeMillis()</i> (which is only updated every 50 to 60 milliseconds on Windows 95) make it difficult to distinguish the two list merge sorts. With 32,000 strings, the Java VM froze without any errors, so I was unable to empirically test which of my two new list sorts was faster.</p>

<p>All of the new linked-list functions sort linked lists faster than <i>Arrays.sort()</i> can sort the equivalent array. Since <i>Collections.sort()</i> calls <i>Arrays.sort()</i>, naturally all of the new linked-list sorts are much faster than the JDK's linked-list sort. This was not just on average but in every test case. The most notable result is that the JDKv1.2b2 <i>Collections.sort()</i> was 33 percent slower than the recursive merge sort, and the <i>Arrays.sort()</i> function was slower than the linked-list merge sort by 22 percent. Best of all, the merge sort cannot degrade to quadratic time for any data set. Thus, it is not true that linked-list sorting must be slower than array sorting, nor is it true that the quicksort is the fastest comparison sort.</p>

<p>As <a name="rf2"><a href="9805pf2.htm">Figure 2</A> shows, the linked-list binary search was over nine times faster on average than the sequential search using 1000 to 4000 randomly generated strings. The shapes for both searches are parabolic because the searches are <i>O(n)</i>, and the test program searches for every string in the list, yielding<i> O(n</i><sup>2</sup><i>)</i> test performance. </p>

<h3>Small Errors, Big Impact</h3>

<p>When first testing <i>Collections.sort()</i>, I noticed it was getting results that were poorer than I expected -- 140 seconds for 16,000 strings. <i>Collections.sort()</i> only adds a few linear time operations to an<i> O(n log n)</i> array sort, so it shouldn't be much slower than array sorting.</p>

<p>The first step of <i>Collections.sort()</i> is to convert the linked list to an array; this was taking 136 seconds in my first test. The <i>toArray()</i> function itself turned out to be <i>O(n</i><sup>2</sup><i>),</i> apparently because the iterator's <i>next()</i> function in <i>LinkedList</i> restarted at the beginning of the list in each call. As a temporary fix, my test program subclasses <i>LinkedList</i> to overload the <i>iterator()</i> function. The new version merely returns <i>listIterator()</i>, with a substantial performance improvement.  </p>

<h3>Linked-list Quicksort</h3>

<p>It may come as something of a surprise that the quicksort can be done on a singly linked list since it is impossible to efficiently traverse backwards. In fact, I haven't encountered any other work describing a linked-list partition sort, so I believe that this is an original algorithm (though I'd be glad to hear from any reader who knows otherwise). <a name="rl1"><a href="9805p.htm#l1">Listing One</A> is Java source for this new sort.</p>

<p>The basic method uses the first node's key as the pivot. The algorithm traverses forward through the list using two references called <i>aNode</i> and <i>aNodePrev</i>. Nodes with a key value greater than or equal to the pivot are ignored. When a node containing a lesser key is encountered, the algorithm deletes <i>aNode</i> by connecting <i>aNodePrev.next</i> to <i>aNode.next</i>. Then, <i>aNode</i> is pushed onto the front of the list, becoming the new first node. Once the list is divided into two sublists, it can be sorted by recursively applying this partitioning procedure to each sublist.</p>

<p>This still leaves the quicksort's problem of quadratic performance on sorted/nearly sorted and reversed/nearly reversed data. The simplest and most popular solution to this problem is the median of three method, which can be effected in constant time per subarray partition. However, the median of three method requires linear time to scan the entire linked list prior to the partition pass. Truthfully, this really isn't such a bad idea. Since the partition already takes linear time, having the median of three method take <i>O(n) </i>time doesn't change the <i>O(n log n)</i> average case rating of the sort. The demo program (available electronically) provides a median of three function for the list quicksort.</p>

<p>But if linear time is acceptable, then it is possible to do something more elegant. In the case of the linked-list quicksort, the pivot advancement method (see <a name="rf3"><a href="9805pf3.htm">Figure 3</A>) is much shorter to code and easier to understand, yet it results in much faster performance on random data as well as <i>O(n),</i> overall sort behavior on nearly sorted and nearly reversed lists.</p>

<p>Pivot advancement starts with two references, <i>Pivot</i> and <i>aNode</i>, at the start of the list. The algorithm advances <i>aNode</i> through the list as long as it is monotonic nondecreasing, and every two advancements of <i>aNode</i> yields one advancement for the <i>Pivot</i>. If the pivot advancement loop traverses the entire sublist, then it is sorted so the function returns with no further processing. If the pivot advancement loop does not traverse the entire loop, then the pivot is the median element of the entire monotonic nondecreasing sublist at the start of the input list. Furthermore, we leave <i>aNode</i> at its given position rather than starting the partition loop at the node after the pivot because the nodes between the <i>Pivot</i> node and <i>aNode</i> are already known not to have lesser key values than the pivot.</p>

<p>Thus, with only one extra comparison per partition, pivot advancement handles nearly sorted data sets in linear time rather than quadratic time. Reversed data is also handled in linear time because nodes pulled from the second sublist are pushed onto the front of the first sublist, which reverses them into sorted order. On random data, pivot advancement outshines median of three because the former does little work in choosing a pivot at or near the beginning of the list (rather than traversing the whole list). The probability that <i>aNode</i> will be advanced in random data sets diminishes exponentially (1 in 2<sup><i>i </i></sup>chances of <i>i</i> advancements).</p>

<p>Finally, note that the linked-list quicksort presented here still has a quadratic worst case. The worst case occurs with sequences with the pattern 3,2,1,6,5,4,9,8,7... However, these worst case examples are unlikely to occur in practice, and the expected running time is <i>O(n log n)</i> with low hidden constants.</p>

<h3>Linked-List Merge Sort</h3>

<p>Conventional wisdom has it that the quicksort is quicker than all other comparison sorts, and the merge sort in particular. However, this conclusion is generally based on analyzing the merge sort's performance on an array, which is not the ideal data structure for the merge sort. The merge of two sorted arrays requires an auxiliary array, which ultimately results in the merge sort performing twice as many movement operations as the quicksort on average. By comparison, the linked-list merge operation can be done in-place with only constant memory overhead for a few temporary variables. This eliminates both of the detracting factors for the merge sort. In fact, since the merge sort performs fewer key comparisons than the quicksort on average, it is the merge sort that should be quicker when sorting linked lists.</p>

<p>As the empirical results show, the linked-list merge sort (see <a name="rl2"><a href="9805p.htm#l2">Listing Two</A>) on a linked list is so fast that it outperforms the quicksort in <i>Arrays.sort()</i> using the same data placed in an array. The list quicksort presented in this article is also slower than the list merge sort by an average 6 percent. Furthermore, the merge sort's recursive calls always divide the list evenly in half. So, given that the merge step is a simple linear time process, the merge sort can never be slower than <i>O(n log n)</i>. Finally, the guaranteed logarithmic function call depth means that the merge sort will never generate a stack overflow exception. A worst case array quicksort could generate an exception in Java with as few as 5000 elements, which is the current size limit on Java's function call stack.</p>

<h3>Nonrecursive Merge Sort</h3>

<p>It is also commonly believed that non-recursive sorts are better than recursive ones. Again, this is due to the quicksort. It is better to use a nonrecursive quicksort simply to guarantee that no stack overflow exception will occur if a worst-case data set is encountered. The merge sort doesn't have this problem, so recursion poses no threat. However, a nonrecursive version does avoid function calls, so I coded two nonrecursive merge sorts. To my surprise, neither of the nonrecursive solutions managed to outperform recursive merge sort.</p>

<p>The first nonrecursive method traverses the list merging all lists of sizes 2, then 4, 8, and so on. It properly handles uneven cases with a minimum of overhead (see <a name="rl3"><a href="9805p.htm#l3">Listing Three</A>). The second method is slightly faster, so its times are the ones reported in <a href="9805pf1.htm">Figure 1</A>. It is much more complicated. The idea was inspired by the consolidation step of the Fibonacci heap. The idea is to build an array of lists, all initially empty. Each list is responsible for holding the number of nodes in successive powers of 2. You add the nodes from the original list one at a time. Then you add nodes, one at a time, to the 2<sup>0</sup> list. That list overflows on the second addition, at which point it is moved to the 2<sup>1</sup> list, emptying the 2<sup>0</sup> list. Then, if you add another two nodes, the 2<sup>0</sup> list overflows again. But this time you have to merge it with the existing 2<sup>1</sup> list. When done, the 2<sup>1</sup> list has four elements, so it is moved to the 2<sup>2</sup> list. The procedure is conceptually similar to incrementing a binary number. If there were 15 nodes in total, then the first four lists (2<sup>0</sup>, 2<sup>1</sup>, 2<sup>2</sup>, and 2<sup>3</sup>) would be full. If you then add a 16th node, then successive merge operations would join all nodes into the 2<sup>4</sup> list, emptying all other lists in the process.</p>

<p>Though more elaborate, this method is faster than the first nonrecursive merge sort because it does not traverse any sublist twice. I also built a second merge function because this sort didn't need some of the parameters and extra work of the recursive sort's merge function. Despite this, the second nonrecursive merge sort did not run faster than the recursive merge sort. The empirical results in <a href="9805pf1.htm">Figure 1</A> show the nonrecursive merge sort to be 0.3 percent slower than the recursive version, but it is really too close to call.</p>

<h3>Sort Stability</h3>

<p>The merge sort provides stability, a feature that the quicksort doesn't provide without extra programming (including the array sort in the JDK). Stability in this case refers to a sort's ability to keep equivalent keys in the same order that they appeared in the unsorted data. The test program has a few lines of commented code that can be uncommented to demonstrate stability. The test simply adds the field <i>Pos</i> to each node. Before sorting, the numeric position of each node is recorded. After the sort, the key data and <i>Pos</i> value of each node is printed. After the merge sort, the position values for equivalent keys are always ascending, but they are in random orders for the list quicksort. To make a stable quicksort, you could include <i>Pos</i> as a secondary key, but this slows the sort and requires an extra field in the node structure. With the merge sort, stability is free.</p>

<h3>Binary Search</h3>

<p>According to the empirical results, the linked-list binary search (<a name="rl4"><a href="9805p.htm#l4">Listing Four</A>) was the biggest winner. While it's true that a binary search requires sorted order, this is often required anyway. And even when it's not, the results show that, in Java, the cost of sorting is paid for with relatively few searches (for example, about 25 searches on a 4000 element list).</p>

<p>The algorithm starts by setting <i>PartitionFirst</i> equal to the first list element and <i>PartitionSize</i> equal to the number of list nodes. In each iteration of the outer loop, the inner loop traverses to the list's middle node. If the search key is less than the key at the middle node, then the search should continue in the first half of the list. So, the algorithm merely reduces the <i>PartitionSize</i> because the <i>PartitionFirst</i> already refers to the start of the desired sublist. If the search key is greater than the middle node's key, then <i>PartitionFirst</i> is set equal to the node after the middle node, and the <i>PartitionSize</i> is set equal to the count of nodes after the middle node.</p>

<p>On average, the sequential search does <i>n</i>/2 node hops and <i>n</i>/2 comparisons. The number of node hops in a binary search is never less than <i>n</i>/2 but is almost always at or near <i>n</i>. Despite the fact that the binary search does nearly twice as many node hops, the binary search is over nine times faster than a sequential search because the binary search does only <i>log(n) </i>comparisons. The linked-list binary search pays for itself if key comparisons are more complicated than two or three node hops. This is true of many data sets -- for example, search by name requires string comparisons -- but key comparisons are especially costly in Java because <i>compareTo()</i> is a virtual function invocation.</p>

<p><b>DDJ</b></p>

<HR>
<H4><a name="l1">Listing One</H4>

<pre>// Singly Linked List Quicksortprivate Node QuickSort(Node before, Node first, int n) {
  int Num1=0, Num2=n, i=1;
  Node Pivot=first, aNode=first, aNodePrev=first;
  // Pivot Advancement 
  for (i=1; i&lt;n; i++, aNode=aNode.next) {
    if (aNode.compareTo(aNode.next) &gt; 0)
      break;
    if ((i&amp;1)==0) {
      Pivot = Pivot.next;
      Num1++;
    }
  }
  // Recognize sortedness in linear time
  if (i == n) return first;
  // Partition list by unlinking nodes with values less 
  // than the pivot and pushing them onto front of list
  for (aNodePrev = aNode; i &lt; n; i++) {
    aNode = aNodePrev.next;
    if (Pivot.compareTo(aNode) &gt; 0) {
      aNodePrev.next = aNode.next;
      aNode.next = first;
      first = aNode;
      Num1++;
    }
    else aNodePrev = aNode;
  }
  if (before!=null) before.next = first;
  Num2 = n - Num1 - 1;
  // Recurse to sort sublists
  if (Num1 &gt; 1) first = QuickSort(before, first, Num1);
  if (Num2 &gt; 1) QuickSort(Pivot, Pivot.next, Num2);
  return first;
}
</pre>
<P>
<a href="#rl1">Back to Article</A>
</P>
<H4><a name="l2">Listing Two</H4>

<pre>// Singly Linked List Recursive Merge Sortprivate void mergesort(Node before, Node F1, int N1, NodePair NP) {
  if (N1 &lt;= 1)
    NP.first = NP.last = F1;
  else {
    Node F2;
    int N2; 
    N2 = N1; N1 &gt;&gt;= 1; N2 -= N1;
    mergesort(before, F1, N1, NP);
    F1 = NP.first;
    F2 = NP.last.next;
    mergesort(NP.last, F2, N2, NP);
    F2 = NP.first;
    merge(before, F1, N1, F2, N2, NP);
  }
}
//==============================================================
private void merge(Node before, Node F1, int N1,
                   Node F2, int N2, NodePair NP) {
  Node first=null, last=null, temp=null;
  int I, J;
  first = last = F1.compareTo(F2) &lt;= 0 ? F1 : F2;
  for (I=J=0; I &lt; N1 || J &lt; N2; ) {
    if (I &lt; N1 &amp;&amp; (J&gt;=N2 || F1.compareTo(F2) &lt;= 0))
         { temp = F1; F1 = F1.next; I++; }
    else { temp = F2; F2 = F2.next; J++; }
<p></p>
    last.next = temp;
    last = temp;
  }
  if (before == null) First = first;
  else before.next = first;
  last.next = F2;
  NP.first = first;
  NP.last = last;       
}
</pre>
<P>
<a href="#rl2">Back to Article</A>
</P>
<H4><a name="l3">Listing Three</H4>

<pre>// Simpler Non-recursive Merge Sort// NOTE: Use same merge() as in Listing Two
private void nr2_mergesort() {
  int i, j, k, N1, N2;
  Node F1, F2, before;
  NodePair NP = new NodePair();
  for (i=1; i &lt; NumNodes; i&lt;&lt;=1)
    for (before=null, N1=N2=i, j=0; j+N1&lt;NumNodes; j+=i&lt;&lt;1) {
      F1 = F2 = before==null ? First : before.next;
      for (k=0; k&lt;N1; k++) 
           F2 = F2.next;
      if (N2 &gt; NumNodes-j-N1) 
          N2 = NumNodes-j-N1;     
      merge(before, F1, N1, F2, N2, NP);
      before = NP.last;
    }
}
</pre>
<P>
<a href="#rl3">Back to Article</A>
</P>
<H4><a name="l4">Listing Four</H4>

<pre>// Singly Linked List Binary Searchpublic Node binarySearch(Object SearchKey) {
  Node PartitionFirst=First, MidPtr=null;
  int  PartitionSize=NumNodes, Mid, I, Result;
  while (PartitionSize &gt; 0) {
    Mid = PartitionSize / 2;
    MidPtr = PartitionFirst;
    for (I=0; I &lt; Mid; I++)
      MidPtr = MidPtr.next;
    Result = MidPtr.compareTo(SearchKey);
    if (Result &gt; 0)
      PartitionSize = Mid;
    else if (Result &lt; 0) {
      PartitionSize -= Mid;
      PartitionFirst = MidPtr;
    }
    else return MidPtr;         
  }
  return null;
}
<p></p>
</pre>
<P>
<a href="#rl4">Back to Article</A>
</P>

<HR><I>Copyright &copy; 1998, Dr. Dobb's Journal</I><BR>

</BODY>
</html>
