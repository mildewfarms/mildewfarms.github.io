<html>
<head>
<title>Apr02: Algorithm Alley</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Unbiasing  Random Bits</h1>
<p><i>Dr. Dobb's Journal</i> April 2002</p>

<h3>By Michael Mitzenmacher</h3>

<I>
Michael is a professor of computer science at Harvard University and can be contacted at <a href="mailto:michaelm@eecs.harvard.edu">michaelm@eecs.harvard.edu</a> or <a href="http://www.eecs.harvard.edu/~michaelm/">http://www.eecs.harvard.edu/~michaelm/</a>.</I>

<hr>

<p>Most computers use a pseudorandom number generator to mimic random numbers. While such pseudorandom numbers are sufficient for many applications, they may not suffice when more secure randomness is needed, such as when you are generating a cryptographic key. For example, years ago the security of the Netscape browser was broken when people found how the seed for the pseudorandom number generator was created (see "Randomness and the Netscape Browser," by Ian Goldberg and David Wagner, <i>DDJ</i>, January 1996; <a href="http://www.ddj.com/documents/s=965/ddj9601h/9601h.htm">http://www.ddj.com/documents/s=965/ddj9601h/9601h.htm</a>).</p>
<p>When better randomness is required, software can be used to obtain randomness from the computer system, including behaviors such as disk movement, user keystrokes, mouse clicks, or sounds recorded by microphones. While many of these physical phenomena can produce random events that are hard to predict, it is not clear how to distill this randomness into something useful, such as random bits that are "0" half the time and "1" the other half. For example, you might try using the number of microseconds between keystrokes to generate random numbers, outputting a 0 if the number of microseconds is even and 1 if the number is odd. While it might be hard to accurately predict the number of microseconds between user keystrokes, it may happen that some users consistently end up with an odd number of microseconds between key strokes 70 percent of the time.</p>
<DDJADVERTISEMENT INLINE>

<p>In this article, I'll present a means of efficiently extracting random bits from a possibly biased source of bits. I focus on a simple model of a random source: It generates bits that are 0 with probability <i>p</i> and 1 with probability <i>q</i>=1<i>-p.</i> Here, <i>p</i> should be strictly between 0 and 1. Each bit is independent; that is, whether it is 0 or 1 is not correlated with the value of any of the other bits. What you want is to generate fair bits that are independent and take on values 0 and 1, each with probability 1/2. </p>

<p>For a more physical interpretation, consider this: You have a coin and would like to use it to generate random bits. Unfortunately, the coin may be dented or weighted in some way you do not know, so it might be that it comes up heads with some probability <i>p</i> that does not equal 1/2. Can you use this coin to generate fair bits? Interestingly, you can even if you do not know the value of <i>p</i>. This procedure has practical applications to software that extracts randomness from biased sources, and also leads to some fun mathematics. The approach described here is based on work by Yuval Peres (see "Iterating von Neumann's Procedure for Extracting Random Bits," <i>Annals of Statistics,</i> March 1992).</p>



<h3>Extracting Single Bits</h3>


<p>The first question to consider is how you can use the possibly biased coin to generate a single fair bit. For convenience, assume coin flips as coming up either heads or tails and the bits produced as being 0s and 1s. The key insight you need (which has been attributed to John von Neumann) is to use symmetry. Suppose you flip the coin twice. If the coin lands heads and then tails, you should output a 0. This happens with probability<i> pq</i>. If instead the coin lands tails and then heads, you should output a 1. This happens with probability <i>qp=pq</i>. In the case where the coin provides two heads or two tails, you simply start over. Since the probability you produce a 0 or 1 is the same for each pair of flips, you must be generating fair bits. This procedure does not even need to know the value of <i>p</i>.</p>


<p><A NAME="rl1"><A HREF="#l1">Listing One</A> is the process just described. The procedure looks at consecutive pairs of flips and determines if they yield a fair bit. The variable <i>NumFlips</i> represents the number of biased flips available. </p>

<p>While this function provides a good first step, it doesn't seem efficient. The function discards pairs of flips when there are two heads (probability <i>p</i><sup>2</sup>) or two tails (probability <i>q</i><sup>2</sup>=(1<i>-p</i>)<sup>2</sup>). Using calculus or a graphing calculator reveals that <i>p</i><sup>2</sup>+(1-<i>p</i>)<sup>2</sup> achieves its minimum value of 1/2 when <i>p</i>=1/2. Hence, no matter what the value of <i>p</i> is, the function throws away a pair of flips at least half of the time.</p>

<p>Suppose you define a function <i>B(p) </i>to represent the average number of fair bits per coin flip when the coin lands heads with probability <i>p</i>. You should note that 0<IMG SRC="lteq14.gif"><i>B(p)<IMG SRC="lteq14.gif"></i>1; you can't get more than one fair bit out of even a fair coin! Also, <i>B(p) </i>is meant to represent a long-term average; it ignores issues such as when you have an odd number of coin flips, the last one is useless under this scheme. For every two flips, you get a fair bit with probability 2<i>pq, </i>so<i> B(p)=pq.</i> When <i>p=q=</i>1/2, so that my coin is fair and I could conceivably extract one full fair bit per flip, <i>B(p) </i>is just 1/4.</p>



<h3>Making More Use of Symmetry</h3>


<p>A better approach also uses symmetry beyond pairs of flips. For example, suppose you flip two heads followed by two tails. In the original extraction scheme, you obtain no fair bits. But if you decide that two heads followed by two tails produces a 0, while two tails followed by two heads produces a 1, then you maintain symmetry while increasing the chances of producing a fair bit.</p>


<p>There is a nice way to visualize how to do this. Consider the original sequence of flips. You build up a new sequence of flips in the following way: Whenever you get a pair of flips that are the same in the original sequence of flips, you introduce a new flip of that type into the new sequence; see, for example, <A NAME="rt1"><A HREF="0204kt1.htm">Table 1</A>.</p>

<p>Whenever a pair of flips is heads-tails or tails-heads, you generate a fair bit using von Neumann's approach. Whenever a pair is heads-heads or tails-tails, add a new coin flip to the new sequence. After you finish with the original sequence of flips, turn to the new sequence of flips to get more fair bits. You append the fair bits from the new sequence to those produced by the original sequence. In <A NAME="rt1"><A HREF="0204kt1.htm">Table 1</A>, the final output would be 010101. The new sequence looks for the symmetry between the sequences heads-heads-tails-tails and tails-tails-heads-heads. </p>

<p>There is no reason to stop with just a single new sequence. You can use the new sequence to generate another new sequence, recursively. For example, suppose you change the previous example to <A NAME="rt2"><A HREF="0204kt2.htm">Table 2</A>. In this example, the second level produces no extra fair bits, but if you generate a further new sequence recursively, you obtain one more fair bit.</p>

<p><A NAME="rl2"><A HREF="#l2">Listing Two</A> implements the recursive variation. You can again define a function <i>B(p) </i>to represent the average number of fair bits you get per coin flip when the coin lands heads with probability <i>p</i>. For every two flips, you again get a fair bit with probability 2<i>pq</i>; this adds <i>pq</i> fair bits per coin flip, on average. If you don't get a fair bit, you get a new flip. Recall the coin gave two heads with probability<i> p</i><sup>2 </sup>and two tails with probability <i>q</i><sup>2</sup>, so on average you get <i>(p</i><sup>2</sup>+<i>q</i><sup>2</sup>)/2 additional recursive coin flips per original coin flip. Also, each coin flip at the new level is heads with probability <i>p</i><sup>2</sup>/(<i>p</i><sup>2</sup>+<i>q</i><sup>2</sup>) and tails with probability <i>q</i><sup>2</sup>/(<i>p</i><sup>2</sup>+<i>q</i><sup>2</sup>). (These values sum to 1, and preserve the proper ratio between two heads and two tails.) </p>

<p>So now you can write the appropriate equation, as in <A NAME="rf1"><A HREF="0204kf1.htm">Figure 1</A>(a). While this equation <i>B(p)</i> does not appear to have a simple closed form, you can use it to calculate specific values of <i>B(p)</i> recursively. Moreover, when <i>p=q=</i>1/2, you obtain <i>B(p)</i>=1/3, as shown in <A NAME="rf1"><A HREF="0204kf1.htm">Figure 1</A>(b). While this is an improvement for fair coins over the initial scheme, you are still far from the optimal of one fair bit per coin flip when the coin is fair. The recursive extraction removes some of the waste, but there is still more to be done. </p>



<h3>Making Even More Use of Symmetry</h3>


<p>There is another symmetry you can take advantage of. Consider when the coin lands heads-heads-heads-tails and heads-tails-heads-heads. Both sequences produce one fair bit and one flip for the next sequence in the simple recursive scheme. But you have not taken advantage of the order in which these two events happened; in the first sequence, the fair bit was produced second, and in the second sequence, it was produced first. The symmetry between these two situations can yield another fair bit.</p>


<p>An easy way to extract this extra bit is to again use the idea of creating an additional sequence of coin flips. From the original sequence, you can derive a new sequence that gives you a biased coin flip for each consecutive pair of original flips. If the two flips are the same, you get heads; if the two flips are different, you get tails. You then apply von Neumann's scheme to the new sequence as well as the original sequence; see <A NAME="rt3"><A HREF="0204kt3.htm">Table 3</A>. You can again glue together the two outputs to obtain 01010010.</p>

<p>Now you can also use the other new sequence described previously. So when you start with an original sequence, use it to derive two further sequences as in <A NAME="rt4"><A HREF="0204kt4.htm">Table 4</A>. It turns out that if you glue all the bits together, you get independent and fair bits.</p>

<p>Of course, you can go further by recursively extracting more bits from each sequence. That is, each new sequence should generate two further new sequences, and so on. This recursive construction is easy to code; see <A NAME="rl3"><A HREF="#l3">Listing Three</A>.</p>

<p>All of the procedures to this point have been designed recursively, so the bits from one sequence are output before the bits from the derived sequences are output. Is this important? In fact, you must be somewhat careful to make sure that you do not introduce correlations by ordering the bits in an unusual fashion. The recursive approach is known to be safe, so I recommend sticking with it. </p>

<p>You can again write an equation that determines the long-term average number of bits produced per flip. The equation is similar to the previous case, except now for every two flips, we also get an additional flip in one of the derived sequences. This flip is heads with probability <i>p</i><sup>2</sup>+<i>q</i><sup>2</sup>, since it comes up heads whenever the pair of flips are the same. This results in the equation in <A NAME="rf2"><A HREF="0204kf2.htm">Figure 2</A>(a). As shown in <A NAME="rf2"><A HREF="0204kf2.htm">Figure 2</A>(b), this equation yields that <i>B</i>(1/2)=1. Now if you flip a fair coin, you expect in the long run to get out one fair bit per flip using this recursive procedure. You are finally doing essentially as good as you could hope for.</p>

<p>This process extracts the maximum number of fair bits possible for every value of <i>p</i>. To prove this requires knowing some information theory. You need to know that the maximum rate at which fair bits can be produced from biased bits is given by the entropy function; see <A NAME="rf3"><A HREF="0204kf3.htm">Figure 3</A>(a). The complex looking equation has a straightforward solution; <i>B(p) </i>equals the entropy<i> H(p). </i>You can verify this by plugging <i>B(p)=H(p) </i>into the recurrence. It is easiest to first calculate term by term, and to use the fact that when <i>p+q=</i>1, you have 1-<i>p</i><sup>2</sup>-<i>q</i><sup>2</sup>=2<i>pq.</i> Suppose <i>B(p)=H(p).</i> Then you first calculate easier expressions for the terms on the right side; see <A NAME="rf3"><A HREF="0204kf3.htm">Figures 3</A>(b) and 3(c). Now check that the right side comes out to <i>H(p)=B(p)</i>; see <A NAME="rf3"><A HREF="0204kf3.htm">Figure 3</A>(d).</p>



<h3>Conclusion</h3>


<p>Although I have presented some basic psuedocode, you might find it interesting to write more efficient versions on your own. There is a collection of implementations and related links at <a href="http://www.ciphergoth.org/software/unbiasing/">http://www.ciphergoth.org/software/unbiasing/</a>, and Peres's work has been the subject of discussion on the sci.crypt newsgroup. </p>




<h3>Acknowledgment</h3>


<p>Michael's work is supported by NSF grant numbers CCR-9983832, CCR-0118701, and CCR-0121154.</p>



<p><b>DDJ</b></p>

<H4><A NAME="l1">Listing One</H4>


<pre>Function ExtractBits ( Flips[0,NumFlips-1] )
    for (j = 0; j &lt; (NumFlips-1)/2; j++) {
        if ( Flips[2*j] == Heads ) and ( Flips[2*j+1] == Tails ) print 0;
        if ( Flips[2*j] == Tails ) and ( Flips[2*j+1] == Heads ) print 1;
    }
} 
</pre>
<P>
<A HREF="#rl1">Back to Article</A>
</P>
<H4><A NAME="l2">Listing Two</H4>


<pre>Function ExtractBits ( Flips[0,NumFlips-1] )
    NumNewFlips = 0;
    for (j = 0; j &lt; (n-1)/2; j++) {
        if ( Flips[2*j] == Heads ) and ( Flips[2*j+1] == Tails ) print 0;
        if ( Flips[2*j] == Tails ) and ( Flips[2*j+1] == Heads ) print 1;
        if ( Flips[2*j] == Heads ) and ( Flips[2*j+1] == Heads) {
             NewFlips[NumNewFlips++] = Heads;
        }
        if ( Flips[2*j] == Tails ) and ( Flips[2*j+1] == Tails) {
             NewFlips[NumNewFlips++] =Tails;
        }
    }
    if (NumNewFlips &gt;= 2) ExtractBits (NewFlips[0,NumNewFlips-1]);
}
</pre>
<P>
<A HREF="#rl2">Back to Article</A>
</P>
<H4><A NAME="l3">Listing Three</H4>


<pre>Function ExtractBits ( Flips[0,NumFlips-1] )
    NumNewFlipsA = 0;
    NumNewFlipsB = 0;
    for (j = 0; j &lt; (NumFlips-1)/2; j++) {
        if ( Flips[2*j] == Heads ) and ( Flips[2*j+1] == Tails ) {
            print 0;
            NewFlipsA[NumNewFlipsA++] = Heads;
        } 
        if ( Flips[2*j] == Tails) and ( Flips[2*j+1] == Heads ) {
            print 1;
            NewFlipsA[NumNewFlipsA++] = Heads;
        }
        if ( Flips[2*j] == Heads ) and ( Flips[2*j+1] == Heads) {
            NewFlipsB[NumNewFlipsB++] = Heads;
            NewFlipsA[NumNewFlipsA++] = Tails;
        }
        if ( Flips[2*j] == Tails ) and ( Flips[2*j+1] == Tails) {
            NewFlipsB[NumNewFlipsB++] = Tails;
            NewFlipsA[NumNewFlipsA++] = Tails;
        }
    }
    if (NumNewFlipsA &gt;= 2) ExtractBits (NewFlipsA[0,NumNewFlipsA-1]);
    if (NumNewFlipsB &gt;= 2) ExtractBits (NewFlipsB[0,NumNewFlipsB-1]);
}</pre>
<P>
<A HREF="#rl3">Back to Article</A>
</P>


</body>
</html>
