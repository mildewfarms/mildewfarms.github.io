<html>
<head>
<title>Aug02: Checkpointing Multithreaded Programs</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Checkpointing Multithreaded Programs</h1>
<p><i>Dr. Dobb's Journal</i> August 2002</p>

<h3>By Christopher D. Carothers and Boleslaw K. Szymanski</h3>

<I>
Christopher and Boleslaw are professors in the Department of Computer Science at Rensselaer Polytechnic Institute. They can be contacted at chrisc@cs.rpi.edu and szymansk@cs.rpi.edu, respectively.</I>

<hr>

<p>Checkpointing is the process by which you grab snapshots of running programs. The most common use of checkpointing is in fault-tolerant computing, where the goal is to minimize loss of CPU cycles when a long executing program crashes before completion. By checkpointing a program's state at regular intervals, the amount of lost computation is limited to the interval from the last checkpoint to the time of crash. </p>
<p>Another common use of checkpointing is in the synchronization of optimistic parallel discrete-event simulations, often referred to as "state saving." In these systems, out-of-order event computations are possible because the synchronization strategy employs a detection-and-recovery scheme. To support the recovery operation, the state of each simulation object must be recorded as it is modified. A number of checkpoint algorithms have been developed to reduce the amount of memory required for checkpointing, including infrequent state saving, incremental state saving, and reverse computation.</p>
<DDJADVERTISEMENT INLINE>

<p>Our interest focuses on the fast, efficient checkpointing of threaded programs that execute on shared-memory platforms such as Linux. What motivated us to examine this topic were problems we encountered in developing parallel simulation and computation synchronization methodologies. This led us to a new algorithm for checkpointing large-scale, shared-memory multithreaded programs. Although based on Linux, the core idea and algorithmic approach is general enough for any operating system. </p>



<h3>LinuxThreads</h3>


<p>Multithreaded programs are typically implemented on Linux using threading packages such as LinuxThreads (http://pauillac .inria.fr/~xleroy/linuxthreads/) or Next Generation POSIX Threading (http://oss.software.ibm.com/pthreads/). Developed by Xavier Leroy, the LinuxThreads package is an implementation of POSIX 1003.1c Pthread interface, which provides the appearance of kernel-level threads by realizing each thread as a separate UNIX process that shares the same address space with all other threads. Scheduling between threads is handled by the kernel scheduler, just like scheduling between UNIX processes.</p>


<p>A drawback of LinuxThreads is that each thread is realized as a full kernel process. This prevents the kind of threading models where many threads can be bound to a particular kernel-level process or thread. Currently, SGI, Sun, and IBM UNIX variants support this threading model. Other problems with LinuxThreads include different process identifiers for each thread and the use of user-defined signals, which prevents programs that need both threading and user-defined signals from operating cleanly. </p>

<p>Threads in LinuxThreads are created via the <i>clone </i>system call, which lets processes be created so that they can share resources at a variety of different levels. In particular, a process and its child can be configured to share (or not) virtual memory, filesystem information, file descriptors, and signal handlers.</p>

<p>When a thread is created in LinuxThreads, a thread manager process is instantiated, which spawns the new thread using the <i>clone</i> system call. This manager thread then waits for other thread <i>create</i> requests. Additionally, it performs other thread management functions. </p>

<p>The <i>clone</i> system call can be used to checkpoint threaded programs, but requires modifications to the LinuxThreads library. In particular, the Pthread manager would have to be modified to clone itself where none of its previous resources are shared by threads. This newly cloned thread manager would then create new threads that share resources with the cloned thread manager. The members of the cloned thread group and their respective parent threads would then have to coordinate the transfer of a thread-specific state across two address spaces, such as thread stacks. The stacks could be reproduced by having the parent threads call <i>setjmp</i> to save the stack context and by having the child threads call <i>longjump</i> using the stack context pointer set by the parent threads' call to <i>setjmp</i>. Because thread stacks are realized in the heap space of the thread manager, stack copying could be avoided; however, there is a performance penalty associated with these additional system calls and thread synchronization.</p>

<p>An additional disadvantage of using <i>clone</i> for checkpointing is that implementation would be tied to LinuxThreads. Again, there are other thread packages available under Linux. Moreover, the LinuxThreads package is mated to the GNU C Library glibc and upgrading or modifying the local version of glibc is difficult and must be done with care. The problem is that such modifications risk breaking every binary in the system because of the use of shared libraries (for example, the new shared library is no longer compatible with the version your binaries were linked against). Since an OS <i>checkpoint</i> system call is not tied to a specific thread package, it is ultimately easier to implement and support.</p>



<h3>System Calls and Process Creation</h3>


<p>On the x86 port of Linux, system calls are realized by using interrupt 0x80. Internal to the OS is a jump table of system calls that relates their numbers to the specific code address where each system call begins. </p>


<p>Because the invocation of a system call is architecture specific, all top-level system call handler routines are in the ASM code directory. As a matter of convention, all system call handlers have the <i>sys_</i> prefix; for example, the <i>fork</i> system call handler is <i>sys_fork</i>. These system calls then typically invoke a more general handler routine that is not architecture specific. The prefix for those handlers is <i>do_</i>. In the case of process creation, the general handler routine for all types of processes creation (<i>fork</i>, <i>vfork</i>, or <i>clone</i>) is <i>do_fork</i>.</p>

<p>As a part of the design of a system call, the kernel always provides access to the calling process's control block or task structure by invoking the macro <i>current</i>, as well as the CPU register state, which is passed as an argument. This macro returns a pointer to the task structure that invoked this system call. Beyond system calls, <i>current</i> is the process that has control of the CPU. In the case of multiple CPUs, each CPU is running a different process; thus, <i>current</i> will be different across CPUs. The structural layout of the process control block includes variables to record scheduling priority and policy, memory management, current processor, list pointers used to place a process in a wait queue or run queue, signal handler state, filesystem information, interprocess communication information, and process-specific statistics such as CPU usage, and so on. In Linux, this is the <i>task_struct</i> structure. Within the <i>task_struct</i>, process-specific memory-management data is encapsulated into its own structure, called <i>mm_struct</i>. This data structure contains a mapped address space of the process. Thus, by switching a process from one <i>mm_struct</i> to another, its execution address space is changed. We use this feature to cleanly implement our new system call. (These structures are defined in /usr/ src/linux/include/linux/sched.h.)</p>



<h3>The Checkpoint Algorithm</h3>


<p>Our approach leverages the existing copy-on-write capability of virtual memory by introducing a new <i>checkpoint</i> system call. This new system call is similar to the <i>fork </i>and <i>clone</i> system calls. The primary difference is that <i>checkpoint</i> considers all processes that are part of a multithreaded program. The algorithm works by creating a rendezvous of all threads inside the kernel. By using a rendezvous approach, the system call guarantees that the checkpoint is made consistent. No copy of the address is made until all threads have entered the system call and ceased all user-level execution. </p>


<p>Once all threads of a program are inside the system call, the thread with the smallest process identifier is made the parent or master thread. The parent thread creates a new <i>mm_struct</i>, which is a copy of its own memory-management structure. The parent thread then makes this new structure active by setting the <i>task_struct </i>memory-management pointers to the new <i>mm_struct</i>. Meanwhile, the other threads are in a barrier waiting for the parent thread to complete creation and swap of memory-management structures. Once the copy is complete, each thread then swaps its <i>task_struct</i> memory-management pointers for the ones in the parent thread. Now, all threads are actively using the new management structure. </p>

<p>At this point, our algorithm behaves like the <i>clone</i> system call. After swapping the old memory-management structure for the new one, each thread concurrently invokes the <i>do_fork</i> routine. This routine does the work of process creation. However, each thread invokes the <i>do_fork</i> routine in such a way that it shares the current memory address space. So, each new thread created uses the new memory structure that was just allocated and made active. Once all threads complete the <i>do_fork</i> routine, each thread then swaps the new memory-management structure back for its old one. Thus, the members of the new set of threads (children) are running in the copy-on-write shared address space of their original parent threads.</p>

<p>On returning from the <i>checkpoint</i> system call, the child threads have a return value of zero, and each parent thread has a return value of the child thread that it created. At this point, each parent thread could put itself to sleep or decide to lower its priority and slowly write its state to stable storage from which the program could be restarted in the event of a hardware failure.</p>

<p>To revert to a previous checkpointed state in the multithreaded program (that is, rollback), the child threads would signal the parent threads to wake up, then kill themselves. Thus, the rollback operation is completely accomplished at the user level. The parent threads could then decide to redo the checkpoint or progress forward, depending on the needs of the application.</p>



<h3>Global Data Structures</h3>


<p><A NAME="rl1"><A HREF="#l1">Listing One</A> presents the new global data elements. The design philosophy is that correctness and robustness must be guaranteed to the greatest possible extent because this is OS-level software. In keeping with this design philosophy, we employ a multiphase approach in which a barrier synchronization among all the threads is used between each phase.</p>


<p>The first variable is <i>checkpoint_waits</i>. This array of four integers implements the various barriers between phases. The <i>checkpoint_mm_lock</i> is a lock for the <i>checkpoint_mm</i> variable, which is a pointer to the current memory-management structure being checkpointed among a group of threads. Since only one set of threads can be checkpointed at a time, <i>checkpoint_mm_lock</i> prevents another set of threads from initiating a checkpoint operation until the current set is complete. <i>checkpoint_task_lock</i> provides internal synchronization and coordination between phases. Finally, <i>checkpoint_parent_task </i>is the pointer to the thread that is master (that is, possesses the smallest process identifier) among all the threads involved in the checkpoint operation.</p>



<h3>Core Algorithm</h3>


<p>In keeping with Linux system call convention, <i>sys_checkpoint</i> is the top-level handler of the system; see <A NAME="rl2"><A HREF="#l2">Listing Two</A>. This handler routine invokes the architecture-independent routine, <i>do_checkpoint</i>, and is divided into phases: <i>admission</i>, <i>create_mm</i>, <i>clone_threads</i>, <i>restore_mm</i>, and <i>leave</i>. These phases correspond to the different parts of <A NAME="rl2"><A HREF="#l2">Listing Two</A>.</p>


<p>The <i>admission</i> phase (<A NAME="rl3"><A HREF="#l3">Listing Three</A>) determines which threads are allowed into the core parts of the <i>checkpoint</i> system call. The first part determines if there are no other threads sharing the current process's memory-management structure (that is, a single threaded/uniprocessor program). If so, the <i>checkpoint</i> system call behaves just like a <i>fork</i> system call by directly invoking the <i>do_fork</i> general handler routine. This is possible because the <i>do_fork</i> routine can handle the concurrent processing of fork system calls since shared variables are placed inside of critical sections.</p>

<p>Since setting <i>checkpoint_mm</i> signals the  existence of a group of threads in the process of checkpointing, it must be determined if the current process is with the current checkpoint group by comparing the <i>checkpoint_mm</i> variable to <i>mm</i>, the process's memory-management structure pointer. If the process is with the checkpoint thread group, then it is allowed to pass through the barrier; otherwise, it waits via the <i>schedule_timeout</i> internal routine for, say, 10 milliseconds. During this time, the Linux scheduler executes other runnable processes. This kind of barrier enables many threads bound to a single processor to be involved in a checkpoint operation. Next, if <i>checkpoint_mm</i> is not set, then this process atomically sets the variable to the address of its memory-management structure. Once a thread is admitted (moves past the first barrier), it determines the number of other threads in this thread group using the memory-management structure's <i>mm_users</i> variable. </p>

<p>The current process <i>checkpoint_counter</i> variable records the number of times this process has been checkpointed. Currently, we are special casing the first checkpoint for programs that use LinuxThreads. Since LinuxThreads creates a thread manager, the <i>mm_users</i> variable is 1 greater than the number of checkpointing threads. Consequently, we need to reduce the number of <i>mm_users</i> by 1 to keep an accurate count of the number of threads involved in the checkpoint operation. This is crucial because the subsequent barriers block until every process moves into the barrier. </p>

<p>The last part of the admission phase is the election of the parent thread followed by a task barrier. The task barrier uses independent <i>wait</i> variables because, with a large numbers of threads, it cannot be guaranteed that the last thread has left the previous barrier before the first one enters the next barrier. Lastly, these barriers only allow an atomic evaluation of the barrier condition. We took this conservative approach to ensure robustness. While it may be possible to relax this condition, a more comprehensive analysis and testing on other processors is needed before any conclusions can be made about the efficacy of this synchronization approach. </p>

<p>The <i>create</i> phase in <A NAME="rl4"><A HREF="#l4">Listing Four</A> lets the parent process allocate a new memory-management structure, then swap this new one for its original. During this allocation the other threads wait in the <i>checkpoint_wait</i> barrier, which releases them only when the parent has completed the allocation and swap of memory-management structures. Once complete, each process then swaps the original memory-management structure for the new one as well.</p>

<p>A closer look at the memory-management structure allocation/swap process reveals a number of interesting details. To create a new memory-management structure, the space is not only allocated, but also copied. After the copy, a Linux-specific initialization routine is invoked (not shown in the algorithm). After that, the virtual memory page tables are duplicated in the <i>dup_mmap</i> routine. In Linux, this operation is encapsulated in a semaphore. Last, descriptor tables that are used by the processor to perform address translation are copied in the <i>copy_segments </i>routine.</p>

<p>The swapping of memory-management structures requires that the old structure be deactivated and the new one must take its place. This is done by the <i>activate_mm</i> routine. With the new memory-management structure created, the threads enter the clone phase; see <A NAME="rl5"><A HREF="#l5">Listing Five</A>. In it, each thread creates a child thread using the <i>do_fork</i> handler routine that takes its place and utilizes the newly allocated address, which is a copy-on-write instance of the original address space. Once complete, all threads synchronize in the third barrier. Next, the original memory-management structure needs to be restored (see <A NAME="rl6"><A HREF="#l6">Listing Six</A>). The <i>restore_mm</i> completes this task by reverting back to the original memory-management structure and then reactivating it.</p>

<p>The last phase of the <i>checkpoint </i>system call is <i>leave</i>. In <A NAME="rl7"><A HREF="#l7">Listing Seven</A>, the parent waits in the fourth barrier while all other processes exit. Once all threads have left, the parent resets all global variables, which lets the next set of threads enter the system call, thus restarting the algorithm.</p>



<h3>Performance Study</h3>


<p>The computing platform we used is a dual-processor (400-MHz Pentium II) system running Linux 2.4 with 256 MB of shared RAM. In the first series of experiments, we compared the execution time of the <i>checkpoint</i> system call to a user-level memory copy method of checkpointing as a function of the number of threads and the amount of data being checkpointed. We measured performance in terms of speedup relative to memory copy (that is, memory copy execution time divided by system call execution time).</p>


<p><A NAME="rf1"><A HREF="0208df1.htm">Figure 1</A> shows that the speedup for the two-thread case varies from 25 to 67. Speedup is attributed to the efficiency of copy-on-write semantics of the underlying virtual memory system. Interestingly, nonlinear speedup behavior is observed. For instance, there is a large drop off in speedup when the data size changes from 8 MB to 16 MB, then a sharp increase at 32 MB followed by a sharp decrease at 64 MB. While we don't completely understand the cause of this nonlinear behavior, we hypothesize that it is due to differences in the amount of data copied between memory copy and checkpoint at the various data points. However, a more thorough performance analysis of the Linux virtual memory subsystem is required before any definitive conclusions can be drawn. </p>

<p>Next, <A NAME="rf2"><A HREF="0208df2.htm">Figure 2</A> shows the speedup results for the four- and eight-thread cases. There are between two to four times more threads than processors. Thus, each thread context switches several times during the processing of the system call and generates much greater overheads. Because of this, there's a significant drop in speedup, particularly for small checkpoint data sizes. However, what is surprising is that at 32- and 64-MB data sizes, the speedup results are above four for the four-thread case and above two for the eight-thread case.</p>

<p>As indicated in the first series of performance results, high speedups are attributed to the copy-on-write semantics of the underlying virtual memory system. To better understand how these raw system-call performance statistics would translate into overall start-to-finish program performance, we conducted a full program performance test where the start-to-finish execution time of a synthetic workload program was measured. The workload program consists of two threads and 64 MB of data. The synthetic threaded program performs 10 checkpoint operations of system using either memory copy or the <i>checkpoint</i> system call. In between the checkpoints, the amount of modified data is varied from 4 KB to 1 MB.</p>

<p>The total execution of the program using the <i>checkpoint</i> system call takes around one second with a small increase in total execution time as the amount of modified data is increased. However, we saw that the memory copy execution time remains unchanged regardless of how much data is modified. When execution times are translated into speedup results, we see that overall program performance is increased by a factor of eight for small data sizes and a factor of five for the 1-MB data size when the new system call is used. We have observed that when the amount of modified data approaches the total amount of data in the program, the execution time is the same for both memory copy and the <i>checkpoint</i> system call.</p>



<h3>Acknowledgments</h3>


<p>This work was partially supported by DARPA contract #F30602-00-2-0537 with the Air Force Research Laboratory (AFRL/IF) and by a grant from the University Research Program of Cisco Systems Inc. </p>


<p></p>


<p><b>DDJ</b></p>

<H4><A NAME="l1">Listing One</H4>


<pre>int checkpoint_waits={0,0,0,0}
pid_t checkpoint_min_pid=0x7fffffff
spinlock_t checkpoint_mm_lock=
  SPIN_LOCK_UNLOCKED
struct m_struct*checkpoint_mm=NULL
  struct_task_lock = SPIN_LOCK_UNLOCKED
struct tast_struct*
  checkpoint_parent_task=
  NULL
</pre>
<P>
<A HREF="#rl1">Back to Article</A>
</P>
<H4><A NAME="l2">Listing Two</H4>


<pre>sys_checkpoint( regs )
{
  return(do_checkpoint(regs));
}

do_checkpoint(regs)
{
  admission();
  if( create_mm( regs ) == error )
    return( error );
  clone_threads(regs);
  restore_mm();
  leave();
}
</pre>
<P>
<A HREF="#rl2">Back to Article</A>
</P>
<H4><A NAME="l3">Listing Three</H4>


<pre>admission()
{
  old_mm = current-&gt;mm;
  if( current-&gt;mm-&gt;mm_users == 1 ) 
    {
      clone_flags = SIGCHLD;
      return(do_fork(clone_flags, regs));
    }
  
  spin_lock(&amp;checkpoint_mm_lock);
  
  while( checkpoint_mm != NULL &amp;&amp; 
	 checkpoint_mm != old_mm )
    {
      spin_unlock(&amp;checkpoint_mm_lock);
      schedule_timeout(1);
      spin_lock(&amp;checkpoint_mm_lock);
    }
  
  if( checkpoint_mm == NULL )
    checkpoint_mm = old_mm;

  spin_unlock(&amp;checkpoint_mm_lock);
  
  if( current-&gt;checkpoint_counter == 0 )
    {
      mm_users = current-&gt;mm-&gt;mm_users - 1;
    }
  else
    {
      mm_users = current-&gt;mm-&gt;mm_users;
    }
  
  spin_lock(&amp;checkpoint_task_lock);
  
  if( current-&gt;pid &lt; checkpoint_min_pid )
    {
      checkpoint_min_pid = current-&gt;pid;
    }
  
  checkpoint_waits[0]++;
  
  while( checkpoint_waits[0] &lt; mm_users )
    {
      spin_unlock(&amp;checkpoint_task_lock);
      schedule_timeout(1);
      spin_lock(&amp;checkpoint_task_lock);
    }
  
  spin_unlock(&amp;checkpoint_task_lock);
}
</pre>
<P>
<A HREF="#rl3">Back to Article</A>
</P>
<H4><A NAME="l4">Listing Four</H4>


<pre>create_mm(regs)
{
  if( current-&gt;pid == checkpoint_min_pid )
    {
      checkpoint_parent_task = current;
      parent = current;
      if(( new_mm = allocate_mm() ) == NULL )
	{
	  notify_other_threads_of_error;
	  reset_ global_variables;
	  return(error);
	}

      memcpy(new_mm, parent-&gt;mm);
      dup_mmap(new_mm);
      copy_segments(new_mm);
      old_mm = parrent-&gt;mm;
      parent-&gt;mm := new_mm;
      parent-&gt;active_mm = new_mm;
      activate_mm(old_mm, new_mm);        
      spin_lock(&amp;checkpoint_task_lock);   
      checkpoint_waits[1] = 1;
      spin_unlock(&amp;checkpoint_task_lock);  
    }
  else
    {
      spin_lock(&amp;checkpoint_task_lock);  
      
      while( checkpoint_waits[1] == 0 )
	{
	  spin_unlock(&amp;checkpoint_task_lock);
	  if( parent_detects_error )
	    {
	      return(error);
	    } 
	  schedule_timeout(1);                  
	  spin_lock(&amp;checkpoint_task_lock);   
	}
      spin_unlock(&amp;checkpoint_task_lock);   
      parent = checkpoint_parent_task;          
      old_mm = current-&gt;mm;  
      current-&gt;mm = parent-&gt;mm; 
      current-&gt;active_mm = parent-&gt;mm; 
      current-&gt;mm-&gt;mm_users++; 
      activate_mm(old_mm, current-&gt;mm); 	
    }
}
</pre>
<P>
<A HREF="#rl4">Back to Article</A>
</P>
<H4><A NAME="l5">Listing Five</H4>


<pre>clone_threads(regs)
{
  current-&gt;checkpoint_counter++; 
  clone_flags = (CLONE_VM | SIGCHLD); 
  retval = do_fork(clone_flags, regs);
  current-&gt;checkpoint_counter--; 

  spin_lock( &amp;checkpoint_task_lock); 

  while( checkpoint_waits[2] &lt; mm_users )
    {
      spin_unlock(&amp;checkpoint_task_lock); 
      schedule_timeout(1);		      	
      spin_lock(&amp;checkpoint_task_lock); 
    }
  spin_unlock( &amp;checkpoint_task_lock); 
}
</pre>
<P>
<A HREF="#rl5">Back to Article</A>
</P>
<H4><A NAME="l6">Listing Six </H4>


<pre>restore_mm(regs)
{
  new_mm = current-&gt;mm; 
  current-&gt;mm = old_mm;   
  current-&gt;active_mm = old_mm; 
  new_mm-&gt; mm_users--; 
  activate_mm(new_mm, old_mm); 
}
</pre>
<P>
<A HREF="#rl6">Back to Article</A>
</P>
<H4><A NAME="l7">Listing Seven</H4>


<pre>leave(regs)
{
  if( parent == current )
    {
      spin_lock(&amp;checkpoint_task_lock); 
      while( checkpoint_waits[3] != mm_users -1 )
	{
	  spin_unlock(&amp;checkpoint_task_lock); 
	  schedule_timeout(1); 
	  spin_lock(&amp;checkpoint_task_lock); 
	}

      checkpoint_min_pid = 0x7fffffff; 
      checkpoint_waits = {0,0,0,0}; 
      checkpoint_parent_task = NULL;

      spin_lock(&amp;checkpoint_mm_lock); 
      checkpoint_mm = NULL; 
      spin_unlock(&amp;checkpoint_mm_lock); 
      spin_unlock(&amp;checkpoint_task_lock); 
    }
  else
    {
      spin_lock(&amp;checkpoint_task_lock); 
      checkpoint_waits[3]++; 
      spin_unlock(&amp;checkpoint_task_lock); 
    }
}
</pre>
<P>
<A HREF="#rl7">Back to Article</A>
</P>


</body>
</html>
