<html>
<head>
<title>Aug02: Algorithm Alley</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Star  Encoding</h1>
<p><i>Dr. Dobb's Journal</i> August 2002</p>

<h3>By Mark R. Nelson</h3>

<I>
Mark is a DDJ contributing editor. You can reach Mark at markn@ieee.org or via his web site at http://www.dogma.net/markn/.</I>

<hr>

<p>Transformation algorithms are an interesting class of data-compression techniques in which you can perform reversible transformations on datasets to increase their susceptibility to other compression techniques. For instance, the Burrows-Wheeler Transform (see my article "Data Compression with the Burrows-Wheeler Transform," <i>DDJ</i>, September 1996) is a completely reversible sorting algorithm that can create long runs of identical characters in the output text. Using a move-to-front algorithm followed by Huffman or arithmetic coding results in compression that outperforms all but the best algorithms. Likewise, in the JPEG compression algorithm, 8&times;8 blocks of pixels can be run through the Discrete Cosine Transform (DCT), which transforms the spatial data points to the frequency domain. At that point, lossy compression algorithms can then be effectively applied.</p>
<p>The transformations I describe here generally don't compress data at all; instead, they prep the data before compression is applied. Thus, a key factor in using any transform is understanding what compression technique can effectively be used on the output. With the Burrows-Wheeler Transform (BWT), the best results come from a three-stage compressor. The output of the BWT transform is usually piped through a move-to-front stage, then a run-length encoder stage, and finally an entropy encoder (normally arithmetic or Huffman coding). </p>
<DDJADVERTISEMENT INLINE>



<h3>Star Encoding</h3>


<p>BWT is a sorting algorithm that radically changes the nature of an input file. Star encoding is simpler to understand and can be implemented easily with the help of the C++ Standard Library.</p>


<p>Star encoding works by creating a large dictionary of commonly used words expected in the input files. The dictionary must be prepared in advance and must be known to both the compressor and decompressor. Each word in the dictionary has a star-encoded equivalent, in which as many letters as possible are replaced by the "*" character. For example, a commonly used word such as "the" might be replaced by the string "t**". The star-encoding transform simply replaces every occurrence of the word "the" in the input file with "t**". </p>

<p>Ideally, the most common words have the highest percentage of "*" characters in their encodings. If done properly, this means that the transformed file will have a huge number of "*" characters. This ought to make the transformed file more compressible than the original plain text.</p>

<p><A NAME="rf1"><A HREF="0208if1.htm">Figure 1</A>(a), for example, is a section of text from Project Gutenberg's version of <i>Romeo and Juliet</i>. Running this text through the star encoder yields <A NAME="rf1"><A HREF="0208if1.htm">Figure 1</A>(b). You can see that the encoded data has exactly the same number of characters, but is dominated by stars. It certainly looks as though it is more compressible. But before you can test the compressibility of the output data, you need to build programs to create the dictionary, star encode a file using a dictionary, and decode the file using an identical dictionary.</p>



<h3>Creating the Dictionary</h3>


<p>MakeDictA.cpp (available electronically; see "Resource Center," page 5) creates the dictionary. MakeDictA is invoked with a list of filenames as command-line arguments. It scans through each file looking for tokens, adds them to the dictionary if needed, and updates the token's frequency count.</p>


<p>Once the tokens have been counted, the list is sorted by frequency. The program then goes through the list, starting with the most frequent tokens, and assigns them star codes. As a final step, the dictionary is written to standard output.</p>



<h3>Tokenizing and Scanning</h3>


<p>Unfortunately, the C++ Standard Library doesn't have a stream tokenizer. If my needs were more complicated, I might use the Boost Tokenizer (http://boost.org/libs/ tokenizer/index.htm), but in this case, I simply wrote the code shown in <A NAME="rl1"><A HREF="#l1">Listing One</A>. This function is called repeatedly from <i>main()</i>. The <i>map&lt;string,int&gt;</i> object named <i>frequency</i> is updated as each token is encountered. If the string is not found in <i>frequency</i>, it is inserted with a count of 1. Otherwise, the count is incremented; see <A NAME="rl2"><A HREF="#l2">Listing Two</A>.</p>




<h3>Sorting and Assigning Codes</h3>


<p>After all the tokens from all the input files have been scanned, you have a map with an appearance count for every word encountered in the input text. The next step requires that you iterate through all the words, starting at the most frequent and going all the way to the least.</p>


<p>Unfortunately, the frequency map is keyed on the word name, not the frequency. So before code assignment can start, you need to reorder the data. You can do this with just a few lines of code by inserting all the string/integer pairs into a <i>map&lt;int,string&gt;</i> object called <i>counts</i>. Since this map is keyed on the integer value instead of the string, the values are automatically sorted in the form you want; see <A NAME="rl3"><A HREF="#l3">Listing Three</A>.</p>

<p>With the data sorted correctly, the remaining piece of hard work is to iterate over the <i>counts</i> map and assign a code to each one, as in <A NAME="rl4"><A HREF="#l4">Listing Four</A>. A couple of interesting things to note show up in this loop. First, each time a new code is assigned to a token, the code value is stored in a <i>set&lt;string&gt;</i> called <i>used_codes</i>. You have to keep track of the used code values during the assignment process so that no star code is inadvertently used twice.</p>

<p>Second, you can see that the value of each token and its code is being written to standard output. (I write the frequency count as well, although this is technically not needed. It helps me to keep an eye on the process and ensure it is working as desired.) </p>



<h3>The Code Assignment Algorithm </h3>


<p>In MakeDictA, I perform a star-code assignment in a routine called <i>create_star_code()</i>. This algorithm uses a greedy heuristic to assign codes, attempting to use the highest possible number of "*" characters to each token; see <A NAME="rl5"><A HREF="#l5">Listing Five</A>. For a token of length <i>N</i>, the assignment routine first attempts to replace all characters in the token with stars, then all but one, then all but two, and so on. The often overlooked <i>next_permutation</i> function from the C++ Standard Library helps with this.</p>




<h3>Executing  MakeDictA</h3>


<p>To test this program, I went to Project Gutenberg (http://promo.net/pg/) and retrieved the text of eight Shakespeare plays (also available electronically). With allowances for line breaks, the text below shows the command line I used to create the dictionary file:</p>


<blockquote><p>
[c:\star] MakeDictA text\AsYouLikeIt.txt text\Hamlet.txt text\JuliusCaesar.txt text\KingLear.txt text\Macbeth.txt text\RomeoAndJuliet.txt text\TheTamingOfTheShrew.txt text\TheTempest.txt &gt; DictA.txt</p>

</blockquote>

<p></p>

<p>The program creates 16,927 codes in fairly short order, producing DictA.txt (available electronically) shown here in a somewhat abridged form:</p>


<blockquote><p>
the *** 4556</p>

<p>
I * 4390</p>

<p>and **d 3676</p>

<p>to ** 3218</p>

<p>of *f 2899</p>

<p>you **u 2775</p>

<p>a a 2714</p>

<p> ...[skipping 8500 lines]...</p>

<p>vexations ****t**n* 1</p>

<p>vestall **s*a** 1</p>

<p>versall **r**l* 1</p>

<p>verities *** i***s 1</p>

<p>verge v*r** 1</p>

<p> ...[skipping 8500 lines]...</p>

<p>'Twentie *T****** 1</p>

<p>'Saue *S*** 1</p>

<p>'Prethee *P****** 1</p>

<p>'Pre *P** 1</p>

<p>'Boue *B***</p>


</blockquote>

<p>With this somewhat limited vocabulary, you can see that even words that appear very rarely are replaced with a very high percentage of stars. The most frequent words are replaced completely.</p>



<h3>Compressing and Decompressing  The Transformed Data</h3>


<p>StarEncode.cpp and StarDecode.cpp (both available electronically) transform text files to and from the star-encoded format. Both programs are invoked with the name of the dictionary file on the command line. The programs then transform standard input to standard output. Internally, they simply read the dictionary file into a <i>map&lt;string,string&gt;</i> object that holds all the encodings. They then read in text, passing special characters through unchanged, and translating tokens whenever possible.</p>


<p><A NAME="rf2"><A HREF="0208if2.htm">Figure 2</A> is the encoding program in operation, as it encodes <i>Romeo and Juliet</i> using the dictionary created earlier. The result of this encoding should be a file that is now more compressible than the original. To test this theory, I compressed both the original and encoded file in WinZip, with the results shown in <A NAME="rf3"><A HREF="0208if3.htm">Figure 3</A>. The results are encouraging. The compressed file decreases in size by almost 12 percent, with the compression ratio improving from 60 percent to 64 percent &#151; tangible savings with only a small increase in computation. </p>



<h3>Variant B</h3>


<p>While building the encoder, I started wondering about the structure of my star codes. It seemed kind of arbitrary to insist that every star code have the same number of stars as the word it was encoding. For example, "the" (the most frequent word) was encoded as "***," while "I" (the second most frequent word) was encoded with a single star. Shouldn't the most frequent word get the fewest number of stars?</p>


<p>To test this theory, I created a second dictionary builder using a new heuristic for assigning codes. In this heuristic, a star code is simply a single-star character concatenated with a plain-text code number, with the most frequent token being assigned code 0.</p>

<p>Using this scheme simplified my encoder. The main loop that assigns codes to tokens no longer needed to keep track of used codes and was simple enough to do code assignment inside the loop, instead of in a separate function; see <A NAME="rl6"><A HREF="#l6">Listing Six</A>. Comparing the output from Variant B to that described earlier shows how the two strategies create quite different dictionaries:</p>

<blockquote><p>
the *0 4556</p>

<p>
and *1 3676</p>

<p>to *2 3218</p>

<p>of *3 2899</p>

<p>you *4 2775</p>

<p> ...[skipping 8500 lines]...</p>

<p>vexations *8480 1</p>

<p>vestall *8481 1</p>

<p>versall *8482 1</p>

<p>verities *8483 1</p>

<p>verge *8484 1</p>

<p> ...[skipping 8500 lines]...</p>

<p>'Twentie *16899 1</p>

<p>'Saue *16900 1</p>

<p>'Prethee *16901 1</p>

<p>'Pre *16902 1</p>

<p>'Boue *16903 1</p>


</blockquote>

<p>One difference, in this case, is that I don't bother to encode strings of just a single character in length, as their encoded values would always be larger. </p>

<p>To test this variant, I compiled MakeDictB.cpp, created DictB.txt, and ran RomeoandJuliet.txt through StarEncode.cpp; see <A NAME="rf1"><A HREF="0208if1.htm">Figure 1</A>(c). While the text is somewhat harder to read, it is more compact. The character set is much more limited here because every word is star encoded using only the star character and the integers.</p>



<h3>Comparing A and B</h3>


<p><A NAME="rf4"><A HREF="0208if4.htm">Figure 4</A> is the listing of a Zip file that contains both encoded texts plus the original. In this case, the difference between a standard Zip compressor and Variant B runs nearly 15 percent, quite a nice savings. You can see that Variant B comes out ahead, with a delta that seems worth going for.</p>




<h3>Recommendations and Caveats</h3>


<p>Star encoding is a handy way to squeeze some additional fluff out of your files. Of course, there is a catch. For star encoding to be effective, you have to know in advance what the vocabulary of the source files is going to be. There are many times when this is the case; for example, computer-generated reports, e-mail or newsgroup archives, trace or log files, cached web pages, and the like. Still, I doubt star encoding could be used in a general-purpose compressor. A hypothetical archiver called "StarZip" might attempt to create a good dictionary and tokenizer for various file types, but the task seems difficult.</p>


<p>While the code presented could form the basis for production code, it needs some hardening. For example, the programs presented here can't cope with something as simple as a star character embedded in the plain text. Caveat emptor.</p>

<p>Star encoding was developed by a team at the University of Central Florida working under Professor Amar Mukherjee. The research that led to this work was supported by the National Science Foundation.</p>



<h3>For More Information</h3>


<p>Franceschini, Robert and Amar Mukherjee, "Data Compression Using Encrypted Text," <i>Proceedings of the Third Forum on Research and Technology</i>, Advances on Digital Libraries, 1996.</p>


<p>Franceschini, Robert et al., "Lossless, Reversible Transformations that Improve Text Compression Ratios" (http://vlsi.cs.ucf .edu/datacomp/papers/textcomp.doc).</p>

<p>The VLSI and Data Compression Lab at the University of Central Florida (http://vlsi.cs.ucf.edu/</p>



<p><b>DDJ</b></p>
<H4><A NAME="l1">Listing One</H4>


<pre>string get_token( ifstream &amp;file )
{
    string token;
    while ( file )
    {
        char c;
        file &gt;&gt; c;
        if ( isalpha( c ) || c == '\'' )
            token += c;
        else if ( token.size() )
            return token;
    }
    return token;
}
</pre>
<P>
<A HREF="#rl1">Back to Article</A>
</P>
<H4><A NAME="l2">Listing Two</H4>


<pre>string token = get_token( infile );
if ( token.size() == 0 )
  break;
map&lt; string, int &gt;::const_iterator ii = frequency.find( token );
if ( ii == frequency.end() )
  frequency[ token ] = 1;
else
  frequency[ token ]++;
</pre>
<P>
<A HREF="#rl2">Back to Article</A>
</P>
<H4><A NAME="l3">Listing Three</H4>


<pre>multimap&lt;int, string&gt; counts;
for ( map&lt;string, int&gt;::iterator ii = frequency.begin() ; 
      ii != frequency.end();
      ii++ )
  counts.insert( pair&lt;int,string&gt;( (*ii).second, (*ii).first ) )
</pre>
<P>
<A HREF="#rl3">Back to Article</A>
</P>
<H4><A NAME="l4">Listing Four</H4>


<pre>set&lt;string&gt; used_codes;
for ( multimap&lt;int, string&gt;::reverse_iterator jj = counts.rbegin() ; 
      jj != counts.rend() ; 
      jj++ )
{
  string code = create_star_code( (*jj).second, used_codes );
  used_codes.insert( code );
  codes[ (*jj).second ] = code;
  cout &lt;&lt; (*jj).second &lt;&lt; " " &lt;&lt; code &lt;&lt; " " &lt;&lt; (*jj).first &lt;&lt; endl;
}
</pre>
<P>
<A HREF="#rl4">Back to Article</A>
</P>
<H4><A NAME="l5">Listing Five</H4>


<pre>for ( int star_count = token.size() ; star_count &gt; 0 ; star_count-- )
{
  vector&lt;char&gt; pattern( token.size(), '*' );
  for ( int i = star_count ; i &lt; token.size() ; i++ )
    pattern[ i ] = '-';
  do 
  {
    string test( token );
    for ( int j = 0 ; j &lt; token.size() ; j++ )
      if ( pattern[ j ] == '*' )
        test[ j ] = '*';
    set&lt;string&gt;::const_iterator kk = used_codes.find( test );
    if ( kk == used_codes.end() )
      return test;
  } while ( next_permutation( pattern.begin(), pattern.end() ) );
}
return token;
</pre>
<P>
<A HREF="#rl5">Back to Article</A>
</P>
<H4><A NAME="l6">Listing Six</H4>


<pre>int star_code = 0;
for ( multimap&lt; int, string &gt;::reverse_iterator jj = counts.rbegin() ; 
      jj != counts.rend() ; 
      jj++ )
{
  string token = (*jj).second;
  if ( token.size() &gt; 1 )
  {
    cout &lt;&lt; token 
         &lt;&lt; " " 
         &lt;&lt; '*' 
         &lt;&lt; star_code++ 
         &lt;&lt; " " 
         &lt;&lt; (*jj).first 
         &lt;&lt; endl;
  }
}

</pre>
<P>
<A HREF="#rl6">Back to Article</A>
</P>


</body>
</html>
