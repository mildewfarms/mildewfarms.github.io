<html><head><title>Jun02: Algorithm Alley</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h1>Enhancing  Newton's Method</h1><p><i>Dr. Dobb's Journal</i> June 2002</p><h3>By Namir Clement Shammas</h3><I>Namir is a technical writer specializing in application and SDK documentation. He can be reached at <a href="mailto:nshammas@aol.com">nshammas@aol.com</a>.</I><hr><p>Many numerical analysis problems involve solving for the roots of nonlinear equations. Numerical analysis offers various kinds of algorithms to perform this task. In this article, I examine the Newton-Raphson method and show how you can enhance it.</p><p>Solving for the root of a single nonlinear function is affected by the curvature, maxima, minima, and oscillation of that function. In the case of functions that are not smooth, you are better off using root-bracketing methods like the Bisection or False Position. These methods start with a range of values that contains the root and systematically narrow that range around the root value. While these methods are robust in yielding an answer, there is a cost involved. Typically, root-bracketing methods require more iterations than other methods, such as the Newton-Raphson method (which I'll simply call "Newton's method"). In the case of smooth functions, you can use algorithms that offer faster convergence, such as Newton's method. The source code that implements Newton's method, along with executables and output text files, is available electronically; see "Resource Center," page 5.</p><DDJADVERTISEMENT INLINE><h3>Back to Basics</h3><p>Newton's method is based on the Taylor expansion, see <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(a), where <i>r</i>, <i>x</i>, <i>f(x)</i>, <i>f'(x)</i>, and <i>f''(x)</i> are the root, the guess for the root, the function, the function's first derivative, and the function's second derivative, respectively. Newton's method uses the first two terms on the right side of the Taylor expansion to yield <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(b), which has proven to be an efficient and popular root-seeking algorithm. The weakness of Newton's method appears when the slope of the function near the root and/or its guess is very small. This kind of slope value causes the next guess for the root to shoot far off from the vicinity of the actual root value.</p><p>It is often easier to calculate the first derivative numerically using the approximation in <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(c), where <i>h </i>is a small increment value. Combining <A NAME="re1"><A HREF="0206ie1.htm">Examples 1</A>(b) and 1(c) yields <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(d), which requires evaluating the function at three values; namely, the guess and two neighboring values near the guess.</p><p>Of course, you can estimate the value of the function's second derivative using <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(e), which indicates that estimating the second derivative requires evaluating the function at three values; namely, the guess and two neighboring values near the guess. With this knowledge, you can go back to <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(a) and include the third term that contains the second derivative, since there are no extra function calls involved to estimate the second derivative. Since <i>f(r)</i> is zero, you can rewrite <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(a) as <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(f). Including the second derivative in the root-seeking algorithm lets that algorithm take the function's curvature (offered by the value of the second derivative) into account. <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(f) is essentially a quadratic equation for the factor <i>(r-x)</i>. You can solve the quadratic equation and write the result as <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(g).</p><h3>A Second Approach</h3><p>Another way to solve <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(g) yields <A NAME="re2"><A HREF="0206ie2.htm">Example 2</A>, which shows the root <i>r</i> on both sides of the equation. To use this equation, employ <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(d) to calculate an estimate for the value for <i>r</i> on the right-hand side of the equation.</p><p>Why employ <A NAME="re1"><A HREF="0206ie1.htm">Examples 1</A>(g) and 2, you ask? When I first came up with <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(g) (which I call the "Quad Newton method") and implemented it in a prototype application, the algorithm gave mixed results. On one hand, it offered faster convergence than Newton's method. On the other hand, the discriminate value was often negative, yielding run-time errors. By contrast, <A NAME="re2"><A HREF="0206ie2.htm">Example 2</A> (which I will call the "Enhanced Newton method") is more stable despite the fact that it requires estimating the value for <i>r</i> using <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(d). I fine-tuned the algorithm by using the steps in <A NAME="rf1"><A HREF="0206if1.htm">Figure 1</A>, which shows the pseudocode that involves calculating intermediate values for <i>r</i>. The pseudocode shows that <A NAME="re2"><A HREF="0206ie2.htm">Example 2</A> is applied twice after applying <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(d). The algorithm does not make additional calls to the function beyond <i>f(x)</i>, <i>f(x+h)</i>, and <i>f(x-h)</i>. The implementation for the algorithm stores the values returned by these function calls and then repeatedly recalls these values when needed.</p><h3>The Test Code</h3><p>Root.cpp (available electronically) contains the implementation of the two versions of the Enhanced Newton method. The listing declares a set of five math functions, the class <i>Root</i>, the function <i>main</i>, and a few auxiliary functions.</p><p>The class <i>Root</i> declares:</p><ul>  <li>The data member <i>m_nMaxIter</i> stores the maximum number of iterations.  <li>The data member <i>m_nIter</i> stores the number of iterations.  <li>The data member <i>m_fToler</i> stores the tolerance used to determine convergence.  <li>The constructor that initializes the data members.  <li>The member function <i>getIters</i> returns the number of iterations.  <li>The member function <i>Newton</i> implements Newton's algorithm.  <li>The member function <i>ExRoot</i> implements the Enhanced Newton method (<A NAME="re2"><A HREF="0206ie2.htm">Example 2</A>).  <li>The member function <i>QuadRoot</i> implements the Quad Newton method; <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(g). This member function returns 1.0e+100 if the value of the discriminate is negative.</ul><p>The function <i>main</i> creates <i>objRoot</i> as an instance of class <i>Root</i>. The function tests the various math functions and uses the auxiliary function output to obtain the roots and generate output. The function output writes results to the console, the text file root.dat (which closely echoes the console output), and the comma-delimited text file rootCDD.dat.</p><h3>The Results</h3><p><A NAME="rt1"><A HREF="0206it1.htm">Table 1</A> shows the results of finding one of the roots of the equation <i>exp(x)-</i>3<i>*x</i><sup>2</sup> (plotted in <A NAME="rf2"><A HREF="0206if2.htm">Figure 2</A>). This function has three roots near -0.45, 0.91, and 3.73. The table shows that the Enhanced Newton method (using <A NAME="re2"><A HREF="0206ie2.htm">Example 2</A>) was faster than the traditional Newton method. The Quad Newton algorithm, <A NAME="re1"><A HREF="0206ie1.htm">Example 1</A>(g), also outperformed Newton's method and, in most cases, was up to par with the Enhanced Newton method. Using the improved algorithms pays off in this case because of the significant curvature in the range of the root and guesses.</p><p><A NAME="rt2"><A HREF="0206it2.htm">Table 2</A> shows the results of finding one of the roots of the equation <i>cos(x)-x</i> (plotted in <A NAME="rf3"><A HREF="0206if3.htm">Figure 3</A>). The table shows that the Enhanced Newton method takes one or two iterations less than Newton's method to obtain the root. The table also shows that the Quad Newton algorithm failed in finding the root of the function. The function <i>cos(x)-x</i> has a smooth curvature, giving a little advantage for the Enhanced Newton method over Newton's method. </p><p><A NAME="rt3"><A HREF="0206it3.htm">Table 3</A> shows the results related to solving for the root of function <i>(x+</i>15<i>)* (x+</i>10<i>)*(x+</i>20<i>)*(x-</i>4.5<i>)</i>. <A NAME="rf4"><A HREF="0206if4.htm">Figure 4</A> shows a plot for this function. The initial guesses direct the methods to zoom in on the root at 4.5. All the methods work well. The Enhanced Newton and Quad Newton methods are consistently one iteration ahead of Newton's method. While the function has multiple roots and inflection points, its curvature is smooth near the root at 4.5. This smooth curvature gives the improved algorithms a modest advantage.</p><p><A NAME="rt4"><A HREF="0206it4.htm">Table 4</A> contains the results for the root of the function <i>cosh(x)</i><sup>2</sup><i>+sinh(x)</i><sup>2</sup>+2<i>* x-</i>11. <A NAME="rf5"><A HREF="0206if5.htm">Figure 5</A> shows a plot of this function. Again, all of the methods work well. The Enhanced Newton and Quad Newton methods are significantly ahead of Newton's method for the initial guesses of 0.5 and 1, respectively. As the initial guesses increase, the new methods are one iteration ahead of Newton's method.</p><p><A NAME="rt5"><A HREF="0206it5.htm">Table 5</A> holds the results for solving the root of function 100<i>-x-x</i><sup>2</sup>/2<i>-x</i><sup>3</sup>/3<i>-x</i><sup>4</sup>/4. <A NAME="rf6"><A HREF="0206if6.htm">Figure 6</A> shows a plot of this function. The Enhanced Newton method and the classical Newton method zoom in on the root at 4.03105, while the Quad Newton method zooms in on the root at -4.77164. The Enhanced Newton method leads significantly over Newton's method because of the function's significant curvature near the root.</p><h3>Conclusion</h3><p>The Enhanced Newton and Quad Newton methods offer faster root-seeking methods than the classical Newton method. The advantage varies depending on the mathematical function involved and the initial guesses. Remember that each iteration makes three calls to the mathematical function. When the mathematical function is complex (such as one that involves summations), the saving is relevant. This article presented various types of mathematical functions that showed how the three methods behaved. In all cases, the Enhanced Newton and Newton's method converged to a root. I recommend that you plot the curves for the function (or family of functions) and examine the slope and curvature near the initial guess and root. This kind of graph gives you a good idea of how much advantage you get with the Enhanced Newton method.</p><p><b>DDJ</b></p></body></html>