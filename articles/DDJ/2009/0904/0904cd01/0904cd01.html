<html><head><title> Three Reasons for Moving to Multicore </title><link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/Layout.css" type="text/css"><link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/FontStyles.css" type="text/css"><link rel="Stylesheet" rev="Stylesheet" href="../../../../forms/newarticle.css" type="text/css"><script src="../../../../forms/popwindow.js"></script></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><p><i>Dr. Dobb's Digest</i> April 2009</p><h1> Three Reasons for Moving to Multicore </h1><h2>Performance is at the top of the list</h2><h3>By Christopher Diggins</h3><P><i>Christopher Diggins is a freelance programmer and consultant. He can be contacted at cdiggins@gmail.com</i>.<hr><P>Parallel computing used to be the specialty domain of supercomputers. These days, however, even computers aimed at the home market have at least two processor cores. Four-core machines are already widely available, with affordable six processors on the horizon. As if that's not enough, some hard-core gamers have dual quad-core processors installed, while companies like Intel are prototyping 80-core processors.<P><P>But for decision makers in the software industry, the most pressing question is how does this shift toward parallelism in hardware affect them? To address this question, we turned to <a href="http://www.cilkarts.com">Cilk Arts</a> -- a company co-founded by legendary computer scientist Charles E. Leiseron -- which recently interviewed more than 150 companies regarding their challenges and priorities in supporting multicore platforms. <P><P>When asked what are the <b>top three reasons motivating companies to move to multicore</b>, Cilk Arts' Ilya Mirman reports that they repeatedly heard  three key themes:<P><P><b>Application Performance</b>. Achieving good performance in a concurrent application on multicore hardware is not as simple as adding a bunch of threads to an appliction. In the performance critical sections of the software, all of the cores have to be kept busy. This is especially challenging because the number of cores cannot be known ahead of time. Performance has to be as good as possible if there are 1, 2, or even 16 or more cores available. Other factors affecting performance of concurrent software are efficient management of synchronization mechanisms (e.g., locks), and efficient distribution of work across the cores. Locks are a widely used mechanism for assuring that resources and memory can be shared between threads without corruption. Inadequate usage of locks can lead to race conditions, while overuse leads to poor performance. To maximize usage of the cores, work has to be distributed among the cores dynamically as each core completes its various tasks.<P><P>According to a <a href="http://www.ddj.com/architect/212501541">Rogue Wave survey</a>, performance requirements are the primary reason for them to shift to multi-core hardware:<P><ul><li>58% of respondents said that an increase in performance has been the reason behind their organizations shifting existing applications to multi-core hardware.<li>92% said that their business applications have high-performance requirements; of those that have high performance apps, 69% said that their business applications have requirements to support high throughput.<li>82% said that performance requirements for their organizations' apps are on the rise </ul><P><P><b>Development time</b> is a direct consequence of the fundamental complexity of concurrent software development. Managing this increased complexity takes an increased amount of time for everyone involved in software development: designers, developers, and testers. Closely related to development time is cost. Not only is concurrent software more expensive to write, because it takes longer, but it is more expensive per-programmer because it requires specialized knowledge and training. <P><P><b>Software reliability</b> in a concurrent system is an especially thorny problem. When concurrent software runs on a serial (non-parallel) computer, the parallelism is typically emulated using a timesharing technique. When running the same software on a parallel computers, new possibilities for race conditions arise because of the fact that assembly instructions can be executed simultaneously. According to Mirman "these pernicious bugs are notoriously hard to find. You can run regression tests in the lab for days without a failure only to discover that your software crashes in the field with regularity. If you're going to multicore-enable your application, you need a reliable way to find and eliminate race conditions. To avoid race conditions access to shared resources and memory must be synchronized."<P><P>Most companies when confronted with the prospect of migrating their software base to address the needs of multicore hardware first try an approach of manually managing native threads using thread pools and other techniques, what Leiserson calls <a href="http://www.cilk.com/multicore-blog/bid/5847/The-Folly-Of-Do-It-Yourself-Multithreading">Do It Yourself (DIY) Multithreading</a>. According to Mirman, this is the least fruitful approach for writing concurrent software because it takes too long, is too expensive, and generally produces less reliable software.<P><P>There are solutions to the problem of writing scalable and reliable concurrent software for multicore platforms that don't require a lot of retraining. Many of these are specifically for the C++ developer arena, where performance tends to be of a more immediate concern, and the challenges of concurrent software development are more acute.<P><P>Referred to as "concurrency platforms" by Charles Leiserson in his article <a href="http://www.ddj.com/architect/212001540">The Case for a Concurrency Platform</a> and elsewhere, these solutions are either library-based solutions or minor language extensions. They all provide new abstractions for expressing the inherent parallelism in software, which have to be added by the programmers, but they solve the problem of dividing the work to be done-up among the core efficiently. In effect load balancing the work among the core. All the programmer has to do, is point out where the opportunities for parallelism exist.<P><P>Many of these solutions are based on the principle of "work-stealing". Cilk Arts co-founders Matteo Frigo and Leiserson introduced work-stealing techniques in an award-winning paper <a href="http://supertech.csail.mit.edu/papers/cilk5.pdf ">Implementation of the Cilk-5 Multithreaded Language</a>. This research on work-stealing formed the basis of the Cilk++ product, and influenced other projects such as Intel's Threaded Building Blocks.<P><P>We asked Mirman to tell us a bit about work-stealing in Cilk++: The Cilk++ Runtime System (RTS) enables a Cilk++ program to dynamically and automatically exploit an arbitrary number of available processor cores. With sufficient parallelism and memory bandwidth, the RTS delivers near-perfect linear speed-up as the number of cores increases. Mirman went on to describe how <a href="http://www.cilk.com/multicore-products/cilk-runtime-system/">Cilk++ uses a decentralized scheduling algorithm</a> to efficiently distribute work. <P><P>Some of the other concurrency platforms aimed at the C++ audience migrating to multicore hardware are:<P><ul><li>Intel's <a href="http://www.threadingbuildingblocks.org">Threading Building Blocks</a> (TBB) offers a complete approach to expressing parallelism in a C++ program. It is a library that helps you take advantage of multicore processor performance without having to be a threading expert. Threading Building Blocks is not just a threads-replacement library. It represents a higher-level, task-based parallelism that abstracts platform details and threading mechanism for performance and scalability and performance.<li>The <a href="http://www.openmp.com">OpenMP</a> API supports multi-platform sharedmemory parallel programming in C/C++ and Fortran. OpenMP is a portable, scalable model with a simple and flexible interface for developing parallel applications on platforms from the desktop to the supercomputer.<li><a href="http://www.rapidmind.net">The RapidMind Development Platform</a> is a framework for expressing data-parallel computations from within C++ and executing them efficiently on multicore processors. (See <a href="http://www.ddj.com/architect/199902702">RapidMind: C++ Meets Multicore</a> by Stefanus Du Toit and Michael McCool.)<li>The <a href="http://msdn.microsoft.com/en-us/magazine/cc163340.aspx"> Task Parallel Library</a> is a managed code library for conveniently expressing potential parallelism in existing sequential code, where the exposed parallel tasks will be run concurrently on all available processors.<li><a href="http://www.cilkarts.com">Cilk++</a> from Cilk Arts simplifies the task of parallelizing code. Specialized keywords are introduced into what would otherwise be a compliable C program. The keywords indicate the functions that can be parallelized and work units that comprise those functions. The runtime system schedules the work units among the available processing elements, using a "work stealing" paradigm.</ul><P><P>Achieving scalable performance in the face of hardware parallelism without sacrificing reliability, orsignificantly increasing development costs is an issue of managing complexity. Concurrency platformscan alleviate this complexity by providing a level of abstraction for expressing parallelism whichremove the burden of manual thread management.<P><P>There are still going to be some changes needed to the software development pipeline for developingconcurrent software for parallel hardware. Here it is best to take the leads from the high-performancecomputing industry who have been refined their processes for developing concurrent software overseveral decades. Here is how the different phases of software development are affected byconcurrent software development:<P><ul><li><b>Design</b>. Because complexity of concurrent software is increased, careful design is moreimportant than ever. Interactions between modules need to be reduced. Synchronizationbottlenecks need to be identified and avoided as early as possible, to avoid spendingdeveloper time later on.<li><b>Development</b> Concurrency platform are needed for programmers to express algorithmicparallelism, without worrying about the details of distributing work across the number ofcores.<li><b>Quality assurance</b> is where the biggest investments and changes may need to bemade for concurrent software. The QA team needs more time and resource to test a fargreater number of hardware configurations. In addition to testing, new tools and processesneed to be implemented for static and dynamic analysis, in addition to profiling anddebugging.<li><b>Support</b> can be expected that despite a company's best efforts there are going to bemore problems after deployment. Field support engineers can help QA by working closelywith Beta users.</ul><P><P>So while there are steps we can take to manage the complexity of concurrent software in a parallelworld, it is still going to take some investment in training and new tools. Careful investment in thecorrect tools and a re-examination of the software pipeline will go a long way to mitigating these costs.<P><h3>Acknowledgements</h3><P><P>Thanks to Ilya Mirman and Cilk Arts for sharing data from their interviews, and to Kris Unger for providing feedback and suggestions on the article.<P>