<HTML><HEAD><TITLE>Towards Very High-Speed Networking</TITLE></HEAD>
<body bgcolor="FFFFFF">



<H1>Towards Very High-Speed Networking</H1><P>

<P>

<H3><I>William F. Jolitz</I></H3><P>

<P>

When asked what the &quot;information superhighway&quot; really is, even computer professionals sometimes have trouble answering. This isn't surprising considering the current dearth of hard facts about the proposal. Aside from TV and radio commercial blitzes (the AT&amp;T &quot;You will_&quot; series, for example) which promise movie-quality video telephones in our homes real soon now, there's little in the way of solid information you can turn to. That's why Craig Partridge's <I>Gigabit Networking</I> (Addison-Wesley, 1994, ISBN 0-201-56333-9, $46.25) is a welcome addition to the bookshelf. Although far from a complete treatment on what essentially remains a research subject, Partridge's survey does cover the basics of high-speed (gigabit per second) networking and telecommunications--and actually manages to pin down a few key areas.<P>

<I>Gigabit Networking</I> is a collection of snapshot essays on related topics such as fiber optics, cell networking, asynchronous transfer mode (ATM), internetworking, protocols, distributed systems, and the like. When viewed as a personal perspective on the construction of the information superhighway, the book is without a doubt worth reading. The discussion on ATM cell networking, for instance, is excellent. Partridge clearly has a good grasp of this subject (and I hope he'll write more on ATMs in the future). <I>Gigabit Networking</I> admittedly is not a complete examination of what it is going to take to build a high-speed national network. It is a reference book that covers some, but not all, relevant topics relating to technologies, protocols, applications, and research.<P>

For example, the Internet, the closest we currently have to an information superhighway, is too slow to service the demands of a video generation expecting <I>Star Trek</I> technology. Consequently, all feasible mechanisms to increase its speed should be considered in a book such as <I>Gigabit Networking</I>. Far too often, however, Partridge dismisses new ideas instead of pursuing them. For example, he skims over alternative approaches to internetworking protocols--lightweight protocols such as XTP (Xpress Transfer Protocol), for instance. XTP, which takes the best from both internet and transfer protocols (and considers parallel processing too) will likely become an international standard and therefore deserves more coverage than Partridge gives it.<P>

In another section entitled &quot;A Bad Example: Network Buffering in 4BSD,&quot; Partridge (correctly) derides the deficiencies of the <I>mbuf</I> mechanism because of the additional copies it implies, but then fails to mention that <I>mbuf</I>s were originally implemented to <I>reduce</I> copies between different-sized buffers and to provide a limited form of dynamic memory allocation, as explained in the original 4.2 BSD papers. (Early implementations of TCP/IP's predecessor, NCP, were also full of superfluous copies. Reducing copy overhead is a necessary step in the evolution of protocol implementation.)<P>

Another consideration in the discussion of <I>mbuf</I>s was the limited amount of memory dedicated to protocol processing which was available in the early 1980's world of 300-Kbyte PDP-11 and 2-Mbyte VAX timesharing systems servicing 20 to 40 users. The total memory allocated to the network would be only 30 to 56 Kbytes. The <I>mbuf</I> mechanism allowed for finer grain (128 byte) allocation to work within these limitations. With systems which now possess an average resource per user of 8 to 32 Mbytes of RAM, ten times the processor speed, and 1000 times the hard-disk space, the memory limits <I>should</I> be 100 to 500 times the amount of memory previously chosen--no arguments there. But how soon the past is forgotten. Although a good design choice at the time, Partridge still relegates it as a &quot;bad example.&quot;<P>

It's not surprising, by the way, that copies dominate protocol activity, since they need to slice-and-dice data to fit in packets. After all, the point of protocols is to deliver data. Processing protocols without data overhead is a little like having an ice-cream cone without ice cream--it's of no practical use (and not very appetizing). To this end, it would have been appropriate (and useful) had Partridge included in his discussion examples of no-copy solutions such as protocol engines.<P>

While <I>Gigabit Networking</I> doesn't provide concrete answers to how we should construct the information highway, it does lay out almost all of the technical questions we should consider. Additionally, the book's 27-page bibliography provides a valuable pointer for further research.<P>


</body>
</HTML>

