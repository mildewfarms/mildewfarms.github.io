
<html>
<head>
<title>November, 2005: Feedback Strategies</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Feedback Strategies</h1>
<p><i>Dr. Dobb's Journal</i> November, 2005</p>
<h2></h2>


<h3>By Dennis E. Shasha</h3>


<I>Dennis, a professor of computer science at New York University, is the author of four puzzle books: </I>The Puzzling Adventures of Dr. Ecco<I> (Dover, 1998); </I>Codes, Puzzles, and Conspiracy<I> (Freeman 1992, reprinted by Dover in 2004 as </I>Dr. Ecco: Mathematical Detective)<I>; and recently </I>Dr. Ecco's Cyberpuzzles<I> (W.W. Norton, 2002); </I>and Puzzling Adventures<I> (W.W. Norton, 2005). With Philippe Bonnet, he has written </I>Database Tuning: Principles, Experiments, and Troubleshooting Techniques<I> (2002, Morgan Kaufmann). With Cathy Lazere, he wrote </I>Out of Their Minds: The Lives and Discoveries of 15 Great Computer Scientists<I> (1995, Copernicus/Springer). He can be contacted at DrEcco@ddj.com.</I>

<hr>



<a href="0511qs1.html">Dr. Ecco Solution</a><br>


<hr>




"I promise that you will enjoy this one," Baskerhound said, coming in for a surprise visit. "It will resonate with your theories of human nature. Let me start with the populist pitch.

<p>"Have you ever admired your own skill at navigating a narrow road at high speed? If not, imagine the following alternative method of travel: Pore over a detailed map of the same road, figure out how much the wheel should turn and the accelerator should be pressed at every time point, and then drive down the road blindfolded. Even without obstacles, this is beyond the memory and trigonometric capacity of most of us."</p>

<p>"I will grant you that," Ecco replied with a sly smile.</p>

<p>"Instead, we're hardly conscious of the intellectual effort of driving," Baskerhound continued. "Perhaps the reason is that the act of driving consists of very short-term plans (a few seconds at most) followed by adaptation based on eyesight. The driver has an overall goal&#151;get to the end of the road&#151;but the plan is incremental and adaptive. This requires less brainpower and is far more robust to changes in the environment.</p>

<p>"Any person on the street understands this argument, but my bosses require quantification. So to make this concrete, I have proposed the following game. Consider this standard checkerboard that has 8 rows and 8 columns (see <a name="rf1"></a><a href="0511qf1.html">Figure 1</a>).</p>

<p>"You want to go from row 1, column 4 (the black square above the <i>S</i>) to row 8 column 5 (the black square below the <i>E</i>). Each move goes from black square to black square and proceeds up a row and either to the left or right diagonally adjacent square. If you fall off the checkerboard or reach the top row without reaching the correct square, you lose.</p>

<p>"At each move, you get to aim to go either right or left. You will achieve that step's aim with probability <i>Pgood</i>, whose values we will discuss in a minute. There are two kinds of strategies: <i>FeedYes</i> and <i>FeedNo</i>.</p>

<p>"A <i>FeedYes</i> strategy can decide where to aim on the <i>i</i>th move after seeing the results of the first <i>i-</i>1 moves. A <i>FeedNo</i> strategy must decide where to aim at step <i>i</i> from the very beginning.</p>

<p>"Here is an example to show you the difference. Suppose that you want to go from row 1, column 4 to row 3, column 4. Suppose that <i>Pgood</i> is 0.9. Then in the <i>FeedYes</i> strategy, you might aim right the first move. If you in fact go right (probability 0.9), then you would aim left the second move. But if you go left on the first move (probability 0.1), you will aim right the second move. The net result is that you have a probability of 0.9 to hit your destination. In the <i>FeedNo</i> strategy, you might do something like aim right the first move and aim left the second. There are two cases in which you would win with that strategy: You in fact move right in move 1 and left in move 2 (probability 0.9&times;0.9=0.81) or you move left in move 1 and right in move 2 (probability 0.1&times;0.1= 0.01). So <i>FeedNo</i> has a probability of 0.82 of hitting the destination.</p>

<p>"Call the feedback dividend the probability of hitting the destination with the optimal <i>FeedYes</i> strategy divided by the probability of hitting it with the optimal <i>FeedNo</i> strategy. (Optimal means that you do as well as you can based on the probabilities.) In the example here, the feedback dividend is 0.9/0.81.</p>

<p>"Here's a warm-up: Are there any values of <i>Pgood</i> for which the feedback dividend is 1 regardless of source and destination?"</p>

<p>"My dear Benjamin, of course," said Dr. Ecco. "If <i>Pgood</i> were 0.5 or 1, the feedback dividend would be only 1. In the first case, it doesn't matter where you aim. In the second, you don't need feedback. For all other <i>Pgood</i> values, the dividend will exceed 1."</p>

<p>"I didn't think that would be hard," said Baskerhound. "Now here is the full problem. You start at row 1, column 4 and you want to hit row 8, column 5. </p>

<p>"1. If <i>Pgood</i> is 0.9, what is the probability of hitting in the <i>FeedYes</i> strategy and in the <i>FeedNo</i> strategy?</p>

<p>"2. For which value of <i>Pgood</i> does the feedback dividend reach its greatest value? What is the feedback dividend in that case?"</p>

<p>I was quite surprised by the result of this second question. Not intuitive at all.</p>

<p>After Baskerhound left, Ecco asked me one other: "3. If we cut off the three rightmost columns and the two left-most columns, then which value of <i>Pgood</i> would give the highest feedback dividend? Assume that falling off the board gives a certain loss."</p>

<p><b></b></p>
<p><b>DDJ</b></p>




</body>
</html>