
<html>
<head>
<title>May, 2005: Na&iuml;ve Bayesian  Text Classification </title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Na&iuml;ve Bayesian  Text Classification </h1>
<p><i>Dr. Dobb's Journal</i> May, 2005</p>
<h2>Fast, accurate, and easy to implement</h2>


<h3>By John Graham-Cumming </h3>


<I>John is chief scientist at Electric Cloud, which focuses on reducing software build times. He is also the creator of POPFile. John can be contacted at jgc@electric-cloud.com.</I>

<hr>





<p>Paul Graham popularized the term "Bayesian Classification" (or more accurately "Na&iuml;ve Bayesian Classification") after his "A Plan for Spam" article was published (http://www.paulgraham.com/spam.html). In fact, text classifiers based on na&iuml;ve Bayesian and other techniques have been around for many years. Companies such as Autonomy and Interwoven incorporate machine-learning techniques to automatically classify documents of all kinds; one such machine-learning technique is na&iuml;ve Bayesian text classification.</p>

<p>Na&iuml;ve Bayesian text classifiers are fast, accurate, simple, and easy to implement. In this article, I present a complete na&iuml;ve Bayesian text classifier written in 100 lines of commented, nonobfuscated Perl.</p>

<p>A text classifier is an automated means of determining some metadata about a document. Text classifiers are used for such diverse needs as spam filtering, suggesting categories for indexing a document created in a content management system, or automatically sorting help desk requests.</p>

<p>The classifier I present here determines which of a set of possible categories a document is most likely to fall into and can be used in any of the ways mentioned with appropriate training. Feed it samples of spam and nonspam e-mail and it learns the difference; feed it documents on various medical fields and it distinguishes an article on, say, "heart disease" from one on "influenza." Show it samples of different types of help desk requests and it should be able to sort them so that when 50 e-mails come in informing you that the laser printer is down, you'll quickly know that they are all the same.</p>
<h3>The Math</h3>

<p>You don't need to know any of the underlying mathematics to use the sample classifier presented here, but it helps.</p>

<p>The underlying theorem for na&iuml;ve Bayesian text classification is the Bayes Rule:</p>

<blockquote>
P(A|B) = ( P(B|A) * P(A) ) / P(B)<br>

</blockquote>

<p>The probability of <i>A</i> happening given <i>B</i> is determined from the probability of <i>B</i> given <i>A</i>, the probability of <i>A</i> occurring and the probability of <i>B</i>. The Bayes Rule enables the calculation of the likelihood of event <i>A</i> given that <i>B</i> has happened. This is used in text classification to determine the probability that a document <i>B</i> is of type <i>A</i> just by looking at the frequencies of words in the document. You can think of the Bayes Rule as showing how to update the probability of event <i>A</i> happening given that you've observed <i>B.</i></p>

<p>A far more extensive discussion of the Bayes Rule and its general implications can be found in the Wikipedia (http://en.wikipedia.org/wiki/Bayes%27_Theorem). For the purposes of text classification, the Bayes Rule is used to determine the category a document falls into by determining the most probable category. That is, given this document with these words in it, which category does it fall into?</p>

<p>A category is represented by a collection of words and their frequencies; the frequency is the number of times that each word has been seen in the documents used to train the classifier.</p>

<p>Suppose there are <i>n</i> categories <i>C</i><sub>0</sub> to <i>C</i><sub>n-1</sub>. Determining which category a document <i>D</i> is most associated with means calculating the probability that document <i>D</i> is in category <i>C</i><sub>i</sub>, written <i>P(C</i><sub>i</sub>|<i>D)</i>, for each category <i>C</i><sub>i</sub>.</p>

<p>Using the Bayes Rule, you can calculate <i>P(C</i><sub>i</sub>|<i>D)</i> by computing:</p>

<blockquote>
P(C<sub>i</sub>|D) = ( P(D|C<sub>i </sub>) * P(C<sub>i</sub>) ) / P(D)<br>

</blockquote>

<p><i>P(C</i><sub>i<i></sub>|D)</i> is the probability that document <i>D</i> is in category <i>C</i><sub>i</sub>; that is, the probability that given the set of words in <i>D</i>, they appear in category <i>C</i><sub>i</sub>. <i>P(D|C</i><sub>i<i></sub>)</i> is the probability that for a given category <i>C</i><sub>i</sub>, the words in <i>D</i> appear in that category.</p>

<p><i>P(C</i><sub>i<i></sub>)</i> is the probability of a given category; that is, the probability of a document being in category <i>C</i><sub>i</sub> without considering its contents. <i>P(D)</i> is the probability of that specific document occurring.</p>

<p>To calculate which category <i>D</i> should go in, you need to calculate <i>P(C</i><sub>i</sub>|<i>D)</i> for each of the categories and find the largest probability. Because each of those calculations involves the unknown but fixed value <i>P(D)</i>, you just ignore it and calculate:</p>

<blockquote>
   P(C<sub>i</sub> |D) = P(D|C<sub>i</sub> ) * P(C<sub>i</sub>)<br>

</blockquote>

<p><i>P(D)</i> can also be safely ignored because you are interested in the relative&#151;not absolute&#151;values of <i>P(C</i><sub>i</sub>|<i>D)</i>, and <i>P(D)</i> simply acts as a scaling factor on <i>P(C</i><sub>i</sub>|<i>D)</i>.</p>

<p><i>D</i> is split into the set of words in the document, called <i>W</i><sub>0</sub> through <i>W</i><sub>m-1</sub>. To calculate <i>P(D</i>|<i>C</i><sub>i<i></sub>)</i>, calculate the product of the probabilities for each word; that is, the likelihood that each word appears in <i>C</i><sub>i</sub>. Here's the "na&iuml;ve" step: Assume that words appear independently from other words (which is clearly not true for most languages) and <i>P(D</i>|<i>C</i><sub>i<i></sub>)</i> is the simple product of the probabilities for each word:</p>

<blockquote>
     P(D|C<sub>i</sub>) = P(W<sub>0</sub>|C<sub>i</sub>) * P(W<sub>1</sub>|C<sub>i</sub>) * ... * P(W<sub>m-1</sub>|C<sub>i</sub>)<br>

</blockquote>

<p>For any category, <i>P(W</i><sub>j|<i></sub>C</i><sub>i<i></sub>)</i> is calculated as the number of times <i>W</i><sub>j  </sub>appears in <i>C</i><sub>i</sub> divided by the total number of words in <i>C</i><sub>i</sub>. <i>P(C</i><sub>i<i></sub>)</i> is calculated as the total number of words in <i>C</i><sub>i</sub> divided by the total number of words in all the categories put together. Hence, <i>P(C</i><sub>i</sub>|<i>D)</i> is:</p>

<blockquote>
     P(W<sub>0</sub>|C<sub>i</sub>) * P(W<sub>1</sub>|C<sub>i</sub>) * ... * P(W<sub> m-1</sub>|C<sub>i</sub>) * P(C<sub>i</sub>)<br>

</blockquote>

<p>for each category, and picking the largest determines the category for document <i>D.</i></p>

<p>A common criticism of na&iuml;ve Bayesian text classifiers is that they make the na&iuml;ve assumption that words are independent of each other and are, therefore, less accurate than a more complex model. There are many more complex text classification techniques, such as Support Vector Machines, <i>k</i>-nearest neighbor, and so on. In practice, na&iuml;ve Bayesian classifiers often perform well, and the current state of spam filtering indicates that they work very well for e-mail classification.</p>

<p>A useful toolkit that implements different algorithms is the freely available Bow toolkit from CMU (http://www-2.cs.cmu.edu/~mccallum/bow/). It makes a useful testbed for comparing the accuracy of different techniques. A good starting point for reading more about na&iuml;ve Bayesian text classification is the Wikipedia article on the subject (http://en.wikipedia.org/wiki/Na&iuml;ve_Bayesian_classification).</p>
<h3>Implementation</h3>

<p>The Perl implementation (<a name="rl1"></a><a href="#l1">Listing One</a>) uses the hash (associative array) <i>%words</i> to store the word counts for each word and for each category. The hash is stored to disk using a Perl construct called a "tie" that, when used with the<i> DB_File</i> module, results in the hash being stored automatically in a file called "words.db" so that its contents persist between invocations.</p>

<blockquote>
use DB_File;<br>
my %words;<br>
tie %words, 'DB_File', 'words.db';<br>

</blockquote>

<p>The hash keys are strings of the form category-word: For example, if the word "potato" appears in the category "veggies" with a count of three, there will be a hash entry with key "potato-veggies" and value "3." This data structure contains enough information to compute the probability of a document and do a na&iuml;ve Bayesian classification.     </p>

<p>The subroutine <i>parse_file</i> reads the document to be classified or trained on and fills in a hash called <i>%words_in_file</i> that maps words to the count of the number of times that word appeared in the document. It uses a simple regular expression to extract every 3- to 44-letter word that is followed by whitespace; in a real classifier, this word splitting could be made more complex by accounting for punctuation, digits, and hyphenated words. </p>

<blockquote>
sub parse_file<br>
{<br>
   my ( $file ) = @_;<br>
   my %word_counts;<br>
   open FILE, "&lt;$file";<br>
   while ( my $line = &lt;FILE&gt; ) {<br>
      while ( $line =~<br>
          s/([[:alpha:]]{3,44})[ \t\n\r]// ){<br>
        $word_counts{lc($1)}++;<br>
      }<br>
   }<br>
   close FILE;<br>
   return %word_counts;<br>
}<br>

</blockquote>

<p>The output of <i>parse_file</i> can be used in two ways: It can be used to train the classifier by learning the word counts for a particular category and updating the <i>%words</i> hash, or it can be used to determine the classification of a particular document. </p>

<p>To train the classifier, call the <i>add_words</i> subroutine with the output of <i>parse_file</i> and a category. In the Perl code, a category is any string and the classifier is trained by passing sample documents into <i>parse_file</i> and then into <i>add_words: add_words( &lt;category&gt;, parse_file( &lt;sample document&gt;));</i></p>

<blockquote>
sub add_words<br>
{<br>
   my ( $category, %words_in_file ) = @_;<br>
   foreach my $word (keys %words_in_file) {<br>
      $words{"$category-$word"} +=<br>
         $words_in_file{$word};<br>
   }<br>
}<br>

</blockquote>

<p>Once document training has been done, the <i>classify</i> subroutine can be called with the output of <i>parse_file</i> on a document. <i>classify</i> will print out the possible categories for the document in order of most likely to least likely: </p>

<blockquote>
classify ( parse_file( &lt;document to classify&gt; ) );<br>
<br>
sub classify<br>
{<br>
   my ( %words_in_file ) = @_;<br>
   my %count;<br>
   my $total = 0;<br>
   foreach my $entry (keys %words) {<br>
      $entry =~ /^(.+)-(.+)$/;<br>
      $count{$1} += $words{$entry};<br>
      $total += $words{$entry};<br>
   }<br>
   my %score;<br>
   foreach my $word (keys %words_in_file) {<br>
      foreach my $category (keys %count) {<br>
         if (defined($words{"$category-$word"})) {<br>
            $score{$category} +=<br>
               log( $words{"$category-$word"} /<br>
                  $count{$category} );<br>
         } else {<br>
            $score{$category} +=<br>
               log( 0.1 /<br>
                  $count{$category} );<br>
         }<br>
      }<br>
   }<br>
   foreach my $category (keys %count) {<br>
      $score{$category} +=<br>
         log( $count{$category} / $total );<br>
   }<br>
   foreach my $category (sort { $score{$b} &lt;=&gt; $score<br>
{$a} } keys %count) {<br>
      print "$category $score{$category}\n";<br>
   }<br>
}<br>

</blockquote>

<p><i>classify</i> first calculates the total word count (<i>$total</i>) for all categories (which it needs to calculate <i>P(C</i><sub>i<i></sub>)</i>) and the word count for each category (<i>%count </i>indexed by category name, which it needs to calculate <i>P(W</i><sub>j</sub>|<i>C</i><sub>i<i></sub>)</i>). Then <i>classify</i> calculates the score for each category: The score is the value of <i>P(C</i><sub>i</sub>|<i>D)</i>. It's preferable to call it a score for two reasons: Ignoring<i> P(D)</i> means that, strictly speaking, the value is being calculated incorrectly and <i>classify</i> uses logs to reduce overflow errors and replace multiplication by addition for speed. The score is in fact <i>log</i> <i>P(C</i><sub>i</sub>|<i>D)</i>, which is:</p>

<blockquote>
log P(W<sub>0</sub>|C<sub>i</sub>) + log P(W<sub>1</sub>|C<sub>i</sub>) + ... + log P(W<sub>m-1</sub>|C<sub>i</sub>) + log P(C<sub>i</sub>)<br>

</blockquote>

<p>(Recall the equality <i>log (A*B)=log A+log B</i>). In that <i>log</i> form, it is still suitable for comparison. After the score has been calculated, <i>classify</i> calculates <i>log P(C</i><sub>i<i></sub>)</i> for each category and then sorts the scores in descending order to output the classifier's opinion of the document. <i>classify</i> makes an estimate of the probability for a word that doesn't appear in a particular category by calculating a very small, nonzero probability for that word based on the word count for the category:</p>

<blockquote>
$score{$category} += log( 0.1 / $count{$category} );<br>

</blockquote>

<p>A small amount of Perl code wraps these three subroutines into a usable classifier that accepts commands to add a document to the word list for a category (and hence, train the classifier), and to classify a document.</p>

<blockquote>
if ( ( $ARGV[0] eq 'add' ) &amp;&amp; ( $#ARGV == 2 ) ) {<br>
   add_words( $ARGV[1], <br>
                     parse_file( $ARGV[2] ) );<br>
} elsif ( ( $ARGV[0] eq 'classify' ) &amp;&amp; ( $#ARGV == 1 )<br>
) {<br>
   classify( parse_file( $ARGV[1] ) );<br>
} else {<br>
   print &lt;&lt;EOUSAGE;<br>
Usage: add &lt;category&gt; &lt;file&gt; - Adds words from &lt;file&gt;<br>
to category &lt;category&gt;<br>
   classify &lt;file&gt; - Outputs classification<br>
of &lt;file&gt;<br>
EOUSAGE<br>
}<br>
untie %words;<br>

</blockquote>

<p>If the Perl code is stored in file bayes.pl, then the classifier is trained like this: </p>

<blockquote>
perl bayes.pl add veggies article-about-vegetables<br>
perl bayes.pl add fruits article-about-fruits<br>
perl bayes.pl add nuts article-about-nuts<br>

</blockquote>

<p>to create three categories (veggies, fruits, and nuts). Asking bayes.pl to classify a document will output the likelihood that the document is about vegetables, fruits, or nuts:</p>

<blockquote>
% perl bayes.pl classify article-I-just-wrote<br>
fruits -4.11700258611469<br>
nuts -6.60190923590268<br>
veggies -11.9002266024507<br>

</blockquote>

<p>Here, bayes.pl shows that the new article is most likely about fruits. </p>
<h3>E-Mail Classification</h3>

<p>If you are interested in classifying e-mail, there are a couple of tweaks that improve accuracy in practice: Don't fold case on values from headers and count words differently if they appear in the subject or body.</p>

<p>In the aforementioned Perl implementation, there is no difference between the words <i>From, FROM,</i> and <i>fRoM</i>: They are all considered to be instances of <i>from</i>. The <i>parse_file</i> subroutine lowercases the word before counting it. In practical e-mail classifiers, the names of e-mail headers turn out to be a better indicator of the type of an e-mail if case is preserved. For example, the header <i>MIME-Version</i> was written <i>MiME-Version</i> by one piece of common spamming software.</p>

<p>Distinguishing words found in the subject versus the body also increases the accuracy of a na&iuml;ve Bayesian text classifier on e-mail. The simplest way to do this is to store a word like <i>forward</i> as <i>subject:forward</i> when it comes from the subject line, and simply <i>forward</i> when it is seen in the body.</p>
<h3>Performance</h3>

<p>The Perl code presented here isn't optimized at all. Each time <i>classify</i> is called, it has to recalculate the total word count for each category and it would be easy to cache the log values between invocations. The use of a Perl hash will not scale well in terms of memory usage.</p>

<p>However, the algorithm is simple and can be implemented in any language. A highly optimized version of this code is used in the POPFile e-mail classifier to do automatic classification. It uses a combination of Perl and SQL queries. The Bow toolkit from CMU has a fast C implementation of na&iuml;ve Bayesian classification. </p>
<h3>Uses of Text Classification</h3>

<p>Although spam filtering is the best-known use of na&iuml;ve Bayesian text classification, there are a number of other interesting uses on the horizon. IBM researcher Martin Overton has published a paper concerning the use of na&iuml;ve Bayesian e-mail classification to detect e-mail-borne malware (http://arachnid.homeip.net/papers/VB2004-Canning-more-than-SPAM-1.02.pdf). In Overton's paper, presented at the Virus Bulletin 2004 conference, he demonstrated that a text classifier could accurately identify worms and viruses, such as W32.Bagle, and that it was able to spot even mutated versions of the worms. All this was done without giving the classifier any special knowledge of viruses. </p>

<p>The POPFile Project is a general e-mail classifier that can classify incoming e-mail into any number of categories. Users of POPFile have reported using its na&iuml;ve Bayesian engine to classify mail into up to 50 different categories with good accuracy, and one journalist uses it to sort "interesting" from "uninteresting" press releases. </p>

<p>At LISA 2004, four Norwegian researchers presented a paper concerning a system called DIGIMIMIR, which was capable of automatically classifying requests coming into a typical IT help desk and in some cases responding automatically (http://www.digimimir.org/). They use a document clustering approach that, while not na&iuml;ve Bayesian, is similar in implementation complexity and allowed the clustering together of "similar" e-mails without knowing the initial set of possible topics.</p>


<p><b>DDJ</b></p>



	
	
<br>
<br>
<b><a name="l1"></a>Listing One</b><br>
<pre>use strict;
use DB_File;

# Hash with two levels of keys: $words{category}{word} gives count of
# 'word' in 'category'.  Tied to a DB_File to keep it persistent.

my %words;
tie %words, 'DB_File', 'words.db';

# Read a file and return a hash of the word counts in that file

sub parse_file
{
    my ( $file ) = @_;
    my %word_counts;

    # Grab all the words with between 3 and 44 letters

    open FILE, "&lt;$file";
    while ( my $line = &lt;FILE&gt; ) {
        while ( $line =~ s/([[:alpha:]]{3,44})[ \t\n\r]// ) {
            $word_counts{lc($1)}++;
        }
    }
    close FILE;
    return %word_counts;
}

# Add words from a hash to the word counts for a category
sub add_words
{
    my ( $category, %words_in_file ) = @_;

    foreach my $word (keys %words_in_file) {
        $words{"$category-$word"} += $words_in_file{$word};
    }
}

# Get the classification of a file from word counts
sub classify
{
    my ( %words_in_file ) = @_;

    # Calculate the total number of words in each category and
    # the total number of words overall

    my %count;
    my $total = 0;
    foreach my $entry (keys %words) {
        $entry =~ /^(.+)-(.+)$/;
        $count{$1} += $words{$entry};
        $total += $words{$entry};
    }

    # Run through words and calculate the probability for each category

    my %score;
    foreach my $word (keys %words_in_file) {
        foreach my $category (keys %count) {
            if ( defined( $words{"$category-$word"} ) ) {
                $score{$category} += log( $words{"$category-$word"} /
                                          $count{$category} );
            } else {
                $score{$category} += log( 0.01 /
                                          $count{$category} );
            }
        }
    }
    # Add in the probability that the text is of a specific category

    foreach my $category (keys %count) {
        $score{$category} += log( $count{$category} / $total );
    }
    foreach my $category (sort { $score{$b} &lt;=&gt; $score{$a} } keys %count) {
        print "$category $score{$category}\n";
    }
}

# Supported commands are 'add' to add words to a category and
# 'classify' to get the classification of a file

if ( ( $ARGV[0] eq 'add' ) &amp;&amp; ( $#ARGV == 2 ) ) {
    add_words( $ARGV[1], parse_file( $ARGV[2] ) );
} elsif ( ( $ARGV[0] eq 'classify' ) &amp;&amp; ( $#ARGV == 1 ) ) {
    classify( parse_file( $ARGV[1] ) );
} else {
    print &lt;&lt;EOUSAGE;
Usage: add &lt;category&gt; &lt;file&gt; - Adds words from &lt;file&gt; to category &lt;category&gt;
       classify &lt;file&gt;       - Outputs classification of &lt;file&gt;
EOUSAGE
}

untie %words;</pre>
<a href="#rl1">Back to article</a><br>
	
	
	


</body>
</html>