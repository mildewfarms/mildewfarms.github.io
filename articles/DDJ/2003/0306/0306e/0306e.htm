<html><head><title>Jun03: Algorithm Alley</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h1>Heap Ltd.</h1><p><i>Dr. Dobb's Journal</i> June 2003</p><h2>Searching for the  most relevant results</h2><h3>By Evgeniy Gabrilovich  and Alex Gontmakher</h3><I>Evgeniy and Alex are Ph.D. students in Computer Science at the Technion-Israel Institute of Technology. They can be contacted at gabr@cs.technion.ac.il and gsasha@cs .technion.ac.il, respectively.</I><hr><a name="rs1"><a href="0306es1.htm">The Heap Data Structure</a><hr><p>When you perform a search on a web site, you usually want to see the most relevant results first. To do this, the search engine assigns each search result a relevance score, and ultimately returns 10 or 20 highest scoring results.</p><p>Selecting several best elements is obviously not limited to the Web. When monitoring system health, for instance, you might want to find the 10 biggest files on a given disk. Or in the realm of genetic algorithms, during each phase, you might choose a certain number of the fittest organisms to form the next generation. </p><DDJADVERTISEMENT INLINE><p>While finding a single best element in a sequence is straightforward, selecting <i>k </i>best elements is a tricky business. There are several solutions, each with different performance characteristics. The trade-off between time and space is not trivial, and the most practical algorithm must be carefully selected.</p><p>What is the best that you can expect from a good solution? On the one hand, each element must be examined at least once, so the time complexity can be no lower than <i>O(N)</i>. On the other hand, the resulting <i>k</i> best elements need to be stored; hence, the memory complexity can be no better than <i>O(k)</i>. The problem is that you cannot have both time and memory complexity low at the same time.</p><p>In short, you often need to select a number of best elements from a sequence of values. This problem is not new, and several algorithms have been developed to address it, each having different time complexity characteristics. In this article, we'll examine the various existing algorithms, and then present one called "limited heap," which arguably provides the best trade-off between speed and memory utilization. </p><h3>Solution #1: Sorting</h3><p>A naive way to select the <i>k</i> largest elements is simply to store the entire sequence in memory, sort it in decreasing order, then return the first <i>k</i> elements from the sorted sequence. Coding this technique is straightforward since sorting algorithms are part of the Standard Library in almost all languages.</p><p>Time complexity, however, is another story. <i>O(N</i>*log<i>(N))</i> is mediocre. In addition, storing the entire sequence consumes <i>O(N)</i> additional memory. When <i>N</i> is very large&#151;especially when the elements are produced on the fly and need not be permanently stored otherwise&#151;this technique may impose an unreasonable burden on memory usage.</p><h3>Solution #2: Heapsort</h3><p>A better approach is to use a variant of heapsort. The original heapsort algorithm (see <i>Introduction to Algorithms</i> by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein; MIT Press, 1990) collects all the elements in an array, rearranges the array as a heap (this can be accomplished in <i>O(N)</i> in the worst case), and then extracts the largest element from the heap <i>N</i> times (this amounts to <i>O(N</i>*log<i>(N))</i> since the heap property needs to be restored after each extraction). In our case, we only need to perform <i>k</i> extractions to obtain the <i>k</i> largest elements; hence, the overall time complexity is <i>O(N+k</i>*log<i>(N))</i>.</p><p>Still, this solution suffers from the same drawback as the previous one, as it requires <i>O(N)</i> additional memory to operate the heap that initially contains the entire sequence. (For background information on heapsort, see the accompanying text box entitled "The Heap Data Structure.")</p><h3>Solution #3: Limited Heap</h3><p>When the number of elements in the sequence <i>(N)</i> is huge, you'd rather not store them all merely to select the <i>k</i> largest ones. You can, however, minimize the memory requirements, using only <i>O(k)</i> additional memory to store the <i>k</i> elements requested.</p><p>To do this, use a limited heap, which cannot grow beyond <i>k</i> elements. You sift the entire sequence through it, while at any given moment, the heap stores the <i>k </i>largest elements seen so far. New elements are only inserted if they are larger than the current smallest element in the heap, in which case, they replace the latter, and the heap size never grows beyond <i>k</i>.</p><p>Ideally, you would like to sift all the sequence elements through the heap one by one, removing the worst element whenever the heap size becomes larger than <i>k</i>. However, while the heap provides easy access to its best element, removing the worst element is more complex and can require as many as <i>O(k)</i> operations. To circumvent this problem, observe that during the selection process you do not need access to the best element&#151;only to the worst. Thus, you reverse the heap order, so that the heap root always contains the current smallest element. In fact, you maintain a "min-heap," even though what you really want is to select the <i>k </i>largest elements.</p><p>At the steady state, when the heap contains <i>k</i> items, determining the value of the smallest one takes <i>O</i>(1) (due to min-heap ordering). Whenever applicable, replacing the smallest element takes <i>O(l</i>og<i>(k))</i>; thus, the overall worst-case time complexity is <i>O(N</i>*log<i>(k))</i>. Since the heap is limited, it only keeps as many elements as are eventually required; hence, the additional memory complexity is only <i>O(k)</i>&#151;a substantial savings when <i>k&lt;&lt;N</i>!</p><p>Implementing this algorithm is straightforward. Reversing the order of elements in the heap means you need only override the comparison operation it uses. In C++, the comparison predicate is a template parameter to the heap operations, so the algorithm can be readily implemented using the heap manipulation functions from the C++ Standard Library.</p><p><A NAME="rl1"><A HREF="#l1">Listing One</A> is C++ code that implements the limited heap (template class <i>KMaxValues</i>). The class inherits from <i>std::vector</i>, and capitalizes on its container functionality. To make the template more generic, allow elements to carry a payload in addition to their values (eliminating the payload when it is not necessary should be straightforward). The complete source code that implements the limited heap is available electronically; see "Resource Center," page 5.</p><p>The limited heap interface is provided through a constructor, a <i>push_back</i> function that sifts new elements through the structure, and (constant) iterators that allow access to individual elements. All the rest of the original vector functions are declared private, so that their inadvertent use does not invalidate the heap property. Most of these functions, such as <i>erase</i> and <i>pop_back</i>, are actually inapplicable to the heap structure. The only notable exception is <i>operator[]</i>, whose nonconstant version may render the heap inconsistent; we block it altogether as it is impossible to selectively allow only the constant version.</p><p>Whenever the <i>push_back</i> function adds a new item to the heap, it first uses <i>std::pop_heap</i> to pop off the smallest element. The way <i>pop_heap</i> is implemented, it moves the first (that is, smallest) heap element to the last position (namely, <i>vector</i>[<i>_n-</i>1]), then restores the heap property. Subsequently, the new value is injected into this last position; using <i>vector::push_back</i> would both unnecessarily grow the vector and include anew the element that has just been removed by <i>pop_heap</i>. Ultimately, <i>std::push_heap</i> is invoked to include the newly added element into the heap.</p><p>For various combinations of <i>k</i> and <i>N</i>, solution #2 (heapsort) may be asymptotically more time efficient than #3 (limited heap), but the modest memory requirements of the latter can hardly be beaten. (Mathematically inclined readers will find out by complexity comparison that the former solution is preferable when <i>k/</i>(log <i>k</i>-1)<i>&lt;N/</i>log<i>N</i>, or roughly when <i>k&lt;&lt;N</i>). Also, in both solutions, elements are extracted from the heap in a sorted order. This might be a benefit (if sorted order is actually desired), or a drawback (if stable operation is necessary; reminiscent of stable sort, stable operation in this context simply preserves the original relative order of equivalent elements). In the latter case, the payload fields may be utilized to track the original element ordering.</p><h3>Other Options</h3><p>At this point, you probably start wondering, "Is the limited heap algorithm optimal? Is it the best we can hope for? Can you really achieve the <i>O(N)</i> lower bound necessary for scanning the input sequence?" It turns out that there are algorithms that can do this (alas, have we mentioned that there's no such thing as a free lunch?).</p><p>There are so-called selection algorithms that find the <i>k</i>th largest (single) element in a sequence. In computer science, a closely related value of the <i>k</i>th smallest element is referred to as the <i>k</i>th order statistic of a sequence. Given such an element, the sequence can be trivially partitioned (in a single <i>O(N)</i> pass) so that all the <i>k</i> largest elements are grouped together. The problem, however, is that algorithms that work reasonably well on average might require as much as <i>O</i>(<i>N</i><sup>2</sup>) time in the worst case, while algorithms with guaranteed <i>O(N)</i> selection time are extremely slow in practice. In either case, <i>O(N)</i> additional memory is required to store the entire sequence&#151;a considerable drawback when working on the fly.</p><h3>Conclusion</h3><p>Time complexity is important in choosing an algorithm. However, in real life, you should not blindly select the algorithm that advertises the best complexity. Other concerns, such as memory requirements, can often make a seemingly inferior algorithm preferable in practice.</p><p></p><p><b>DDJ</b></p><H4><A NAME="l1">Listing One</H4><pre>using namespace std;template&lt;class _Key,class _T&gt; class KMaxValues:public vector&lt;pair&lt;_Key,_T&gt; &gt; {   typedef vector&lt;pair&lt;_Key, _T&gt; &gt; Base;   int _n; /* maximum size allowed */   /* Block access to extraneous functions inherited from std::vector,      lest their invocation might invalidate heap properties. */   using Base::assign; using Base::erase; using Base::insert;   using Base::pop_back; using Base::resize; using Base::swap;   using Base::operator[];using Base::iterator;   struct greater : public binary_function&lt;value_type, value_type, bool&gt; {      bool operator()(const value_type&amp; x, const value_type&amp; y) const         { return (x.first &gt; y.first); }    };public:   explicit KMaxValues(int maxSize = 1) : _n(maxSize)       { reserve(maxSize); /* preallocate storage */ }   void push_back(const value_type&amp; x) {      if (size() &lt; _n) {         Base::push_back(x);         push_heap(begin(), end(), greater());      } else { /* maximum size reached */         if (x.first &lt; begin()-&gt;first)            return; /* no need to add the element at all */         /* delete the smallest element, then add the new one */         pop_heap(begin(), end(), greater());         (*this)[_n-1] = x; /* inserts the new element into last position */         push_heap(begin(), end(), greater()); /* restore heap property */      }   }};</pre><P><A HREF="#rl1">Back to Article</A></P></body></html>