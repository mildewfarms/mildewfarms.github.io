<HTML>
<META NAME="year" CONTENT="1996">
<HEAD>
<!-- This document was built using HyperWriter Professional by NTERGAID Inc. -->
<TITLE>FEB96: ALGORITHM ALLEY</TITLE></HEAD>
<body bgcolor="FFFFFF">
<h1>ALGORITHM ALLEY<a name="00d3_0041"><a name="00d3_0041"><a name="00d3_003f"><a name="00d3_0000"></h1><P>

<h2>Binary Search</h2><P>

<h3>Micha Hofri</h3><P>

<P>
<i><a name="00d3_0040"><a name="00d3_0000">Micha, a member of the computer science department
at Rice University, is the author of Analysis of Algorithms: Mathematical Methods, Computational Tools
(Oxford University Press, 1995). He can be contacted at hofri@cs.rice.edu.</i></p><hr><P>

Introduction <P>

<h3>by Bruce Schneier</h3><P>

Binary searches are among those algorithmic staples that have uses everywhere. If you want to locate an
entry in a sorted array, a binary search is the most efficient way to do so. I can't think of a college course
in basic algorithms that doesn't cover binary searches.<P>

The technique isn't perfect, however. Records must all be the same length and must be stored in a static
array. If the entries are of variable length or in some kind of dynamic data structure, other less-efficient
search techniques--search trees, interpolation searches, hashing, and the like--take over. But since the
entry can be a key pointing to a more-complex record, binary-search techniques can be used in many
situations.<P>

This month, Micha Hofri sees how efficient he can make a basic binary-search algorithm. Why are we
bothering with something so basic? Micha's analysis is interesting not so much in what it reveals about
binary search, but in how the algorithm works. Good programmers do this kind of analysis with any
algorithm that has significant effects on the performance of the system. (At least, they do when they have
the time and budget to program right, not when the deadline is in five hours and management doesn't care
what it looks like as long as it works.) Micha's trade-offs--iteration versus recursion, more simple steps
versus fewer complex steps--are the type that can be made almost everywhere. Even in a world of
ever-increasing processor power and clock speeds, a finely crafted algorithm is still a thing of beauty.<P>

By the way, I am still interested in hearing your column ideas, whether you want to write a column
yourself or you'd like to see a certain topic explored. You can contact me at schneier@winternet.com.<P>

Binary search is the method of choice for searching a list for a given key value. In fact, it is the optimal
comparison-based search algorithm (hashing methods can do better--with some trade-offs). To use binary
search, the lists to be searched must be sorted (we assume in increasing order), of known length, and
indexable in an array. This implies that the keys must be all of the same size, and that binary search can
not be used to search a linked list.<P>

These conditions mean that the <I>i</I>th smallest key can be directly accessed, as
<I>A</I><I><SUB>i</I></SUB> (or <I>A</I>[<I>i</I>], using C-like notation). Binary search can be used to
find a given key or to check whether or not the list contains one. The sorting implies, for example, that if
the key value 6 (assuming all the keys are integers) is followed by the key value 10, then 7, 8, and 9 are
not in the list. <P>

I'll present here a basic form of binary search, which I refer to as &quot;BS&quot;; I'll also discuss
&quot;BS1&quot; and &quot;BS2.&quot; Much of the following discussion is based on Gilles Brassard and
Paul Bratley's book <I>Algorithmics: Theory and Practice</I> (Prentice-Hall, 1988). Since BS is a
&quot;divide-and-conquer&quot; algorithm, its form is naturally recursive; see <a href="9602m.htm#00d3_0045">Example 1(a)</A>. Note in line 3 the comment
<img src="hfbrdlf12.gif" alt="|_">(<I>i</I>+<I>j</I>+1)/2<img src="hfbrdrt12.gif" alt="_|">. (The term <img src="hfbrdlf12.gif" alt="|_"><I>a</I><img src="hfbrdrt12.gif" alt="_|"> is the &quot;floor&quot; of <I>a</I>; it is the largest
integer that does not exceed <I>a</I>.) In particular, on entry, when <I>i</I>=0, and <I>j</I>=<I>n</I>-1,
<I>k</I>=<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">. To adapt binary search to check for missing values, you simply change line 2 to <a href="9602m.htm#00d3_0045">Example 1(b)</A>. <P>

<h3><a name="00d3_0042">Analyzing Binary Search<a name="00d3_0042"></h3><P>

When implementing BS, a few design options are possible. All considerations are based on computing the run
time. The purpose here is not just to learn about BS, but to show the power--and limitations--of this type
of analysis.<P>

The first question to resolve is the cost unit. Each basic operation in a source language (such as C) is taken
as one unit, as is each comparison in BS. However, line 3 in <a href="9602m.htm#00d3_0045">Example
1(a)</A>, for instance, needs three operations (although some compilers can do better than that). <I>Q
</I>denotes the cost of a function call and return; therefore, if BS terminates in line 2, its cost is
<I>Q</I>+1; otherwise it is <I>Q</I>+1+3+1+another call.<P>

To be more precise, suppose <I>A</I> has <I>n</I> entries and <I>x</I> is equally likely to match each of
them. If <I>T</I>(<I>n</I>) denotes the total cost of such a call, then <I>T</I>(1)=<I>Q</I>+1 and for
<I>n</I>gt;1, <I>T</I>(<I>n</I>)=<I>Q</I>+5+<I>T</I>(<I>r</I>), where <I>r</I> depends on the result of
the comparison in line 4 and is either <I>k</I>=<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|"> or <I>n-k</I>=<img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif"> (the
&quot;ceiling&quot; of <I>n/2</I> or the smallest integer gt;/=<I>n</I>/2).<P>

Although <I>T(n)</I> is a random variable, we deal here only with its expected value, <I>t(n)</I>. (For a
deeper analysis, see my book <I>Analysis of Algorithms: Mathematical Methods, Computational Tools</I>,
Oxford University Press, 1995.) Since <I>x</I> is equally likely to be each of the entries, it is with
probability of <img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">/2<I>n</I> in the lower part and <img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif">/<I>n</I> in the upper part. <a href="9602d3f1.gif">Figure 1</A> (a) presents an equation--here, a recurrence--for the mean running
time. This kind of equation is typical for divide-and-conquer algorithms. The successful approach to solving
this type of equation is: <P>

<ol>1.     Simplify. </ol><P>

<ol>2.     Look for a sequence of argument values where the equation can be solved by standard analytic
means.</ol><P>

<ol>3.     Use this as a guide to guess a solution from a table of values produced by the recurrence. Then use
substitution to prove that the guess satisfies the equation.</ol><P>

Simplifying means replacing <I>Q</I>+1 by a, <I>Q</I>+5 by b, multiplying by <I>n</I>, and defining
<I>u</I>(<I>n</I>)=<I>n</I><I>t</I>(<I>n</I>). This leaves us with the equation in <a href="9602d3f1.gif">Figure 1</A> (b). We then choose values of <I>n</I> for which the troublesome floor
and ceiling functions relent: powers of 2, or <I>n</I>=2<I><SUP>m</I></SUP>. The result is in <a href="9602d3f1.gif">Figure 1</A> (c). <P>

The relation <I>n</I>/2=2<I><SUP>m</I></SUP><SUP>-1</SUP> suggests a  change of notation, so we
define <I>v</I>(<I>m</I>)=<I>u</I>(2<I><SUP>m</I></SUP>) as in <a href="9602d3f1.gif">Figure 1</A>
(d). This is finally a standard, first-order recurrence, which can be solved by the simple iteration in <a href="9602d3f1.gif">Figure 1</A> (e). <P>

So much for these special arguments; writing <I>m</I>=<I>lg n</I> (binary logarithm), we have
<I>u</I>(<I>n</I>)=<I>n</I>(<img src="beta12.gif"><I>lg</I> <I>n</I>+<img src="alpha12.gif">). Clearly, <I>n</I>(<img src="beta12.gif"> <I>lg</I> <I>n</I>+<img src="alpha12.gif">) cannot be
right for all values of <I>n</I>, since the recurrence generates integer coefficients and <I>lg n</I> is not
integral unless <I>n</I> is a power of 2. We must return to the original recurrence for <I>u</I>(<I>n</I>),
in <a href="9602d3f1.gif">Figure 1</A> (b), and use it--floors, ceilings, and all--to generate several
values, compare them with the special solution, and look for a pattern. Here, after computing about ten
values, everything falls into place; when <I>n</I>=2<I><SUP>m</I></SUP>+<I>r</I>, 0 lt;/=
<I>r</I>lt;2<I><SUP>m</I></SUP>, where <I>m</I>=<img src="hfbrdlf12.gif" alt="|_"><I>lg n</I><img src="hfbrdrt12.gif" alt="_|">, then the solution in <a href="9602d3f1.gif">Figure 1</A> (f) fits all the generated values. When <I>r</I>=0, we get back to the
special solution. Testing by substitution into <a href="9602d3f1.gif">Figure 1</A> (b) is successful, so
<I>t</I>(<I>n</I>)=<I>u(n</I><I>)/</I><I>n</I>=<img src="alpha12.gif">+<img src="beta12.gif"> (<img src="hfbrdlf12.gif" alt="|_"><I>lg n</I><img src="hfbrdrt12.gif" alt="_|">+2<I>r</I>/<I>n</I>). Since
<I>r</I>/<I>n</I> lt; 1, this result supports the standard claim that &quot;BS runs in logarithmic
time.&quot;<P>

So much for the <I>expected</I> run time. Determining its variability requires a different kind of
computation, one that addresses the probabilistic structure directly. I show this in my book; a partial result
is that the number of iterations has only two possible values, <img src="hfbrdlf12.gif" alt="|_"><I>lg n</I><img src="hfbrdrt12.gif" alt="_|"> and <img src="hfbrulf12.gif"><I>lg n</I><img src="hfbrurt12.gif"> (if
<I>n</I>=2<I><SUP>m</I></SUP>, these are both <I>m</I>).<P>

<h3><a name="00d3_0043">Recursion Elimination<a name="00d3_0043"></h3><P>

Having computed the expected cost of BS, let's try to reduce it. One obvious source of cost is the recursive
call at each level of the procedure. This is &quot;tail recursion,&quot; where the last instruction
performed at each level is a single recursive call. The call is easy to replace using a single loop; see <a href="9602m.htm#00d3_0046">Example 2</A>. The logic is identical to that of BS, except that a new
call is not initiated at each level. Instead, either the left or right end of the interval is adjusted.<P>

The logical structure of such a program parallels that of a recursive program, but the analysis is entirely
different. In BS, every instruction was done (at most) once at each level; not so in BS1. That's why the
left-most column is added in <a href="9602m.htm#00d3_0046">Example 2</A>: The number of times
each line is repeated is given as a function of the searched interval size. The symbol <I>a</I>(<I>n</I>)
denotes the number of times that the main instructions (lines 4 and 5) are executed. We define
<I>p</I><SUB>1</SUB> as the probability that the condition in line 5 is satisfied and
<I>q</I><SUB>1</SUB>=1-<I>p</I><SUB>1</SUB>. We make the crucial assumption that
<I>p</I><SUB>1</SUB> depends in the same way on the interval size each time line 5 is reached. This is the
precise analog of the assumption we made when analyzing BS--and for the same reason. Regardless of
which subinterval still needs to be searched, the assumption that <I>x</I> is equally likely to be in any
position translates without change: The previous comparisons delimit the subinterval to be searched, but
provide no other information on the location of <I>x</I>. Hence <I>p</I><SUB>1</SUB> always has the same
functional form as at the first iteration, <img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">/<I>n</I>, and
<I>q</I><SUB>1</SUB>=<img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif">/<I>n</I>.<P>

If <img src="alpha12.gif">' is the cost of the lines performed once--1, 2, 3, and 8--then <img src="alpha12.gif">'=<I>Q</I>+2. <img src="beta12.gif">' counts the cost that
recurs <I>a</I>(<I>n</I>) times--lines 3, 4, 5, 6, or 7--so <img src="beta12.gif">'=6. The equation to determine
<I>a</I>(<I>n</I>) follows from a description of the search: Starting with an interval of size <I>n</I>, one
iteration leaves us with an interval of the size <img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|"> (in probability <img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">/<I>n</I>) or
<img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif"> (with the complementary probability <img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif">/<I>n</I>); hence the equation in <a href="9602d3f2.gif">Figure 2</A> (a). Up to a factor of <img src="beta12.gif">, this is the same as <a href="9602d3f1.gif">Figure 1</A> (a), so the solution is familiar: <I>n</I>=2<SUP><img src="hfbrdlf12.gif" alt="|_"></SUP><I><SUP>lg
</I></SUP><I><SUP>n</I></SUP><SUP><img src="hfbrdrt12.gif" alt="_|"></SUP>+<I>r</I> yields the result in <a href="9602d3f2.gif">Figure 2</A> (b). The amount saved by this modification is
<I>t</I>(<I>n</I>)-<I>t</I><SUB>1</SUB>(<I>n</I>), and writing it in full yields the equation in <a href="9602d3f2.gif">Figure 2</A> (c).<P>

The last relation allows us to estimate <I>Q</I> (without reading arcane compiled code). <a href="9602m.htm#00d3_0048">Table 1</A> represents the average time per search (in <img src="mu12.gif">s) on a
standard workstation running under a UNIX-like operating system. Consider for the moment only the left
four columns; compute for each <I>n</I> the value <img src="hfbrdlf12.gif" alt="|_"><I>lg n</I><img src="hfbrdrt12.gif" alt="_|">+2<I>r</I>/<I>n</I> and perform a linear
regression between these values and the column labeled BS-BS1. This yields the intercept -0.248 and slope
0.383. Since the intercept (in admittedly flexible units) is given in <a href="9602d3f2.gif">Figure 2</A>
(c) as -1, the value of <I>Q</I> in these units is approximately 2.54--about two and a half basic
operations.<P>

The cost of recursion is not trivial for a procedure with such a short body. Still, it is hardly onerous.
However, this evaluation disregards two important facts:<P>
<ul>
<li><I>Q</I> depends on the number of variables pushed into (or popped from) the stack; here
3(<I>i</I>,<I>j</I>,<I>x</I>). This is actually two addresses and <I>x</I>. It would differ for calls with
other argument vectors.</li>
<li><I>Q</I>'s value is materially influenced by details in the implementation of stack operations and cache
and system-storage management. On an older, slower minicomputer with essentially the same operating
system, <I>Q</I>approximately equals 1.9. This approach to estimating <I>Q</I> failed on a different
UNIX-like system, when BS suffered penalties for recursions of depth beyond 5 that raised the running time
by an unexpected factor of 5 to 8! (Yet another experiment suggested that <I>Q</I> without
stack-management overhead is somewhat less than 2.) This was the highest price I saw paid for a flexible,
dynamic memory-management policy.</li>
</ul>

<h3><a name="00d3_0044">Three-Way Comparison<a name="00d3_0044"></h3><P>

The next modification was prompted by the observation that <I>T</I>(<I>n</I>) is essentially constant,
while BS and BS1 sometimes &quot;locate&quot; the desired key early in the search (BS and BS1 are
unaware of this, since equality is never tested, only inequality). The value <I>x</I> could be in position
<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|"> (located at the first stab) but the routines would just go chugging along, only to return there
<I>lg n</I> steps later. <a href="9602m.htm#00d3_0047">Example 3</A> adds just such a test. While
BS1 always makes two comparisons per iteration, BS2 needs two in about half of its iterations, and three
in the other half--but presumably fewer iterations. Is this a good idea? Analysis should provide the answer:
We find in BS2 the same probability <I>p</I><SUB>1</SUB> used before,
Pr(<I>x</I>lt;A[<I>k</I>])=<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">/<I>n</I>, In addition, we have <I>p</I><SUB>2</SUB>, defined
as the probability that <I>x</I>=A[<I>k</I>], given that <I>x</I> is not in the first <I>k</I> positions; hence
<I>p</I><SUB>2</SUB>=1/(<I>n</I>-<I>k</I>). The dependence of the exact number of operations on the
results of the comparisons is somewhat more complex here. It is evaluated by considering the average
number of iterations BS2 will perform, again assuming that <I>x</I> is in the array and is equally likely to
be any element. Reading the evolution of <I>s</I>(<I>n</I>) from the code in <a href="9602m.htm#00d3_0047">Example 3</A> yields the equation in <a href="9602d3f3.gif">Figure
3</A> (a). This equation is more complex then those presented so far; for example, two initial values are
required to drive the recursion. They too can be deduced by reading BS2: <I>s</I>(1) is obviously 0, and
<I>s</I>(2) is always 1. Since <I>k</I>=<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">, then <I>n</I>-<I>k</I>-1=<img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif">-1.<P>

Define the function <I>b</I>(<I>n</I>)<I>ns</I>(<I>n</I>), to get the equation in <a href="9602d3f3.gif">Figure 3</A> (b), where the
initial values are <I>b</I>(1)=0 and <I>b</I>(2)=2. Picking <I>n</I> as a power of 2 does not help as before;
instead, we pick <I>n</I>=2<I><SUP>m</I></SUP>-1,
then<img src="hfbrulf12.gif"><I>n</I>/2<img src="hfbrurt12.gif">-1=<img src="hfbrdlf12.gif" alt="|_"><I>n</I>/2<img src="hfbrdrt12.gif" alt="_|">=2<I><SUP>m</I></SUP><SUP></SUP>-<SUP>1</SUP>-1. We find a
first-order recursion, similar to the equation in <a href="9602d3f1.gif">Figure 1</A> (c). This is shown
in <a href="9602d3f3.gif">Figure 3</A> (c). Using the initial value for <I>m</I>=1, which is
<I>b</I>(2-1)=1<B></B><img src="dot12.gif"><I>s</I>(1)=0, a simple unreeling reveals the solution in <a href="9602d3f3.gif">Figure 3</A> (d). Trying to guess the general solution is more difficult (I needed
more than 30 terms to see the pattern) because the increments change as <I>n</I> reaches 3/2
<I>n</I><SUP>*</SUP> (where <I>n</I><SUP>*</SUP> represents 2<SUP><img src="hfbrdlf12.gif" alt="|_"></SUP><I><SUP>lg
</I></SUP><I><SUP>n</I></SUP><SUP><img src="hfbrdrt12.gif" alt="_|"></SUP>). <a href="9602d3f4.gif">Figure 4</A> (a) shows the
guessed solution; <img src="dblarr12.gif">(<I>n</I>) is given in <a href="9602d3f4.gif">Figure 4</A> (b). Substitution confirms
this as a solution to the equation in <a href="9602d3f3.gif">Figure 3</A> (b).<P>

Finally we compare the costs <I>t</I><SUB>1</SUB>(<I>n</I>) and <I>t</I><SUB>2</SUB>(<I>n</I>). First,
how many iterations, on average, does BS2 save over BS1? The difference is
<I>a</I>(<I>n</I>)-<I>s</I>(<I>n</I>), and is given in <a href="9602d3f5.gif">Figure 5</A> (a) for the
range <I>n</I><SUP>*</SUP><img src="lteq12.gif"><I>n</I>lt;3/2<I>n</I><SUP>*</SUP>. <P>

As <I>n</I> increases from <I>n</I><SUP>*</SUP> to 3/2<I>n</I><SUP>*</SUP>, this difference varies
from 1.5 to 0.66 (approximately). In the rest of the range, the equation in <a href="9602d3f5.gif">Figure 5</A> (b) varies correspondingly, approximately from 1.33 to 1.5. The
savings are modest and are essentially independent of the size of <I>n</I>, but dependent on the ratio
<I>n</I>/<I>n</I><SUP>*</SUP>. Hence the difference between the running times can be roughly set to <a href="9602d3f5.gif">Figure 5</A> (c).<P>

Allow the same cost <I>C</I> for any operation (comparison, arithmetic, or assignment) so that the two
cost factors in <a href="9602d3f5.gif">Figure 5</A> (c) are nearly <I>6C</I> and <I>C</I> (for two
operations, at about half the iterations), respectively. Using only <I>lg </I><I>n</I><SUP>*</SUP>, the
leading term from the solution for <I>s</I>(<I>n</I>), shows that BS1 is better when the equation in <a href="9602d3f5.gif">Figure 5</A> (d) applies.<P>

If we take 4/3 as a representative value for <img src="delta12.gif" alt="d"><I><SUB>n</I></SUB>, we can expect the extra check in BS2
to pay off unless the array is more than about 200 entries long, with a relatively important dependence on
the ratio <I>n</I>/<I>n</I><SUP>*</SUP>. <a href="9602m.htm#00d3_0048">Table 1</A> bears this
out. Since the logarithm of <I>n</I><SUP>*</SUP> appears in this relation, the critical length is sensitive
to minor variations in the implementation. Higher precision requires consideration of the compiler and the
machine code it generates, as well as details of the machine architecture, such as availability of
increment/decrement instructions, separate data and instruction caches, and the like. <P>

Note that in some machines, condition codes (with computed branches) allow for a three-way comparison at
no added cost. If a compiler is designed to take advantage of this and can use a single comparison for lines 5
and 7 of BS2, then it will outperform BS1 on the average for any value of <I>n</I>. Another possibility is to
code it directly in assembly language. Since binary search is used so frequently, this may be a reasonable
approach.<P>

<h4><a name="00d3_0045"><B>Example 1:</B> BS, recursive form of binary search.<a name="00d3_0045"></h4><P>

<pre>(a)
1. BS(<I>A</I>[<I>i</I>..<I>j</I>],<I>x</I>)
2.    if (<I>i</I>==<I>j</I>) return <I>i</I>;                   /* This is it! */
3.    <I>k</I>=(<I>i</I>+<I>j</I>+1)/2;                          /* Computes <img src="hfbrdlf12.gif" alt="|_">(<I>i</I>+<I>j</I>+1)/2<img src="hfbrdrt12.gif" alt="_|"> */
4.    if (<I>x</I>lt;<I>A</I>[<I>k</I>])return BS(<I>A</I>[<I>i</I>..<I>k</I>-1],<I>x</I>); /* When <I>x</I> is below, */
5.          else return BS(<I>A</I>[<I>k</I>..<I>j</I>],<I>x</I>);     /* or above the midpoint */

(b)
2. if (<I>i</I>==<I>j</I>) {if (<I>x</I>==<I>A</I>[<I>i</I>])  return <I>i</I>; else return -1;}</pre>

<h4><a name="00d3_0046"><B>Example 2:</B> BS1, first iterative form of BS.<a name="00d3_0046"></h4><P>

<pre>
(1)           1. BS1(<I>A</I>[<I>n</I>],<I>x</I>)
(1)           2.     <I>i</I>=0; <I>j</I>=<I>n</I>-1;
(<I>a</I>(<I>n</I>)+1)     3.     while <I>i</I>lt;<I>j</I>
(<I>a</I>(<I>n</I>))       4.     {<I>k</I>=(<I>i</I>+<I>j</I>+1)/2;
(<I>a</I>(<I>n</I>))       5.       if <I>x</I>lt;<I>A</I>[<I>k</I>]
(<I>p</I><I><SUB>1</I></SUB> <I>a</I>(<I>n</I>))   6.         <I>j</I>=<I>k</I>-1;
(<I>q</I><I><SUB>1</I></SUB> <I>a</I>(<I>n</I>))   7.       else <I>i</I>=<I>k</I>;
(1)           8.     }  return <I>i</I>;</pre>

<h4><a name="00d3_0047"><B>Example 3:</B> BS2, second iterative form of BS.<a name="00d3_0047"></h4><P>

<pre>(1)           1. BS2(<I>A</I>[<I>n</I>],<I>x</I>)
(1)           2.     <I>i</I>=0; <I>j</I>=<I>n</I>-1;
(<I>s</I>(<I>n</I>)+1)     3.     while <I>i</I>lt;<I>j</I>
(<I>s</I>(<I>n</I>))       4.     {<I>k</I>=(<I>i</I>+<I>j</I>+1)/2;
(<I>s</I>(<I>n</I>))       5.       if <I>x</I>lt;<I>A</I>[<I>k</I>]
(<I>p</I><I><SUB>1</I></SUB><I>s</I>(<I>n</I>))    6.          <I>j</I>=<I>k</I>-1;
(<I>q</I><I><SUB>1</I></SUB><I>s</I>(<I>n</I>))    7.       else if (<I>x</I>gt;<I>A</I>[<I>k</I>])
(<I>q</I><I><SUB>1</I></SUB><I>q</I><I><SUB>2</I></SUB><I>s</I>(<I>n</I>)) 8.         <I>i</I>=<I>k</I>+1;
(<I>q</I><I><SUB>1</I></SUB><I>p</I><I><SUB>2</I></SUB>)      9.       else <I>i</I>=<I>j</I>=<I>k</I>;
(1)          10.     } return <I>i</I>;</pre>

<h4><B><a href="9602d3f1.gif">Figure 1</A>:</B> Analyzing BS.</h4><P>

<h4><B><a href="9602d3f2.gif">Figure 2</A>: </B>Recursion elimination.</h4><P>

<h4><B><a href="9602d3f3.gif">Figure 3</A>:</B> Three-way comparison.</h4><P>

<h4><B><a href="9602d3f4.gif">Figure 4</A>:</B> Form of the guessed solution.</h4><P>

<h4><B><a href="9602d3f5.gif">Figure 5</A>:</B> Determining what BS2 saves over BS1.</h4><P>

<h4><a name="00d3_0048"><B>Table 1:</B> Average time per search (in ms) <a name="00d3_0048"></h4><P>

<pre>
n        BS     BS1  BS-BS1   BS2  BS1-BS2

10      4.22   2.99   1.23   2.76    0.23
100     7.13   4.93   2.20   4.71    0.22
1000   10.18   6.88   3.30   6.99   -0.11
10000  14.96   9.87   5.09  10.45   -4.51
</pre>

</BODY></HTML>
