<HTML> 
<META NAME="year" CONTENT="1996">
<HEAD>
<TITLE>MAY96: Writing a Portable Transport-Independent Web Server</TITLE>
</HEAD>
<body bgcolor="FFFFFF"> <h1>Writing a Portable Transport-Independent Web
Server<a name="01bc_0094"><a name="01bc_0094"><a name="01bc_0092"><a name="01bc_0000"></h1><P>

<h2>TLI makes HTTP transport independent</h2><P>

<h3>John Calcote</h3><P>

<P> <i><a name="01bc_0093"><a name="01bc_0000">John is an engineer on Novell's
Directory Services team. He can be contacted at
jcalcote@novell.com.</i></p><hr><P>

Like all standards, HTTP is being asked to do things its designers never
considered--implementing reliable user authentication for secure business
transactions over the Internet, displaying information in newly created formats,
and the like. Consequently, new versions of the standard are under development in
an attempt to solve emerging problems without losing backward compatibility.<P>

One of the more solvable limitations of most contemporary HTTP servers (those
based on the NCSA or CERN implementations) is that they are implemented on top of
Berkeley Sockets, and thus closely tied to TCP/IP. To address this problem in a
recent project, I chose to implement my Web server on top of the Transport Layer
Interface (TLI). Because TLI is not a transport service provider (TSP), but
rather an abstraction layer over various TSP's, my Web server will still
communicate with existing Sockets-based browsers, as long as I choose to listen
on a TCP socket for Web requests. As it turns out, TLI source code is 98 percent
transport independent (even transport unaware). I demonstrate this fact in the
accompanying source code (available electronically; see &quot;Availability,&quot;
page 3) by creating two listen threads, one for TCP/IP and one for Novell's SPX
II. Both types of requests are mapped to TLI file handles, and are serviced by
the same Web server code path.<P>

<h3><a name="01bc_0095">The OSI Communications Model<a name="01bc_0095"></h3><P>

TLI was developed by AT&amp;T in an effort to solve some of the problems
associated with Berkeley Sockets and to stratify some of the many inconsistent
communication interfaces in the UNIX kernel at the time. In the first place, BSD
Sockets are actually part of the operating-system kernel. The BSD Sockets
functions are system calls. The left side of <a href="9605n1f1.gif">Figure 1</A>
shows the seven layers of the OSI communications model. The right side indicates
the dividing line between those portions of the model that are implemented in the
operating-system kernel and those portions implemented in application space. BSD
Sockets are implemented at the network layer of the OSI model. Because of its
design, it is &quot;hard-wired&quot; into the TCP/IP and UDP/IP protocols.<P>

TLI is implemented at the OSI model's transport layer. These functions actually
become a part of the network application. TLI is implemented as a library of
functions that are either linked into the application statically at compile time
or dynamically at run time in the form of a loadable DLL. In either case, the
code for TLI runs in the same address space as the application, rather than in
kernel space. The implications of this difference are subtle. First, since TLI is
not part of the kernel, some method must be provided for it to communicate with
system resources and networking hardware. Since portability was a major goal in
TLI's design, the UNIX STREAMS interface was chosen as a system-level interface
because of its multipurpose, generic functionality. Most ports of TLI are
implemented on top of a port of the STREAMS subsystem (although this is not a
requirement), which provides an efficient, integrated, full-duplex pipeline to
kernel device drivers.<P>

As <a href="199601bf.htm">Figure 2</A> shows, there are actually three components
to TLI: <P>
<ul>
<li>User-level library, containing the application-visible interface.</li>
<li>Transport-service provider, chosen by the programmer when a TLI end point is
opened.</li> <li>STREAMS subsystem, providing the POSIX <I>open()</I>,
<I>close()</I>, <I>read()</I>, <I>write()</I>, and <I>ioctl()</I> functions
necessary to establish network-communication channels in a manner consistent with
file-system access. </li>
</ul>
TLI is implemented as a particular state machine. A TLI
end point at any given time depends on a number of factors, including the
initialization stage of the end point and asynchronous events that have been
received by TLI on the end point.<P>

<h3><a name="01bc_0096">Local Management<a name="01bc_0096"></h3><P>

Local management of TLI data structures is straightforward. The <I>t_open()</I>
function initializes and returns an OS file descriptor based on a particular
Transport Service Provider (TSP), such as TCP or Novell's SPX II. How you specify
the TSP is implementation dependent, but generally, it takes the form of a string
containing a UNIX-like file system path to the desired device; for example
<I>/dev/tcp</I> or <I>/dev/nspx</I>.<P>

Once a file descriptor has been opened and assigned to a TSP, it may be bound to
a specific or arbitrary network address using <I>t_bind()</I>. This phase creates
one of the few places in TLI application code that is transport specific,
however, the transport-specific information is concise enough to be passed into a
generic routine as a parameter. For instance, an <I>OpenEndPoint()</I> function
might take a string signifying the desired transport provider. The <I>t_bind</I>
structure contains two fields, a <I>netbuf</I> structure and an integer,
<I>qlen,</I> used to specify the maximum number of simultaneous, outstanding
indications a server is willing to handle; see <a href="9605g.htm#01bc_009a">Example 1</A>.<P>

The <I>netbuf</I> structure is generic enough to handle any sort of
transport-specific network address or data imaginable. It is basically a sized
buffer of variable length. The <I>maxlen</I> field tells TLI how large the buffer
actually is (allocation size), while the <I>len</I> field tells the application
how much data was returned by TLI.<P>

Once bound to a network address, a TLI file descriptor may be used by a client to
request a connection with <I>t_connect()</I>, or it may be used by a server to
listen for a connect indication with <I>t_listen()</I>. When <I>t_listen()</I>
returns, signifying that a connect indication has arrived, the server may accept
the indication on the server socket or on another opened and bound end point. If
the server accepts on the server socket, then no other indications may be
serviced until the session has been terminated, at which point the server may
reopen the socket and return to a blocked state by calling <I>t_listen()</I>. If
the server accepts on another end point (as is typical), then it may immediately
return to <I>t_listen()</I> on the server socket after calling <I>t_accept()</I>
and passing the new end point to a responder thread or process.<P>

The <I>t_accept()</I> function takes a listening-file descriptor, a
receiving-file descriptor, and the <I>t_call</I> structure used in
<I>t_listen()</I>. This structure contains three <I>netbuf</I> structures and an
integer value representing the indication sequence number; see <a href="9605g.htm#01bc_009b">Example 2</A>. For most situations the <I>opt</I> and
<I>udata</I> buffers are not used. The <I>addr</I> buffer contains the address of
the calling client, in a transport-specific format, and the sequence value
specifies which of the incoming indications this call is associated with.<P>

The trivial case of <I>qlen</I> is 1. If <I>qlen</I> is set to 1 when a server
port is bound to a TLI end point, then all attempts to connect to the server on
that descriptor while it is processing a previously received connect indication
are rejected by the underlying TSP. This case is far easier to code, but makes
for a rather nonrobust server. If <I>qlen</I> is greater than 1, then outstanding
connect indications must be queued up by the server while it is processing the
current indication. The difficulty in coding servers using a <I>qlen</I> greater
than 1 lies in the fact that asynchronous incoming indications must be gathered
off the wire before <I>t_accept()</I> will work. The server is notified of such
an asynchronous event by <I>t_accept()</I> failing with an error code of TLOOK.
At this point, the server needs to call <I>t_look()</I> to retrieve the event,
and then process it according to the nature of the event.<P>

Two possible events may occur during <I>t_accept()</I>. The T_LISTEN event
indicates that another connect indication has arrived, while the T_DISCONNECT
event means that someone previously queued up for connect desires to cancel his
connect indication. If the event is T_LISTEN, you call <I>t_listen()</I> with a
new call structure and queue it up for later processing. If the event is
T_DISCONNECT, you call <I>t_rcvdis()</I> to retrieve the sequence number of the
disconnecting client, scan the indication queue for the <I>t_call</I> block
containing this sequence number, and then remove it from the waiting queue. You
then return to <I>t_accept()</I> to try accepting the original connect indication
again. Asynchronous events could keep you from accepting the current indication
until you have filled your outstanding indication queue with <I>qlen</I> connect
indications.<P>

When you have finally accepted the indication (establishing the connection on a
new end point), you either fork a new process to handle the client's request, or
begin a new thread in a multithreaded environment, passing the client file
descriptor.<P>

The server must handle all outstanding connect indications before returning to
<I>t_listen()</I> to wait for another incoming indication. It's quite apparent
that on a busy day the server may never get back to blocking on
<I>t_listen(</I><I>)</I>!<P>

<h3><a name="01bc_0097">TLI File Descriptors<a name="01bc_0097"></h3><P>

As in Berkeley Sockets, the end points created by TLI are operating-system file
descriptors. TLI opens such a descriptor by calling the STREAMS <I>open()</I>
function, and pushing the TIMOD STREAMS module onto the stream for this
descriptor. This is where the real fun begins. The request-handler thread cannot
tell the difference between a TLI file descriptor and a standard file descriptor,
because all file descriptors in the UNIX environment are STREAMS based. This
means that the standard C library function <I>fdopen()</I> may be called on the
descriptor to open an I/O stream buffer for the client file handle. (Note that
these standard C library functions are not at all related to the UNIX STREAMS
interface, which is actually based on POSIX.) Once this is done, all of the
standard file I/O functions may be used on the descriptor. Input is gathered from
the client using <I>fgets()</I>, <I>fgetc()</I>, or <I>fread()</I>, and data may
be sent to the client using <I>fprintf()</I>, <I>fputs()</I>, <I>fputc()</I>, or
<I>fwrite()</I>.<P>

Furthermore, in the UNIX heavy-weight process environment, it is common for the
server to redirect the child process's <I>stdio</I> to the client's network file
descriptor before forking, by using the POSIX <I>dup2()</I> function to duplicate
the client descriptor on the system's <I>stdin</I> and <I>stdout</I> file
descriptors. This trick allows the responder to retrieve all client input using
the even simpler <I>stdio</I> interface provided in <I>gets()</I> and
<I>printf()</I>. It makes for some really interesting network application code to
see all client I/O being handled with <I>stdio</I> functions.<P>

When a session is complete, the server simply calls <I>fclose()</I> on the file
pointer to terminate the session. If <I>stdio</I> is redirected, as in the UNIX
server environment, the startup code of the responder process automatically calls
<I>fclose()</I> on the <I>stdio</I> file descriptors when the main function
returns. The HTTP protocol depends on orderly release of the connection for
correct operation. Beware that some ports of TLI do not correctly handle orderly
release of the connection when buffered I/O is used in this manner. Since TLI is
an open specification, not all TLI ports implement the full functionality of the
specification. The specification even documents those portions that are optional.
The way to handle this deficiency is to use the <I>t_sndrel()</I> and
<I>t_rcvrel()</I> functions on the embedded file descriptor before calling
<I>fclose()</I> on the file pointer. <P>

Essentially, when the server is finished sending data, it calls
<I>t_sndrel()</I>, and then blocks on a call to <I>t_rcvrel()</I> while waiting
for the client to indicate that it has finished reading this data. The server
will wake up when the client either calls <I>t_sndrel()</I>, or aborts the
connection with <I>t_close()</I>. This handshake ensures that the client has
received and processed all the data sent by the server before the connection is
torn down. By definition, an abortive release on a TLI file descriptor will
truncate all data not explicitly received by the client.<P>

<h3><a name="01bc_0098">The Hypertext Transfer Protocol<a name="01bc_0098"></h3><P>

HTTP is a line-oriented, ASCII protocol. The most common method of handling I/O
in the responder is to call <I>fgets()</I> to retrieve the client's request
strings, and then send data back using <I>fprintf()</I> or <I>fputs()</I>.<P>

There are several (ever-changing) types of HTTP requests, including GET, HEAD,
PUT, POST, LINK, UNLINK, and DELETE. Of these, GET is by far the most popular,
followed by POST. Since 95 percent of all requests are GET requests, I'll limit
my discussion to it.<P>

An HTTP GET request is made up of a single request line, followed by several MIME
header fields, and then a blank line. If, as in the case of POST or PUT, the
request contains any data, then one of the MIME header fields should be
&quot;content-length.&quot; The value in this field will specify how many bytes
of data are to follow the blank line. Since GET requests don't contain data, this
is not a concern for us. You simply read until you hit a blank line, and then you
can start sending your response. In reality, since these TLI sessions are full
duplex, you can send data before you have retrieved it all. This means that you
could read the request line, and ignore any MIME headers sent by the client. Some
of the most-common MIME headers are listed in <a href="9605g.htm#01bc_009c">Table
1</A>. An HTTP response is formatted in a manner similar to a request. The first
line contains a signature string, a status code, and a human-readable form of the
status. A valid &quot;OK&quot; response might look like &quot;HTTP/1.0 200 Here
she comes!&quot;. <P>

After the status-code line, the server sends zero or more MIME headers indicating
the type and length of the file being sent. Again, a blank line follows the
header, and then the file data itself. At the very least, a server should send a
content-type header field. If a content-length header field is sent, the client
can display a progress bar to the user as the transfer is taking place. <P>

Writing an entire Web server is a topic for more than one article, but the
front-end server code is a large portion of any server (and a daunting one at
that). The sample code in the source accompanying this article should give you a
good headstart if you're planning on using TLI for a transport interface. Like
all programmer's interfaces, there is a learning curve, but TLI has good
documentation and was designed to be easy to learn and easy to use.<P>

<h3><a name="01bc_0099">References<a name="01bc_0099"></h3><P>

Graham, Ian S. <I>The HTML Sourcebook.</I> New York, NY: John Wiley &amp; Sons,
1995.<P>

<I>NLM Transport Interfaces</I> (included with the Network C for NLMs Software
Developer's Kit). Novell Inc.: September 1991. <P>

Rago, Stephen A. <I>Unix System V Network Programming.</I> Reading, MA:
Addison-Wesley, 1993.<P>

<I>The Novell Software Developer's Kit CD</I>, volume 5. Novell Inc.: 1995. <P>

<h4><B><a href="9605n1f1.gif">Figure 1</A>:</B> OSI communications model.</h4><P>

<h4><B><a href="199601c1.htm">Figure 2</A>:</B> TLI component block diagram.
</h4><P>

<h4><a name="01bc_009a"><B>Example 1:</B> The t_bind structure contains two fields, a netbuf structure and an integer, glen.<a name="01bc_009a"></h4><P>
<pre>
struct netbuf {
    unsigned int maxlen;
    unsigned int len;
    char *buf;
};
struct t_bind {
    struct netbuf addr;
    unsigned int qlen;
};
</pre>
<h4><a name="01bc_009b"><B>Example 2:</B> The t_call structure used in t_listen(). <a name="01bc_009b"></h4><P>
<pre>
struct t_call {
    struct netbuf addr;
    struct netbuf opt;
    struct netbuf udata;
    int sequence;
};
</pre>
<h4><a name="01bc_009c"><B>Table 1:</B> Common HTTP response headers.<a name="01bc_009c"></h4><P>
<pre>
Accept:
Accept-encoding:
Authorization:
Content-Length: <P>  (POST or PUT requests only)
Content-Type: <P>  (POST or PUT requests only)
From:
If-Modified-Since:
Pragma:
Referrer:
User-Agent:

Content-Encoding:
Content-length:
Content-Transfer-Encoding:
Content-type:
Date:
Expires:
Last-modified:
MIME-version:
Server:
Title:
WWW-Authenticate:
WWW-Link:
</pre>

</BODY></HTML>
