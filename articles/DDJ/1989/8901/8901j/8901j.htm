<HTML>
<META NAME="year" CONTENT="1989">
<HEAD>

<TITLE>JAN89: EDITORIAL</TITLE></HEAD>
<body bgcolor="FFFFFF">
<h1>EDITORIAL<a name="001b_0003"><a name="001b_0003"></h1><P>
<h3><a name="001b_0001"><a name="001b_0000">Jonathan Erickson</h3><P>
<a name="001b_0002"><a name="001b_0000">Depending on who you talk to, the first IEEE international neural network conference (which took place nearly two years ago) was either a surprising success or a woeful failure. The naysayers point to a relatively small number of people who showed up to hear a smattering of papers, share some theories, and see a dozen or so exhibits.  The yeasayers, however, insist that the conference was successful simply because people showed up at all, especially considering that as recently as a couple of years ago few people knew what neural nets were, let alone how they could be used.<P>
(As an aside, neural nets, as defined by Doug Palmer of Hecht-Nielsen Neurocomputers, are highly parallel dynamical systems that process information by means of response to continuous input.  The structure of a neural net consists of a large number of processing elements, each with multiple inputs, but with a single output.  In addition to the processing element, a neural net is made up of an interconnection scheme [topology], a learning model, and knowledge of the state of the system.  Together, these elements are called the network paradigm; currently, about 20 distinct paradigms have been identified.)<P>
There's little question, however, that the second international neural network conference, which took place in July of 1988, was a success.  Nearly two thousand attendees--including members of the DDJ staff (see Michael Swaine's &quot;Programming Paradigms&quot; column of October, 1988)--converged on San Diego to find out what had happened in the world of neural nets over the space of a year.  What did they talk about?  Among the dozens of papers presented were those on topics such as &quot;Neural Computation for Controlling the Configuration of 2-Dimensional Truss Structure,&quot; &quot;Neural Network Simulation at Warp Speed: How We Got 17 Million Connections per Second,&quot; &quot;Abilities and Limitations of a Neural Network Model for Spoken Word Recognition,&quot; &quot;Neural Network Models and Their Application to Handwritten Digit Recognition,&quot; and &quot;A Multilayer Perception Network for the Diagnosis of Low Back Pain.&quot;<P>
More significantly, the emphasis at the second conference, as well as more recent technical conferences like the Fall ACM or the Pattern Recognition and Advanced Missile System conferences, was on the practical, rather than theoretical, side of neural networking.  It's becoming apparent that neural nets are rapidly moving out of research labs and into commercial development environments because neural technology has the capability of providing effective solutions to many kinds of complex computing tasks.  A good example of this is the kind of applications under development at companies like Accurate Automation (a company located in Tennessee, a state that surprisingly has become a hotbed of neural net development).  AA is building commercial neural net systems for tasks like monitoring the behavior of aircraft engines, for real-time and process control, and even for monitoring of local area networks and database activities using a form of neural nets called the &quot;nearest neighbor classifier&quot; that looks for emergency patterns and reacts accordingly.  The basic questions being asked by programmers who are becoming neural net aware concern which neural net paradigms should be applied in specific situations, not whether or not neural nets are right at all.<P>
With this backdrop, this month's examination of neural networks looks at the technology from the practical side.  Todd King's lead article will give you a feel for what nets are about, and provide you with a new tool for solving old programming problems.  Casey Klimasauskas' article, and Steven Melinkoffs sidebar, give you a chance to see how commercial neural net packages differ from the roll-your-own approach taken by Todd.  Together, our pair of neural net articles also lets you examine the main types of problems--primarily pattern recognition and noise filtering--neural networks are currently being used to solve.<P>
In short, neural nets have come a long way since DDJ last looked at them over a year-and-a-half ago, and you can be assured that, over the next 18 months, you'll see new uses and even more startling developments in this emerging technology.<P>
<P>
<P>
</pre><HR><P>Copyright &copy; 1989, <I>Dr. Dobb's Journal</I></P></BODY></HTML>
