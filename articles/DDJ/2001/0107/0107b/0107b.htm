<html>
<head>
<title>Jul01: Automatically Exploiting Implicit Parallelism</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Automatically Exploiting Implicit Parallelism</h1>
<p><i>Dr. Dobb's Journal</i> July 2001</p>

<h3>By Aart J.C. Bik, Milind Girkar,  Paul M. Grey, and Xinmin Tian</h3>

<I>
The authors are researchers at Intel's Microcomputer Software Laboratories. They can be contacted at aart.bik@intel.com.</I>

<hr>

<p>Recent trends in processor design have introduced new ways for programmers to exploit parallelism. Some of these trends provide SIMD instructions that let multiple functional units operate simultaneously on so-called packed data elements (that is, relatively short vectors that reside in memory or registers). The Pentium processor with 64-bit MMX technology, for example, supports integer operations on eight packed bytes, four packed words, or two packed dwords. Here, a single instruction such as <i>paddb</i> provides eight-way SIMD parallelism by simultaneously adding 8 bytes in the source operand to 8 bytes in the destination operand. The Pentium III and Pentium 4 introduced 128-bit Streaming-SIMD-Extensions (SSE and SSE2), which provide similar SIMD support for floating-point operations on four single-precision floating-point numbers or two double-precision numbers, respectively. In addition, SSE2 widens the integer MMX technology into 128-bit technology. The reduced cost of processor manufacturing has made another trend possible, namely the introduction of shared-memory multiprocessors, such as dual- or quad-Pentium III systems, in mainstream computing.</p>
<p>Explicitly exploiting parallelism in a program, however, is cumbersome and error prone. It may require the use of inline assembly to generate the appropriate SIMD instructions or the use of a complicated threading library to take advantage of the computing power available on a multiprocessor. Although such explicit techniques can be extremely effective, they also greatly complicate program development and maintenance. An alternative approach is to let a compiler do (at least part of) this exploitation automatically. In this approach, a program written in a sequential programming language, such as Pascal, C, or Fortran, is supplied to a compiler, which analyzes the program for implicit opportunities for parallelism and generates code that takes advantage of this implicit parallelism. An example of such a compiler is the Fortran/C++ compiler developed at Intel's Microcomputer Software Laboratories (where we work). This compiler automatically finds opportunities to generate SIMD instructions (a process called "auto-vectorization") and multithreaded code ("auto-parallelization"). In this article, we will examine some of the techniques used by this compiler.</p>
<DDJADVERTISEMENT INLINE>



<h3>Data Dependencies</h3>


<p>Suppose that you have written <A NAME="re1"></a><A HREF="0107be1.htm">Example 1</A>(a) in a sequential programming language such as C. The sequential semantics of C pose the following execution order on the statements: First execute <i>S1</i> (giving <i>a</i> the value 100) and then execute <i>S2</i> (giving <i>b</i> the value -10 if we assume that the initial value of <i>c</i> is 0). In <A NAME="re1"></a><A HREF="0107be1.htm">Example 1</A>(a), however, executing <i>S2</i> before <i>S1</i> (or even executing <i>S1</i> and <i>S2</i> simultaneously) has no effect on the final values of the variables <i>a </i>or <i>b</i>. In contrast, suppose you have written <A NAME="re1"></a><A HREF="0107be1.htm">Example 1</A>(b). Now, the sequential semantics must be respected or else the final outcome of the code changes. If the initial value of <i>a</i> is 0, for example, then interchanging the execution order of the statements would yield the value -10 for <i>b </i>(rather than 90). This difference with the first example is caused by the fact that there exists a flow (read-after-write) dependence between <i>S3</i> and <i>S4</i>. Other data-dependence relations that prohibit interchanging the execution order of two statements are the anti (write-after-read) dependence and the output (write-after-write) dependence.</p>


<p>Central for any compiler that wants to exploit implicit parallelism in a sequential program is the observation that the execution order of statements can be changed without affecting the meaning of these statements, as long as all data-dependence relations are respected. (Here we assume that I/O statements are internally represented by assignments to variables that represent open files, so that the resulting output dependencies prohibit changing the execution order of these statements.) Mathematically speaking, sequential programming languages pose a total order on the execution of statements, but only the partial order induced by data dependencies is relevant for the final outcome. Finding the data-dependence relations in a program (a process called "data-dependence analysis") is relatively simple for straight-line code (as in the previous examples), but becomes more complex when loops are involved. </p>

<p>In <A NAME="re1"></a><A HREF="0107be1.htm">Example 1</A>(c), for instance, the first complication arises from the fact that the loop causes statement <i>S5</i> to be executed many times, which gives rise to different instances of the same statement (which we will denote by <i>S5(1), S5(2), S5(3)</i>, and so on). The second complication arises from the use of an array rather than a scalar variable. Testing for a flow dependence, for example, consists of determining whether an instance <i>S5(i')</i> that precedes another instance <i>S5(i")</i> (viz. <i>i'&lt;i"</i>) can write to the same element of array <i>a </i>that is read by the latter instance (<i>i'=i"-1</i>). Mathematically, this problem is equivalent to solving the following system for <i>i' </i>and <i>i":i'=i"-1 </i>where<i> 1&lt;=i'&lt;i"&lt;100</i>. </p>

<p>In this case, it is relatively simple to see that there is a flow dependence between every instance <i>S5(i') </i>and the instance <i>S5(i'+1)</i> in the next iteration (the solutions for <i>(i',i")</i> are (1,2), (2,3),...,(98,99)). In general, however, even testing for the existence of data dependencies can be as involved as solving an integer linear programming problem. All compilers, therefore, trade off some accuracy for efficiency during data-dependence analysis. The solvers must be conservative, however. That is, it is safe to report data dependencies when dependence does not really exist, but independence must never be reported incorrectly since that could result in an invalid conclusion on implicit parallelism.</p>



<h3>Auto-Vectorization</h3>


<p>Data-dependence information gives a compiler more insight on where implicit parallelism can be exploited. For example, the simplest examples of code fragments that can be directly implemented with SIMD instructions are formed by loops without any data dependencies. </p>


<p>In the loop in <A NAME="re2"></a><A HREF="0107be2.htm">Example 2</A>(a), data-dependence analysis reveals that no instance of statement <i>S6</i> is data dependent on any other instance of this statement, which implies that the statement instances <i>S6(0)</i>, <i>S6(1)</i>,...,<i>S6(98)</i>, <i>S6(99) </i>may be executed in any order without affecting the final outcome of this code fragment. In particular, this means that the compiler can implement this double-precision floating-point loop with two-way SIMD instructions that simultaneously execute two instances of <i>S6</i> in each iteration. (That is, <i>S6(0) </i>and <i>S6(1)</i> are executed simultaneously in the first iteration of the SIMD loop, followed by <i>S6(2)</i> and <i>S6(3)</i> in the second iteration, and so on.) <A NAME="re2"></a><A HREF="0107be2.htm">Example 2</A>(b) is the assembly-language version of this implementation.</p>

<p>Vectorization is not limited to loops without data-dependence relations only, however. In the loop in <A NAME="re2"></a><A HREF="0107be2.htm">Example 2</A>(c), a flow dependence exists between every instance <i>S7(i')</i> and <i>S7(i'+2)</i>. Although the data dependence implies that the instances of statement <i>S7 </i>cannot be executed in any arbitrary order without affecting the final values that are stored back into the elements of array <i>a</i>, this loop can still be executed two steps at a time, as in <A NAME="rf1"></a><A HREF="0107bf1.htm">Figure 1</A>, where data- dependence relations are depicted as arrows. As a general rule, we can say that <i>N</i>-way SIMD parallelism cannot be exploited in a loop if there is a (transitive) data-dependence relation between two instances of the same statement with a distance less than <i>N</i> in the iteration space of the loop. So, <A NAME="re2"></a><A HREF="0107be2.htm">Example 2</A>(d) cannot be vectorized due to the flow dependencies with distance=1.</p>

<p>However, there are exceptions to this rule. For instance, although <A NAME="re2"></a><A HREF="0107be2.htm">Example 2</A>(e) strongly resembles the first loop, there is a small but subtle difference. In this loop, the elements of array <i>x</i> are read before written, causing an antidependence between every instance pair <i>S9(i')</i> and <i>S9(i'+1)</i>. If the compiler translates this single-precision floating-point loop into the four-way instructions in <A NAME="re3"></a><A HREF="0107be3.htm">Example 3</A>(a), then it is easy to verify that these data dependencies are preserved because the four-way load of elements of array <i>x</i> still occurs before the partially overlapping four-way store back into the array in <A NAME="re3"></a><A HREF="0107be3.htm">Example 3</A>(a).</p>

<p>If there are several statements in a loop, then the compiler may have to reorder statements to make all data dependencies lexically forward; see <A NAME="re3"></a><A HREF="0107be3.htm">Example 3</A>(b). If the double-precision floating-point loop at the left side would be naively translated into two-way SIMD instructions, then the pair-wise load of elements of array <i>b</i> in <i>S10</i> would precede the pair-wise store into <i>b</i> in <i>S11</i>, which violates the flow-dependence relations between instances of <i>S11</i> and <i>S10</i>. After the statements are textually interchanged, however, the store is generated before the partially overlapping load, which respects all data-dependence relations. </p>

<p>Finally, <A NAME="re3"></a><A HREF="0107be3.htm">Example 3</A>(c) is an example of a loop that can be translated into SIMD instructions despite self-flow dependencies with distance=1. Accumulating the product of two short arrays, <i>u</i> and <i>v</i>, into a scalar <i>w</i> causes flow, anti, and output dependencies between all instances of statement <i>S12</i>, which clearly prevents straightforward vectorization of this loop. Such reduction idioms can usually still exploit some SIMD parallelism, however, by computing partial results in parallel. On the Pentium 4, the Intel compiler implements this particular mixed-type reduction as in <A NAME="re3"></a><A HREF="0107be3.htm">Example 3</A>(d). </p>

<p>After this eight-way SIMD loop, the accumulator <i>xmm0</i> contains four partial sums that must be added into the final result <i>w </i>(the exact code sequence has been omitted). To illustrate the performance gains that can result from auto-vectorization, <A NAME="rf2"></a><A HREF="0107bf2.htm">Figure 2</A> shows the performance (in 1 million integer operations per second) of a sequential (SEQ) and vectorized (VEC) implementation of the previous loop for different values of the array length <i>N</i> on a 1.5 GHz Pentium 4 processor (a cleanup loop must be used to handle any remaining iterations in case 8 does not evenly divide the array length). The corresponding speedup is depicted in <A NAME="rf2"></a><A HREF="0107bf2.htm">Figure 2</A> as (<i>S</i>).</p>



<h3>Auto-Parallelization</h3>


<p>Data-dependence information also gives a compiler more insights on where implicit parallelism can be exploited to take advantage of the processing power available in shared-memory multiprocessors like dual- or quad-Pentium III systems. Due to the overhead associated with creating threads, the granularity of parallelism that can be effectively exploited is usually less fine-grained than for auto-vectorization. Therefore, auto-parallelization typically focuses more on outermost loops.</p>


<p><A NAME="re4"></a><A HREF="0107be4.htm">Example 4</A>(a), for instance, computes the product of a matrix and a vector. The double loop gives rise to <i>N</i><sup>2</sup> different instances of the statement <i>S</i>. Data-dependence analysis reveals that there are only data-dependence relations between statement instances that occur in the same iteration of the <i>i</i>-loop, as in <A NAME="rf3"></a><A HREF="0107bf3.htm">Figure 3</A>. Consequently, the compiler can generate multithreaded code for the <i>i</i>-loop without affecting the final outcome of <A NAME="re4"></a><A HREF="0107be4.htm">Example 4</A>(a) (provided, of course, that each thread executes the <i>j</i>-loop sequentially). </p>

<p>First, the original double-nested loop is replaced by <A NAME="re4"></a><A HREF="0107be4.htm">Example 4</A>(b), a single run-time library call that forks a number of threads that each will invoke the function <i>_matvec_par_loop0</i>. This function has the form illustrated in <A NAME="re4"></a><A HREF="0107be4.htm">Example 4</A>(c). When a thread invokes this function, first another run-time library call to <i>_static_init</i> is made that will assign a subset of iterations from the original set <i>[0,n]</i> to the thread based on the thread ID <i>tid</i> using the local variables <i>_lower, _upper, </i>and<i> _stride</i>. (This lets us implement a variety of scheduling policies like block scheduling or cyclic scheduling.) Subsequently, each thread executes this subset of iteration. Finally, the threads join again using the library call _<i>static_fini</i>.</p>

<p><A NAME="rf4"></a><A HREF="0107bf4.htm">Figure 4</A> shows the performance (in MFLOPS) of this code for different values of <i>N</i> on a 550-MHz quad-Pentium III system with auto-vectorization disabled (SEQ) and enabled (PAR). The corresponding speedup (<i>S</i>) goes up to 3.2, which means that about 80 percent of the available computing power is effectively utilized during matrixXvector execution.</p>



<h3>Further Reading</h3>


<p>Banerjee, Utpal. <i>Dependence Analysis</i>. Kluwer Academic Publishers, 1997.</p>


<p>Wolfe, Michael. <i>High-Performance Compilers for Parallel Computing</i>. ISBN 0805327304. Addison-Wesley, 2000.</p>

<p>Information on high-performance compilers for Intel architectures: <a href="http://developer.intel.com/software/products/">http://developer.intel.com/software/products/</a>.</p>



<p><b>DDJ</b></p>
</body>
</html>
