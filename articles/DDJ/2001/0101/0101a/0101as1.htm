<html><head><title>Jan01: Programming & The PC Revolution</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h2>The Most Significant Event in the Last 25 Years of Computing:<br> Comments from <i>DDJ</i> Luminaries</h2><p>The worst idea ever to arise in the history of computing is shared code libraries, which are absurd in a time of cheap 30-GB hard drives. Most of the instability of the Windows platforms is due to DLL conflicts. One app, one block of code. The best idea in computing history is component-based visual programming, as implemented in Visual Basic and (especially) Delphi. I can write in a weekend what would have taken me a month 10 or 15 years ago, because RAD allows me to inherit rather than reinvent my wheels. </p><p align="right">&#151;Jeff Duntemann</p><DDJADVERTISEMENT INLINE><p>Future programmers will look back on the latter part of the 20th century as a kind of dark ages&#151;so many programming resources turned to focus on user interfaces (Mac, Windows, X) that algorithm development suffered. Not to say that there were no important algorithms developed, simply that the majority of programmers were no longer looking at algorithms&#151;or even data structures&#151;but instead became slaves of window handles, menus, and the like. Just as record blocking and I/O management eventually became operating system functions, user interface code is becoming more of a commodity you expect from your operating system and this is bringing a focus back to algorithms.</p><p align="right">&#151;Al Williams</p><p>The world changed in the late 1980s when programmers I knew at several startups became multimillionaires. That was great news for the individuals (they're rich), for the public (motivated builders make great products), and for the field (money attracts good people). The dark side has been an obsession with business plans, focus groups, and stock prices.</p><p align="right">&#151;Jon Bentley</p><p>The most significant event in the last 25 years of computing is not a single event, but the general accelerated progress in micro-miniaturizing of digital and mixed-signal electronic devices. This is driving the economics of computing and communications, making them affordable to an ever expanding, worldwide population of users.</p><p align="right">&#151;James Hendrix</p><p>The IBM PC. More than the Altair 8800 or the Apple II, it truly opened up the market for personal computing.</p><p align="right"> &#151;P.J. Plauger</p> <p> There is no most significant event in computing. What we have represents a confluence of ideas and implementations. Today, the Web is an obvious choice as Most Significant, but the Internet would not be so popular and powerful if there were not millions of personal computers available, which required the invention of the microprocessor and so on. Take-it-for-granted computing would not have taken place if we still demanded that everybody had to write shell scripts and memorize DOS commands.</p><p align="right">&#151;Jef Raskin</p><p>We live in the age of the algorithm, in which enormously sophisticated and flexible artifacts are created at keyboards, yet economies and lives depend on their working. The flexibility is so great that we have come to see algorithms everywhere we look. Biologists speak of analog computer modules within DNA transcription. Physicists perform experiments with their computers first and look at the world second. But analogy can warp the mind. When we see the world and think that controllability is the default and design always possible, then we view surprise as a bug that must be stamped out. But surprise is what makes our life journey so interesting. If there is a deity out there, we still underestimate her.</p><p align="right">&#151;Dennis Shasha</p><p>As a user, I nominate Visicalc, the application that made desktop computers necessary by automating a tedious, error-prone manual task that most business people must perform and, in a tie for most significant event, the Mosaic web browser for spawning the technology that made computers an essential home appliance. As a businessman, I nominate the IBM PC&#151;an open-architecture, clonable, business-oriented platform. Prior to that, businesses viewed small computers as unreliable toys built by hippies in garages with frivolous names such as Apple and Altair. The PC and its IBM logo validated the technology. As a programmer, I nominate the common-user-interface, device-independent GUI operating environment. Many hardware and software technologies converged to make it possible for programmers to target common platforms leaving details of the user interface to the environment. As an author, I nominate as a negative event the "...for dummies" book phenomenon, a publishing model that spawned computer book "lines," sacrificing content for format and emphasizing visual eye candy over meaningful information.</p><p align="right">&#151;Al Stevens</p><p>As a harbinger of a business-unusual peer-to-peer computing revolution, as an example of open-source guerrilla development inside the walls of corporate America, and as one of the early and defining artifacts of a new Internet culture that is neither lawless nor unethical but that is evolving new loyalties and new values, Gnutella gets my nomination.</p><p align="right">&#151;Michael Swaine</p><p>The most significant event in the past 25 years of computing is undoubtedly the rise of the Internet. The ability of programmers and technicians to collaborate worldwide has led to huge advances that might be singled out as hugely significant developments. Among them are the rise of Java and, especially of Linux, which was born as an Internet project. The next development enabled by the Internet will be embedded, dedicated devices that serve specialized purposes from metering power usage to medical monitoring and remote diagnostics of complex equipment. We are actually merely standing on the threshold of what an expanded and high-bandwidth Internet can make possible.</p><p align="right">&#151;Tom Williams</p><p>The Xerox Alto. These folks redefined the way that most of us prefer to interact with our machines, and did so in an a way that transcends the operating-system wars. Whether we run a Microsoft product, Mac, Linux&#151;most of us use the UI pioneered by the Alto.</p><p align="right">&#151;Tom Genereaux</p><p>First was Borland's introduction of Turbo Pascal for (I think) $29.95. Not only did this change the entire pricing model for computer software, but it made a real development tool available to the masses, taking software development out of the hands of large companies. The other milestone is the publication of the Gamma, Helms, Johnson, and Vlissides "Design Patterns" book, which not only changed the nature of the design process at the implementation level, but more importantly, introduced a common vocabulary of design that made efficient communication (and thus synergy) possible between designers.</p><p align="right">&#151;Allen Holub</p><a href="0101a.htm#rs1">Back to Article</a></body></html>