<html><head><title>Jan01: Programming & The PC Revolution</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h1>Programming &amp; The PC Revolution</h1><p><i>Dr. Dobb's Journal</i> January 2001</p><h2>On to the next 25 years...</h2><h3>By Eugene Eric Kim</h3><I>Eugene, formerly a senior technical editor for DDJ, is a freelance programmer and writer. He can be contacted at eekim@ eekim.com.</I><hr>Sidebar: <a name="rs1"><a href="0101as1.htm">The Most Significant Event in the Last 25 Years of Computing: Comments from <i>DDJ</i> Luminaries</a><br>Sidebar: <a name="rs2"><a href="0101as2.htm">A <i>DDJ</i> Timeline</a><hr><p>Twenty-five years ago, computer programming was a mature and well-understood field. By the late 1960s, computer scientists had already invented and exhaustively studied programming's most fundamental data structures and algorithms, from lists and trees to searches and sorts. Programming paradigms, from functional to structured to object-oriented, had all been introduced. The term "software engineering" had been coined, and many researchers had begun to study the discipline of programming. Computer scientists spoke excitedly about the future of programming.</p><p>And then, software development seemed to take a giant step backward.</p><DDJADVERTISEMENT INLINE><p>The culprit was the microcomputer, a $400 jumble of wires and switches and lights that ostensibly did the same thing as the sleek and expensive cabinet-sized machines that <i>real</i> programmers used. Most professionals ignored the microcomputer, and so the machine became the domain of so-called hobbyists.</p><p>Most of these newly minted microcomputer junkies had no prior experience with software or programming, and it showed. To the horror of real programmers, who advocated high-level languages and structured programming, these hackers coded mostly in machine language or in Basic. Programmers in the know had officially declared the GOTO statement harmful. Hobbyists sprinkled GOTOs liberally throughout their Basic programs.</p><p>Twenty-five years ago this month, Bob Albrecht and Dennis Allison started a magazine devoted to these programmers and their octal coding, and unstructured programming ways. They called it <i>Dr. Dobb's Journal of Calisthenics and Orthodontia: Running Light Without Overbyte</i>, a name that confused professional programmers and practicing dentists everywhere. Members of both professions subscribed.</p><p>The only word that could accurately describe this magazine, with its whimsical name and its psychedelic cover, was "unconventional." Not only was it unique in its ragged look and irreverent tone, it was unconventional in its coverage. At the time, computer magazines fell under one of three categories: theory, hardware, or business. <i>Dr. Dobb's Journal</i> was all about software.</p><p><i>DDJ</i> was written by and for practicing programmers, and it published source code with abandon. It didn't matter that many of the early readers and contributors didn't know a parser from a preprocessor. More often than not, articles written by these less experienced hackers would cause so-called real programmers to rear their ugly heads. However, instead of passing judgment on their unseasoned peers, they chose to educate them through letters and follow-up articles.</p><p>What played out on the pages of <i>DDJ</i> in those early days proved a point made by Grace Hopper in the late 1970s. After she had delivered a conference speech on the history of programming languages, Bob Rosin of Bell Laboratories complained about attending "a microcomputer discussion in which the phrases 'hand assembly' and 'octal patch' were prominent," and asked Hopper, "How can we enhance the probability that microcomputers will illuminate truths rather than allow people to ignore the hard-won and hard-learned discipline of programming?"</p><p>Hopper explained that it was "an educational and marketing problem," and remarked, "By and large, the people concerned with the large computers have totally ignored the so-called hobbyist community, though I would point out they are not a hobbyist community: They are small businessmen, doctors, lawyers, small towns, counties! They are a very worthwhile audience." </p><p>This audience proved worthwhile, indeed. Personal computers, n&eacute;e microcomputers, brought computing to the masses, and the masses helped revolutionize computing. Far from sending software development back to the vacuum-tube age, personal computers whipped the software industry into adulthood, and in so doing, changed the way software was built.</p><h3>A Culture of Sharing</h3><p>The Basic language served as a starting point for this revolution in software and software development. In many ways, Basic was the perfect language for hobbyists. Thomas Kurtz and John Kemeny had designed the language in the 1960s for the student body at Dartmouth, only a quarter of whom were majoring in the sciences or engineering. Their driving motivation was simplicity and accessibility over all else. In a presentation on the history of Basic, Kurtz explained, "If ordinary persons are to use a computer, there must be simple computer languages for them. Basic caters to this need by removing unnecessary technical distinctions (such as integer versus real), and by providing defaults (declarations, dimensioning, output formats) where the user probably doesn't care."</p><p>Unfortunately, the capabilities that made Basic so accessible and popular also made it unsuitable for larger, more sophisticated software. Its weak typing was great for beginning programmers, but inappropriate for people needing more robust programs and more complex data structures. Its subroutine capability was primitive at best, making it a poor language for writing modular, structured programs.</p><p>Despite its deficiencies, Basic was vital because it made computers more accessible to a broader audience, which in turn, fostered a curiosity among hobbyists about bigger and better things in the world of software development.</p><p>The articles published in early issues of <i>DDJ</i> were representative of this trend. In the first issue of <i>DDJ</i>, Allison presented Tiny Basic, a compact interpreter that he had built, and challenged others to improve on it. In the next issue, Allison, who taught computer science at Stanford, wrote an article entitled "A Critical Look at Basic," where he lambasted the language, stating, "The language lacks the mechanisms to structure the problem's algorithm and data well. It breeds bad habits, habits which are difficult to unlearn."</p><p>That Allison could both encourage the use of Basic and at the same time criticize the language for its deficiencies was indicative of why computers in the hands of hobbyists did not kill progress in programming, as some had predicted. Unlike the white-hooded programmers of the 1950s, who guarded their knowledge with guild-like secrecy, the personal computer programmers encouraged sharing and community &#151; and by the mid 1970s, there was much knowledge to share.</p><p>First and foremost, there were algorithms and data structures. Fundamental problems cropped up over and over again. How do you shuffle a deck of cards? How do you sort a list of names? How do you calculate the square root of a number? Fortunately for the hobbyists, there was no need to figure out the answers to such questions (although many did, either out of ignorance or for the sheer thrill), because the answers were already known. Additionally, many algorithms gained new relevance due to the nature of these new machines. Compression algorithms, for example, were far more important to programmers dealing with harsh memory constraints than they were to programmers writing software for machines with memory to spare.</p><p>Second, there were languages &#151; and over time, lots of them. Initially, personal computer programmers gravitated to system languages like Forth and introductory languages like Basic. However, as programmers stretched the limitations of these languages, they began to explore the many other languages at their disposal.</p><p>Pascal was one such language. Niklaus Wirth had introduced the language in 1970 to promote good programming practices, but it was a different innovation that led to its broad acceptance among personal computer programmers. Wirth had developed a virtual machine architecture, known as the P-machine, to make compiled Pascal programs portable. The University of California at San Diego's Kenneth Bowles recognized the value of the P-machine architecture, and ported it to a number of personal computers, leading to the development of the UCSD Pascal system. Bowles's implementation not only helped evangelize structured programming to the microcomputing masses, it introduced a sophisticated virtual machine architecture technique and it emphasized the importance of portability.</p><p>Portability was one reason why many programmers of personal computers started expressing interest in another language. Bell Labs's Dennis Ritchie introduced C around the same time as Wirth introduced Pascal. Developed for the new UNIX operating system, Ritchie designed C to be a high-level assembly language for writing portable system software. Unlike UCSD Pascal's interpretive system, compiled C was extremely fast, an attribute that was especially attractive to developers dealing with slow microprocessors.</p><p>The third and vital lesson that hobbyists wanted to and did learn from their more experienced peers concerned software engineering. Knowing the best algorithms and languages for a particular task was not enough information to build great software. Technique was just as important as the tools. While not all of the existing experience necessarily applied to what these PC coders were trying to do, at the very least, these programmers could learn about the importance of coding style, testing methods, and the different programming paradigms.</p><h3>Objects: The Center of Change</h3><p>As this new class of programmers rapidly learned what was already known, programming as a discipline continued to progress. While there were many important innovations in software development overall, perhaps the most significant over the past 25 years was the widespread adoption of object-oriented programming.</p><p>Object-oriented programming has many roots, both in computer science and elsewhere. Object-oriented languages can be traced back to the 1950s with Lisp and the late 1960s with Simula. Alan Kay, who coined the term "object-oriented programming" as a researcher at Xerox PARC, found some inspiration for his Smalltalk language from cell biology and the works of the philosopher/mathematician Leibniz.</p><p>Most importantly, Kay was motivated to create Smalltalk and the principles of object-oriented programming by a larger goal: pervasive personal computing. In 1971, Kay predicted that personal computers would be widespread and networked, and that for this to happen, software had to meet three characteristics &#151; zero replication time and cost, high development time and cost, and low change time and cost. Certainly, all three characteristics were desirable for all software, even in a world without personal computers. However, personal computers made such characteristics urgent necessities.</p><p>Smalltalk (and object-oriented programming in general) was a way to achieve these goals. In theory, programs would be easier to write because they were modeled on things that were easier to understand. Smalltalk, in fact, was designed as a programming language for children. Additionally, programs would be easier to read, and hence, easier to maintain, vastly reducing the largest costs associated with software.</p><p>Personal computers made graphical user interfaces important because they improved the interaction between people and machines. Object-oriented languages like Smalltalk enabled the widespread adoption of these interfaces. How many programmers ever have to code a button or a scrollbar from scratch today? And would graphical user interfaces be as common as they are if more programmers did have to create and integrate these widgets and their behavior from scratch?</p><p>These early graphical widgets eventually led to more general software components, with the hope that fully functional applications could be wired together using prewritten objects and minimal coding. While no one has completely achieved this ambitious goal yet, software today requires far less coding than its complexity would seem to indicate. Object-oriented application frameworks such as the Microsoft Foundation Classes &#151; love them or hate them &#151; are largely responsible.</p><p>Most importantly, object-oriented programming helped lay the foundation for many advances in software engineering, from testing techniques to programming methodologies. Perhaps the most important recent innovation is design patterns, which were defined and documented in the early 1990s by a number of specialists in the field of object-oriented design.</p><p>Their observation was simple: Software design, even with the benefit of objects, is complex, and programmers can learn many things from those who do design well. With this in mind, these researchers began exploring and analyzing well-designed object-oriented software, and cataloging the patterns that they discovered. Many of the most common patterns are recorded in Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides's now classic <i>Design Patterns: Elements of Reusable Object-Oriented Software</i>.</p><h3>Coming Full Circle</h3><p>While object-oriented programming had its origins in personal computers, its influence has spread well beyond the PC. In this sense, over the past quarter of a century, software development has come full circle. Twenty-five years ago, computing old-timers were responsible for most of the innovations in software, as microcomputer enthusiasts rushed to catch up and apply the new knowledge. Today, most of the innovation comes from those working in networked, personal computing environments.</p><p>Computing of all kinds have truly converged into a single discipline, and the events of the last few decades demonstrate this. Fifty years ago, programmers had to deal with the challenges of writing compact programs for slow computers with minimal memory. The resulting knowledge rapidly became moot, as computers got faster and cheaper. Then, 30 years later, personal computers arrived on the scene and suddenly the old lessons became pertinent again.</p><p>Recently, embedded systems have become significant, and once again, the old lessons have become important. Despite the software bloat that is now commonplace on personal computers, there is still a need for software that runs quickly, efficiently, reliably, and predictably on slow processors with limited memory. However, programmers are better prepared this time around. They simply have a far greater understanding of how to build good software.</p><p>Software development has flourished over the past quarter century, and, at 25 years old, <i>Dr. Dobb's Journal</i> continues to flourish as well. Sure, the magazine is glossier, the editors a little grayer, and the tone slightly less irreverent. However, while the topics have changed over the years, <i>DDJ</i> continues to maintain its breadth and depth of coverage. Readers and contributors are still passionate about their craft. And the occasional dentist still calls asking about a subscription.</p><p>Some things never change.</p><p><b>DDJ</b></p></body></html>