<html><head><title>Sep01: Programmer's Toolchest</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h1>Selecting EJB Application Servers</h1><p><i>Dr. Dobb's Journal</i> September 2001</p><h2>Benchmark and test a variety of  EJB servers before making a decision</h2><h3>By Ragae Ghaly, Krishna Kothapalli, and Uma Meyyappan</h3><I>Ragae is a senior architect and Uma a lead consultant at Proxicom. They can be reached at <a href="mailto:rghaly@proxicom.com">rghaly@proxicom.com</a> and <a href="mailto:umeyyappan@proxicom.com">umeyyappan@proxicom.com</a>, respectively. Krishna is senior engineer at FineGround Networks. He can be reached at <a href="mailto:krishna@fineground.com">krishna@fineground.com</a>.</I><hr><p>The Enterprise JavaBeans (EJB) architecture is the standard component architecture for building distributed object-oriented business applications in Java. EJB makes it straightforward to write applications, because you do not have to understand low-level transaction and state management details, multithreading, connection pooling, and other complex low-level APIs. Plus, EJB application development follows Java's write-once-run-anywhere philosophy &#151; once an Enterprise Bean is developed, it can be deployed on multiple platforms without recompilation or source-code modification. </p><p>The EJB architecture addresses the development, deployment, and run-time aspects of an enterprise application's life cycle. EJB technology hides the details of the underlying middleware plumbing, letting you focus on writing pure business logic that does not contain server-specific code. The code can then execute on top of any server implementing the EJB specification. Sun provides a J2EE-compliance (Java 2 Platform, Enterprise Edition) program to guarantee portability across servers. </p><DDJADVERTISEMENT INLINE><h3>Application Server Selection</h3><p>There are a number of available EJB application servers, including:</p><ul>  <li>IBM's WebSphere.  <li>iPlanet's iPlanet Application Server.  <li>BEA's WebLogic.  <li>ATG's Dynamo.  <li>Bluestone's Sapphire.  <li>Oracle's Oracle Application Server.</ul><p>Since each application server has its strengths and weaknesses, it is essential that you select an application server based on the requirements of the project at hand. In this article, we present criteria for evaluating and selecting application servers. For instance, one criterion is J2EE compatibility. Sun gives the J2EE logo/brand to products that pass the J2EE Compatibility Test Suite (J2EE CTS), thereby guaranteeing compatibility, portability, and reliability. Other criteria factors include:</p><ul>  <li>Clustering support, both load balancing and fail over.  <li>Security support, such as integration with LDAP.  <li>Transactions processing support.  <li>Scalability and performance benchmarking.  <li>Integration support with data sources and legacy systems.  <li>Integration with development and configuration management tools.   <li>Integration with operations, deployment, and monitoring tools.</ul><h3>Benchmarking Strategy</h3><p>Benchmarking must be carried out using two main strategies &#151; CPU-bound testing and database-bound testing. </p><p>In the CPU-bound testing model, CPU-intensive EJB components are deployed on each application server, and EJB clients make calls to the server. This tests specific functionality and efficiency of the application server under study. Using CPU-bound testing reveals bottlenecks, internal interactions, and thread management of each application server under study. The main differentiating factors are the JNDI access mechanism, thread management, and connection pooling. </p><p>In the database-bound testing model, database-intensive EJB components are deployed on each application server and EJB clients make calls to the server. The purpose of this model is to accurately simulate the application's production environment.</p><p>These testing models must be conducted in either nonclustering operating mode (in which the application components are tested in a single-server mode), or clustering mode (in which the application components are tested in a cluster of two members of the same application server instances, used on two different machines). </p><h3>Benchmarking Metrics</h3><p>In sample tests, we capture benchmarking metrics on both the client and the server simultaneously. Captured data were synchronized together for each testing case. Before each test began, a snapshot of the process running on the application server machine was captured. The same was done at the end of each test to ensure that no other processes were running in between. </p><p>The metrics on the client were measured by eLoad from RSW (<a href="http://www.empirix.com/">http://www.empirix.com/</a>). The list of metrics, along with the unit of measure, includes:</p><ul>  <li>Average response time (in seconds).  <li>Number of pages or hits/second.  <li>Number of transactions/second.  <li>Number of iterations.  <li>Throughput (KB/second).</ul><p>These metrics are recorded by eLoad, and each value represents the average value of all the virtual users participating in each run. (The Average Response Time for 20 virtual users with different scripts or profiles measures the average response time for the overall number of 20 users.) </p><p>The metrics on the server side were measured using Sun Solaris commands (<i>vmstat, iostat, sar -ug</i>, and the like). Both the <i>perfmeter</i> and <i>proctool</i> commands were used for interactively monitoring all other parameters such as CPU utilization, load, network traffic, memory, and disk utilization. A snapshot of the machine state and all processes running on both the application server and Oracle server were captured using the <i>ps</i> command. The metrics recorded for the benchmarking results include Application Server CPU Utilization (in percentage), and Oracle Server CPU Utilization (in percentage).</p><p>At the start of each run triggered by the eLoad controller machine, a concurrent process was manually initiated to capture the server-side metrics. During each run, all other external parameters were monitored using the <i>perfmeter</i> tool. </p><h3>Planning the Environment</h3><p>Planning the testing environment consists of deciding upon:</p><ul>  <li>The hardware and operating system. Both hardware and operating system must be determined based on the enterprise target production environment.  <li>Application server software (and versions used). A common base line (EJB version compatibility, for instance) needs to be established for comparison.  <li>Testing software used. Testing software is used to measure the client and server metrics. Some examples of stress testing software are eLoad (RSW), Load-Runner (Mercury Interactive), and introspection tools such as TestMyBeans and OptimizeIt. (For more on load testing software, see "Load Testing Web Sites," by Nicholas Baran, <i>DDJ,</i> March 2001.)  <li>Other software used &#151; web server(s), database server(s), directory, and messaging servers and their versions.</ul><p>A sample testing configuration consists of two main environments: the stress testing client and the deployment (simulated production) environment. As <A NAME="rf1"><A HREF="0109jf1.htm">Figure 1</A> illustrates, a typical client testing environment might consist of two NT machines to create the stress test or load &#151; one machine acting as the controller and the other an agent. All test runs are conducted from the agent, keeping the controller free to monitor other metrics and resources. The stress testing tool we used was eLoad in running the test scripts, and eTester to develop these scripts. </p><p>In the deployment environment, using a nonclustering mode, a single EJB application server is used. In clustering mode, two members of the same application server instances are used on two different machines. Only homogeneous hardware is used in the application tier configuration. Both the web and the database tiers stay unchanged in all the test runs conducted for all the application servers under study, and in all modes of operations.</p><h3>Prototype Development</h3><p>One of the first things you do is develop a prototype application to be used as a basis for testing across the application servers. This prototype could be a representative piece of the main application. The prototype consists of the following components: </p><p></p><ul>  <li>Database-intensive EJB components. These components perform the database operations. They could be either session, entity beans, or a mix of both.  <li>CPU-intensive EJB components. These components perform operations that are CPU intensive. They do not access the database directly or indirectly. These are purely session beans.  <li>EJB client components. These components call EJB components. They could be JSPs or servlets accessing the EJB server components. </ul><p>Based on the version of the application server, it may be necessary to port the EJB and client code to different application servers. The changes are mostly in the areas of JNDI lookup, database lookup, and deployment descriptors.</p><p>The prototype should address the user pattern that needs to be tested. The user pattern is the profile of queries or requests from all client machines. It represents both the number of all users requesting information, and how they request this information. Since this is just a benchmarking between two application servers, the user pattern can be created with no time delay between requests. This, of course, has a dramatic increase on the load and can achieve fast results. In this benchmarking effort, the virtual user is the unit of load to be applied with no delay. Each user is represented as a separate thread and remains active for exactly five minutes. All user patterns are identical for all servers in all modes and models. </p><h3>CPU-Bound Testing Results</h3><p>For example, in the CPU-bound application model, a user pattern with a mix of 2 to 10 percent data-bound components and 90 to 98 percent CPU-bound components was used. The test could start with 15 users, with an increase in the load linearly by 15 users to a desired maximum number of users.</p><p>These tests could be repeated for clustered and nonclustered configurations as well. The client and server metrics could be plotted for a clustered configuration, as in the sequence of graphs in <A NAME="rf2"><A HREF="0109jf2.htm">Figure 2</A>.</p><p><A NAME="rf3"><A HREF="0109jf3.htm">Figure 3</A> is a summary of these results in the spider graph. This type of graph provides a snapshot view in comparing all the application servers.</p><h3>Database-Bound Testing Results</h3><p>In the database-bound model, we observed that the database server soon reached a saturated high load with very small impact on the application server. The test could start with one user, with an increase in the load linearly by one user to a desired maximum number of users. These tests could be repeated for clustered and nonclustered configurations as well. </p><p>The client and server metrics could be plotted as in the case of CPU-bound testing. </p><h3>Summarizing the Results</h3><p>The spider graph in <A NAME="rf3"><A HREF="0109jf3.htm">Figure 3</A> summarizes the results from the CPU-Bound/CPU-Bound Cluster, and Data-Bound/Data-Bound Cluster tests. The application server that occupies most of the area in the graph is better than the other application server(s). If it encloses the polygon of another application server(s), it clearly dominates the other(s).</p><h3>Lessons Learned</h3><p>We learned several lessons from the benchmarking efforts: </p><p></p><ul>  <li>Do not always believe what vendors say about their app server. We have found many discrepancies between the documentation and the system studied in the lab.  <li>Certain app servers perform well on certain platforms only. We found that an application server vendor only optimized their server performance on NT rather than other platforms.  <li>Some vendors also optimized their servers to perform well only with certain database drivers.   <li>Certain app servers relay only on an old JDK, which has an adverse impact on the performance with CPU-Bound components.</ul><h3>Conclusion</h3><p>Application servers are at the heart of e-commerce and distributed computing environments. In some cases, performance of a high-volume transactional system is more important than integrating with legacy and middleware components. In this article, we only focused on the strategy and criteria for selecting the appropriate application server. We also described a methodology for performing a benchmarking study with the tools recommended for obtaining effective results. The results shown here are merely for demonstration purposes.</p><p></p><p><b>DDJ</b></p></body></html>