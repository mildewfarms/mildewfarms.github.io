<html>
<head>
<title>Mar01: Value Lattice Static Analysis</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<h1>Value Lattice Static Analysis</h1>
<p><i>Dr. Dobb's Journal</i> March 2001</p>
<h2>A new approach to  static analysis finds errors that slip  through the cracks</h2>

<h3>By William Brew  and Maggie Johnson</h3>

<I>
William is director of R&amp;D at Reasoning and can be reached at bill.brew@reasoning.com. Maggie is a professor of computer science at Stanford University and can be contacted at johnson@cs.stanford.edu.</I>

<hr>

<p>Most software developers consider testing a necessary evil. We don't really want to hear about our mistakes. We don't like going back to fix bugs when we are working on the solution to a new and more interesting problem. But we all know it must be done. The software we create is incredibly complex &#151; more complex than the considerable human intellect can comprehend. Consequently, we rely on software testing techniques to help manage this complexity by exercising the software in ways that we did not envision &#151; and by finding the problems we overlooked in the rush to meet a deadline.</p>
<p>The two primary types of test design &#151; white-box and black-box testing &#151; focus on path execution and functional requirements. Both help to uncover a large number of errors (see, for instance, "White-Box Testing," by Oliver Cole, <i>DDJ</i>, March 2000). But these techniques do not do enough. We still release software with crash-causing bugs to customers. How can this happen if we carefully tested the software?</p>
<DDJADVERTISEMENT INLINE>

<p>Bugs happen because it is not feasible to test all possible paths in a complex piece of software. The number of paths grows exponentially as we add control structures to a program. In testing, we tend to focus on the paths that will most likely execute under normal conditions. We may check some standard and some unusual cases, but you have to draw the line somewhere. The problem with this approach is that many of the most devastating bugs we encounter &#151; null pointer dereferences, memory leaks, out-of-bounds array accesses, uninitialized variables &#151; can occur on infrequently executed paths. These are the bugs that can bring a system down, and the infrequently executed paths are the ones that standard testing often misses.</p>

<p>Fortunately, there are automated tools that can detect some of these errors. A static analyzer parses and analyzes source code to find possible faults and anomalies, or to check that coding standards are being enforced. Lint-like tools are examples of static analyzers. (For more on lint-like tools, see "Examining C++ Program Analyzers," by Scott Meyers and Martin Klaus, <i>DDJ</i>, February 1997.) They do not execute a program &#151; they just analyze the source code using one of several different analysis algorithms.</p>

<p>If you have ever worked with lint-like tools, you know about the high noise level these tools produce. A run on a moderately sized program can generate hundreds of warnings, with only a few representing real defects. This high number of false positives is a serious issue with many static analyzers and a primary reason why programmers tend not to use them.</p>

<p>In this article, we'll describe a new and effective approach to static analysis using a "value lattice." This technique finds the most dangerous defects, those that tend to slip through testing, with an accuracy that is much higher than other static analyzers. Also, many parts of the approach are language independent. Currently, the value lattice (VL) system can analyze C/C++, but versions for other languages are fairly straightforward to implement. The VL system is being implemented in C/C++ inspection technology at Reasoning Inc. (the company for which one of the authors works, http://www.reasoning.com/).</p>



<h3>Static Analysis</h3>


<p>Static analyzers perform many different tasks. Some calculate complexity metrics by path analysis. Others check coding standards by determining the length of functions, depth of nesting, or number of variables in an expression. The static analyzers that find defects use a number of different analysis algorithms ranging from path and value analysis to pattern-matching.</p>


<p>A first step for many static analyzers is the creation of a parse tree and control-flow graph. A parse tree is an efficient representation of the source code derived from an application of grammar rules. A control-flow graph shows the different paths of execution through a program. Both of these abstractions are illustrated in <A NAME="rf1"><A HREF="0103bf1.htm">Figure 1</A>. These abstractions are commonly used by compilers to parse and optimize.</p>

<p>In addition to these abstractions, we need a mechanism for storing what values each variable can have at different points in a program. Consider the code in <A NAME="re1"><A HREF="0103be1.htm">Example 1</A>. You can see that the variable <i>z</i> will be returned uninitialized if <i>x</i> is less than or equal to 2. Most lint-like tools will find this bug. Static analyzers detect this bug by tracking the possible values of the variables, given the paths through the function. As you can imagine, there is a lot of information that must be tracked at each line in the source code. This is another exponential explosion that analysis algorithms must handle. </p>

<p>Now take a look at <A NAME="re2"><A HREF="0103be2.htm">Example 2</A>. At certain points of execution you can see that the variable <i>z</i> in the function bar is not initialized. The value of <i>z</i> is dependent on the values of <i>x</i> and <i>y</i> in the function <i>foo</i>, which come in via formal parameters from main. Most lint-like tools will not find this error because there are limitations in how much analysis they do over function boundaries. The value lattice technique, however, does find this error.</p>



<h3>The Value Lattice Approach</h3>


<p>In the value lattice approach to program analysis and defect detection, the initial steps include building a parse tree and a control-flow graph. In addition, a table that cross-references definitions and declarations and the call graph for the program must be built. Another important step is type modeling. All types in the program are modeled using a language-neutral abstraction. This abstraction is just a tree that captures the structure of the type providing an accurate depiction of memory layout. During this initial phase, each program file is processed independently, making parallel processing possible (see <A NAME="rf2"><A HREF="0103bf2.htm">Figure 2</A>).</p>


<p>The next phase cross-references the abstract types to identify which ones are the same. This is an essential step that supports the cross-function analysis that will occur later in the process. Note that only the parsing and the resulting parse tree are language dependent. The control-flow graph and type abstraction are language independent.</p>

<p>It's important to recognize that defect detection is basically a problem of data-flow. You need to have a clear picture of what values each entity in the program can contain at each point in the program. To create such a picture, you need to do several things:</p>


<ul>
  <li>Understand the dependencies between data elements. For instance, in <A NAME="re2"><A HREF="0103be2.htm">Example 2</A>, the value of the <i>z</i> variable in <i>foo</i> is dependent on the values of <i>x</i> and <i>y</i> in the guard condition of the <i>if</i> statement.
  <li>Track all the values that can flow to a particular data element. In <A NAME="re2"><A HREF="0103be2.htm">Example 2</A>, <i>x</i> and <i>y</i> (in the function <i>foo</i>) can take on a number of different values based on the <i>for</i> loop iteration in main. Models of all these values must be maintained along with specific dependencies.

  <li>Uncertainty about values must be captured. In <A NAME="re2"><A HREF="0103be2.htm">Example 2</A>, the possible values of <i>z</i> in the <i>bar</i> function are unknown until further analysis on the caller of <i>bar</i> is done.

</ul>


<p>All this information must be generated and stored for defect detection. These are difficult tasks to perform within the limits of feasible running time and space conditions.</p>

<p>We have found value lattices, in conjunction with computation analysis graphs (CAGs), to be very efficient in handling these tasks. In a nutshell, a VL is a data structure that represents a partial order. For each type present in the program (<i>int, double,</i> or whatever), a VL is created that contains all the values in the program of that type.  </p>

<p>For each function in the program, a CAG is created. A CAG is a data structure that specifies the flow of values through a computation. This structure is based solely on data dependencies rather than control flow. <A NAME="rf3"><A HREF="0103bf3.htm">Figure 3</A> illustrates a simple CAG for a sequential set of instructions. Nodes contain values and operators. Directed edges indicate data flow, and thus data dependencies. A more interesting example in <A NAME="rf4"><A HREF="0103bf4.htm">Figure 4</A> shows how a control construct can be converted to a data-flow representation. We do this by introducing a new type of node called a "selector." Notice that the selector takes on a value based on the value of the decision variable.</p>

<p>The first step of the analysis phase builds a VL for each type, and a CAG for a routine from the structures already created. The algorithm constructs each VL in such a way that the bottom of the lattice contains data elements whose values are known. Further up in the lattice are elements whose values are less certain. At the top, all you really know is that the element has a value. The knowledge you have about values for elements in the middle may be, for example, that you know it lies in a particular range or is one of a set of values. Perhaps all you know is that the element contains a valid or invalid value.</p>

<p>Once each routine's CAG is constructed and optimized, an abstract interpreter begins its processing. At this point, you know about each of the operations in the routine, and you have some knowledge about the values of the data elements. The interpreter performs a symbolic evaluation where each operation in the program is executed and an abstract value is computed from the VL for each node in the CAG. </p>

<p>For example, suppose you have an addition operation on variables <i>x</i> and <i>y.</i> What you know about <i>x</i> from the VL is that it is a valid integer at this program point. You also know that <i>y</i> equals 42. A valid integer plus 42 will always yield another valid integer. Thus, you can conclude that the operation does not produce a bad value.</p>

<p>When the abstract interpreter completes its evaluation, a set of detection rules are fired to see if any bad values were produced. The current VL implementation  handles the values needed to detect null pointer dereferences, uninitialized variables, memory leaks, and out-of-bounds array accesses. The typical scenario during symbolic evaluation is some number of values are resolved to be valid, some are invalid, and the rest are too uncertain to ascertain. Information about bad values is recorded for later reporting.</p>

<p>To handle the uncertain values, begin by evaluating the program inside-out; see <A NAME="rf5"><A HREF="0103bf5.htm">Figure 5</A>. You begin by building the CAG for function <i>A</i>, then run the abstract interpreter. The uncertain values that remain, a summary of the side effects, and return value are spliced into the CAG for the caller of function <i>A.</i> You then run the abstract interpreter on the caller of <i>A.</i> This splicing continues until you reach the top of the program, which is the main function. At that point, most of the values are resolved and information about any bad values recorded.</p>

<p>The values that might remain unresolved include any external inputs coming into the program. There may be other values that get lost due to approximations built into the algorithm to keep the sizes of the data structures reasonable. These approximations are controlled by user-defined parameters that define certain aspects of the algorithm. In general, the more information spliced in from a called CAG, the more precise the analysis &#151; however, the structure size and running time will increase. The less information spliced in from a called CAG, the more likely some values will remain unresolved when the analysis completes; but the analysis will run more efficiently.</p>

<p>As an example of the entire process, consider the source code in <A NAME="re2"><A HREF="0103be2.htm">Example 2</A>. The algorithm begins by building a CAG for an individual routine, and a VL for each type. Then, the abstract interpreter is applied to the CAG. Notice in the <i>bar</i> function the variable <i>z</i> coming in as a formal parameter. You do not have enough context to resolve it within the CAG for <i>bar</i>. You need to process the calling function. The CAG for <i>bar</i> gets spliced into the CAG for <i>foo,</i> the calling function. Now, you can obtain more specific information about <i>z</i>, but you see that its value is dependent on the formals <i>x</i> and <i>y.</i> So the parts of <i>foo</i>'s CAG pertaining to <i>x, y</i>, and <i>z</i> must get passed up and spliced into main. At this point, you do have enough context to discover that <i>z</i> indeed is uninitialized given certain values of <i>x</i> and <i>y.</i> </p>

<p>The final phase of the process is reporting. A large amount of information about the program is stored during the course of the phase. Over 30 different tables of information in a database are generated that describe all aspects of the program. The most important table, of course, is the one with the defects.</p>



<h3>Conclusion</h3>


<p>The value lattice and computation analysis graph approaches to static analysis have many advantages over other static analysis techniques. They can find dangerous defects across function boundaries. The number of spurious warnings is an order of magnitude lower than in lint-like tools.</p>


<p>Language independence is another important feature of the implementation. Only the parser and the resulting parse tree are language dependent. The rest of the structure, including the type abstraction, the control-flow graph, the VLs, and the CAGs are language independent. This allows for a relatively easy application of the approach to other programming languages. </p>

<p>Finally, the system has been designed throughout to take as much advantage of parallel processing as possible. This is essential in an application that must deal with potential exponential explosion in both structure size and path tracing. </p>



<h3>Suggestions for Further Reading</h3>


<p>Aho, A., R. Sethi, and J. Ullman. <i>Compilers: Principles, Techniques and Tools.</i> Addison-Wesley, 1986.</p>


<p>Muchnick, S. <i>Advanced Compiler Design and Implementation.</i> Morgan Kaufman, 1997.</p>

<p>Weise, D. et al. "Value Dependence Graphs: Representation Without Taxation." http://www.research.microsoft.com/scriptpubs/view.asp?TR_ID=MSR-TR-94-03. Microsoft Research, 1995.</p>




<p><b>DDJ</b></p>
</body>
</html>
