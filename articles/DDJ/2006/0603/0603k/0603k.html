
<html>
<head>
<title>March, 2006: AI:  50 Years Young</title>
</head>

<body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000">
<!--Copyright &#169; Dr. Dobb's Journal-->

<p><i>Dr. Dobb's Journal</i>, March 2006</p>
<h1>AI:  50 Years Young</h1>
<h2></h2>



<I></I>

<hr>





<p>Anniversaries come and go, sometimes with fanfare, often times unnoticed. Take, for instance, "artificial intelligence." It's hard to believe, but this year marks the 50th anniversary of the term&#151;and the discipline&#151;of AI. It was in 1956 that John McCarthy, Marvin Minsky (who along with McCarthy founded MIT's AI lab), IBM's Nathaniel Rochester, and Bell Lab's C.E. Shannon presented "A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence" at the Dartmouth Artificial Intelligence Conference (http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html). The conference and the project, convened with the goal of creating truly intelligent machines, established AI as a unique field of study within computer science.</p>

<p>AI immediately leaped to the forefront in academic, research, and commercial communities. AI-centric programming languages sprang up, with more than 25 commercial implementations of Prolog and nearly 20 implementations of Lisp. Companies such as Texas Instruments, Intel, and Apple waded into the fray with hardware-based AI solutions. All in all, real progress was made in robotics, machine vision, mechanical manipulation, advanced automation, and computational geometry, at least according to Minsky and Seymour Papert in their "Progress Report on Artificial Intelligence" (http://web.media.mit.edu/~minsky/papers/PR1971.html). Yes indeed. AI's future looked rosy&#151;then it all seemed to fall apart. </p>

<p>For reasons ranging from unfulfilled promises to unreturned investments, AI became a four-letter word in the late 1980s. Overnight, it seemed, the sure way to kill a product or bankrupt a company was to tag them as "artificial intelligence," and AI bashing became the sport du jour in the <i>Wall Street Journal </i>and <i>Forbes</i>. Even Minsky jumped on the AI-bashing bandwagon, bluntly stating in a 2003 Boston University speech that "AI has been brain-dead since the 1970s." He went on to accuse researchers of throwing in the towel when it comes to building fully autonomous, intelligent machines. In particular, Minsky singled out the expert system community, which, he said, was reinventing the wheel every time a new problem was encountered. The problem, said Minsky, was that expert systems (and the experts behind them) were neglecting "common-sense reasoning," in which computers can grasp the everyday assumptions that we take for granted.</p>

<p>Well, most researchers anyway. Minsky did point to one bright spot&#151;Cycorp (https://www.cyc.com/). The Cycorp Cyc Knowledge Server is a large, multicontextual knowledge base and inference engine with more than one million assertions (rules) in a common-sense knowledge base. Because it is based on common sense, Cyc knows that "trees are usually outdoors, that once people die they stop buying things, and that glasses of liquid should be carried right-side up." Cyc can also handle natural-language queries so that users can ask for "photos of strong and daring men" and a photo of a man climbing a cliff pops up.</p>

<p>Today, Cycorp's goal is to break the "software brittleness bottleneck" by building a foundation of basic common-sense knowledge in applications ranging from e-commerce and machine translation, to speech recognition and games. To enable this, Cycorp built a knowledge base, inference engine, the CycL programming language, natural-language-processing subsystem, the Cyc semantic integration bus, and various Cyc developer toolsets. Additionally, an open-source version of Cyc is freely available (http://sourceforge.net/projects/opencyc/).</p>

<p>Still, Cycorp isn't the only intelligent systems game in town. Franz's Allegro Common Lisp is now in Release 8, and is routinely used in video games, data mining, bioinformatics, modeling and simulation, and the like. Then there's the neural-net application written by Oklahoma State University's Ramesh Sharda, which predicts a movie's success or failure in the marketplace based on rules weighing a movie's cast, genre, competition, special effects, and the like. Other mission-critical applications include robotic jockeys competing in camel races across the Sahara (http://www.uaeinteract.com/news/), the Intellibuddy chatbot (http://www.intellibuddy.com/), and Marion "Tank" LeFleur, the roboreceptionist with an attitude in the lobby at Carnegie Mellon University (http://roboceptionist.com/).</p>

<p>Kidding aside, the successful completion of Stanford University's robotic vehicle "Stanley" (a 2004 Volkswagen Touareg) in DARPA's Grand Challenge&#151;a 128-mile race across the Mojave Desert&#151;was a major step forward that involved significant progress across a wide range of AI technologies (http://devnet.developerpipeline.com/documents/s=9903/ddj051010pc/). Moreover, as David Israel, Program Director at SRI's Artificial Intelligence Center (http://www.ai.sri.com/) points out, DARPA continues strong funding of AI-related efforts through its Information Processing Technologies Office, while in the private sector, companies such as Google, Microsoft, and Yahoo are deeply involved with serious AI research.</p>

<p>So from all appearances, AI&#151;cognitive computing, machine learning, decision support or whatever name it goes by&#151;is alive and kicking. From that perspective, AI sounds like it's 50 years young, instead of 50 years old.</p>




</body>
</html>