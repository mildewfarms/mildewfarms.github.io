<html><head><title>May04: Optimization Techniques</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; Dr. Dobb's Journal--><h1>Optimization Techniques</h1><p><i>Dr. Dobb's Journal</i> May 2004</p><h2>Sharing  hard-won insights</h2><h3>By Tim Kientzle</h3><I>Tim is a DDJ contributing editor and consultant. He can be contacted at kientzle@acm.org.</I><hr><p>Some years ago, a client asked me to write a simple decompressor for extracting still images from DV camcorder data. As requirements changed, I was repeatedly called in to add new features and improve performance. Today, my decoder is one of the fastest available for that format.</p><p>With requirements ratcheting steadily higher for each iteration, I was forced to learn a lot about optimizing code for modern processors. In this article, I share some of those lessons.</p><p>The DV video format is a fairly standard DCT-based image compression system, similar in concept to motion JPEG. There are three major steps to the decoder: variable-length decoding, Inverse DCT, and color-space conversion.</p><p>The DV format stores data using a varying number of bits for each piece of data, similar to Huffman compression. Decompressing requires first splitting the data stream into varying-length pieces at appropriate bit boundaries. The resulting data goes through an Inverse DCT operation, which transforms the resulting 8&times;8 block of frequency information into actual pixel values. This is a standard and well-studied transformation used by many audio and video compression systems. The final pixel values specify YCbCr color information, which must be converted into a color format suitable for display, usually YUV or RGB.</p><h3>Test, Test, Test</h3><p>For the most part, cutting-edge optimization is just a lot of hard work. You have to constantly experiment with different approaches to find the ones that work best for a particular problem. To know which approaches are best, you must constantly measure the real performance.</p><p>Profiling tools such as GNU gprof (http://www.gnu.org/software/binutils/) or Intel's VTune (http://www.intel.com/software/products/vtune/) are useful for determining which parts of your code are consuming the most time. Be prepared to spend a lot of time examining profiling data. Look at the amount of time being spent in each function, and carefully consider whether that matches your expectations.</p><p>In my video decoder, for example, I was not surprised to find that the Inverse DCT calculation required nearly half of the processor time. I was surprised, however, that simply clearing my temporary work variables in preparation for each new block of data required nearly 15 percent of the processor time. A small piece of hand-tuned assembly made a big difference there and resulted in an easy 10 percent speedup of the entire decoder.</p><h3>Building-in Test Code</h3><p>You can use the Time Stamp Counter (TSC) on modern processors to build accurate self-measurement into your code. By checking the TSC before and after critical code sections, you can know exactly how many clock cycles are being used by your code. <A NAME="re1"><A HREF="0405be1.htm">Example 1</A> shows how to read the TSC using two popular compilers.</p><p>For my application, I developed a test harness to read several hundred test frames from a file into memory, then iteratively decompress each one. My decoder reads the TSC at key points and stores the values into a public structure so that timing information can be accumulated by the driver program and reported. In this way, I was able to quickly assess the impact of each change. If a code change resulted in a slowdown, I could immediately back it out and try something else.</p><p>In some cases, you need to remove the timing and test code for production release because the timing and test code itself requires time. In my case, this was not required. The full decoder required millions of cycles per frame; the timing calculations added only a few hundred. This let me simply leave the timing code in as a permanent feature.</p><h3>Assembly Won't Save You</h3><p>Video decoding involves doing many repeated calculations on blocks of data. This is a perfect fit for parallel instructions such as those provided by the MMX, SSE, or 3DNow extensions. Many key algorithms were made significantly faster by careful translation into assembly.</p><p>In particular, I was able to find some exceptionally good implementations of the Inverse DCT calculation that were carefully tuned for parallel operation using MMX and SSE2 instruction sets. A little time spent searching the Internet or asking around can often save you a lot of time.</p><p>However, assembly is no silver bullet. Remember that assembly code is generally more difficult to modify than higher level code. As a result, it takes much longer to fix bugs or implement algorithmic improvements to assembly code. Stick with C where you can and put off assembly until late in the development cycle. This gives you more time to focus on architectural and algorithmic improvements, which often provide much bigger speedups.</p><p>In my case, the initial variable-length decoding was particularly frustrating. I tried several times to use assembly to optimize that code. Each such attempt resulted in modest improvements, but those improvements turned out to be algorithmic; in each case, fitting those algorithmic improvements back into the C version resulted in the C version being faster than the assembly.</p><h3>Use Vector Tables</h3><p>For top performance, you need processor-specific versions of key routines. For example, my video decoder contains C, MMX, and SSE2 versions of the critical Inverse DCT calculation. Managing processor-specific code is not easy. You must determine the processor when you start, then install the correct versions of critical functions. The only effective way to manage this is to define a function pointer for each major routine. <A NAME="re2"><A HREF="0405be2.htm">Example 2</A> shows typical code for supporting this. In this case, you would never invoke <i>Foo(x)</i> directly, you would instead invoke it via the function pointer as <i>(</i>*<i>Foo_Ptr)(x)</i>.</p><p>Determining the processor is easier today than it used to be, thanks to the CPUID instruction on x86 processors and similar capabilities on other processor families. <A NAME="rl1"><A HREF="#l1">Listing One</A> provides basic processor detection for current x86 processors. Unlike many published processor- detection routines, this one is almost all in high-level C, which makes it easy to add checks for new features.</p><p>In practice, of course, you won't need a separate version of every function for every processor. I manage this in my work by keeping each major routine in a separate source file, and pairing each source file with a header file. <A NAME="re3"><A HREF="0405be3.htm">Example 3</A> shows a typical header that defines four separate names with only two underlying implementations. This prevents you from needing to update the code in <A NAME="re2"><A HREF="0405be2.htm">Example 2</A> whenever you implement a new processor-specific function.</p><h3>Factor Carefully</h3><p>Function calls take time, especially if you have to go through a vector table. It is imperative that you not factor your code too finely. For example, one decoder I studied called a function to read each binary bit of input. This obviously swamped performance. On the other hand, you do need to identify pieces of code that can benefit from processor-specific optimizations and factor them out. I found early on that simply clearing my work variables before processing each block of data was a time-consuming task; defining separate functions to exploit MMX or SSE registers when they were available helped significantly.</p><h3>Avoid Branches, Unroll Loops</h3><p>Branches can be very slow on modern processors and should be avoided for maximal performance. <A NAME="re4"><A HREF="0405be4.htm">Example 4</A> shows four different ways to compute the same thing; the first two require a branch, the second two do not. In many cases, the latter versions will be significantly faster.</p><p>Branches commonly appear in loops. Consider the routine in <A NAME="rl2"><A HREF="#l2">Listing Two</A> for clearing a 4K block of memory. I've unrolled the loop significantly to reduce the number of branches required. Of course, extreme loop unrolling also increases the size of the code and hence increases the amount of time required to load instructions from memory. Experiment carefully to find the right balance point. Also note the technique I've used here of carefully separating data changes (the two <i>add</i> instructions) from uses of that data. Modern processors can execute multiple instructions at a time only if there are no dependencies. Usually, of course, it's not this simple to separate the data dependencies, which is one reason that the code from a good optimizing compiler often outperforms hand-generated assembly.</p><h3>Right-Size Your Data</h3><p>Think carefully about data storage. Most processors are more efficient when all of your data is the same width; converting between 16-bit and 32-bit values can be expensive. As a result, you'll generally want to stick with 32-bit values on 32-bit processors. On the other hand, memory bandwidth is a real constraint. Storing large blocks of data as 16-bit values rather than 32-bit values can double performance by halving the amount of memory traffic. In particular, remember that single-precision floating-point values are 32-bit values. Integer algorithms using 16-bit values will almost always be much faster.</p><h3>Be Aware of the Processor Cache</h3><p>Every time you have to go to the L2 cache or main memory, you pay a penalty. As a result, a well-optimized program will organize its work to fit into the L1 cache where possible. My decoder, for example, takes one small block of data and processes it completely before going to the next block. One competing decoder I had a chance to study instead performed complete passes over the entire image. Because each pass processed more data than would fit in the cache, the intermediate data had to be written all the way to main memory and then read back from main memory for the next step. My decoder is significantly faster.</p><h3>Conclusion</h3><p>As processor performance improves and compilers become smarter, assembly language is becoming less of a cure-all for performance problems. However, a good understanding of processor architecture still pays big dividends. Even more important is taking the time to set up a good testing system&#151;one that is convenient for you to use constantly as you test and refine your code.</p><p></p><p><b>DDJ</b></p><H4><A NAME="l1">Listing One</H4><pre>typedef struct {    /* Registers */    int eax;    int ebx;    int ecx;    int edx;    /* Basic CPUID data */    int maxCPUID;    int maxExtendedCPUID;    int features;    int extendedFeaturesAMD;    int family;    int model;    int steppingId;    /* Identification Strings */    char manufacturer[16];    char brand[50];    /* Specific features */    char hasCPUID;    char hasTSC;    char hasCMOV;    char hasMMX;    char hasSSE;    char hasSSE2;} PROCESSOR;voidProcessorCPUID(int arg, PROCESSOR *p) {  if(!p-&gt;hasCPUID) return;  p-&gt;eax = p-&gt;ebx = p-&gt;ecx = p-&gt;edx = 0;#if defined(_M_IX86) &amp;&amp; defined(_MSC_VER)  /* Visual C++ syntax */  __asm {    mov edi,p    mov eax,arg    cpuid    mov [edi]PROCESSOR.eax,eax    mov [edi]PROCESSOR.ebx,ebx    mov [edi]PROCESSOR.ecx,ecx    mov [edi]PROCESSOR.edx,edx  }#elif defined(__GNUC__) &amp;&amp; defined(i386)  /* GNU C/C++ Syntax */  __asm__("mov %4,%%eax;cpuid;mov %%eax,%0;mov %%ebx,%1;"          "mov %%ecx,%2;mov %%edx,%3"          : "=mr" (p-&gt;eax),"=mr"(p-&gt;ebx),              "=mr"(p-&gt;ecx),"=mr"(p-&gt;edx) /* outputs */          : "mr" (arg) /* inputs */          : "eax","ebx","ecx","edx" /* Registers clobbered */);#endif}voidProcessorInfo(PROCESSOR *p) {  int t=0; /* Scratch C register */  memset(p,0,sizeof(*p));  /* Step 1: Determine if this processor supports CPUID. */#if defined(_M_IX86) &amp;&amp; defined(_MSC_VER)  /* Processor supports CPUID if you can set and clear bit 21 of EFLAGS */  /* This sequence sets bit 21 of t if bit 21 can be both set and cleared */  __asm {    pushfd        /* Start with current EFLAGS value */    pop eax    or eax,0x00200000  /* Set bit 21 */    push eax    popfd         /* Store into EFLAGS */    pushfd        /* Read back from EFLAGS */    pop ebx       /* Result into EBX */    xor eax,0x00200000 /* Clear bit 21 */    push eax    popfd         /* Store into EFLAGS */    pushfd        /* Read back from EFLAGS */    pop ecx       /* Result into ECX */    xor ebx,ecx   /* See if bit 21 is different in EBX and ECX */    mov t,ebx     /* Store result into 't' */  }#elif defined(__GNUC__) &amp;&amp; defined(i386)  /* Processor supports CPUID if you can set and clear bit 21 of EFLAGS */  /* This sequence sets bit 21 of t if bit 21 can be both set and cleared */  __asm__("pushf;pop %%eax;"          "or $0x00200000,%%eax;push %%eax;popf;pushf;pop %%ebx;"          "xor $0x00200000,%%eax;push %%eax;popf;pushf;pop %%ecx;"          "xor %%ecx,%%ebx;mov %%ebx,%0"          : "=mr" (t) /* Output 't' can be a memory argument or register */          : /* No inputs */          : "eax","ebx","ecx" /* Registers clobbered */);#endif  if(t &amp; 0x00200000) p-&gt;hasCPUID=1;  /* No CPUID? Then there's not a lot we can determine. */  if(!p-&gt;hasCPUID) return;  /* Use CPUID(0) to determine manufacturer and extent of CPUID support */  ProcessorCPUID(0,p);  p-&gt;maxCPUID = p-&gt;eax; /* Maximum CPUID argument */  {    int *s = (int *)(p-&gt;manufacturer);    s[0] = p-&gt;ebx;    s[1] = p-&gt;edx;    s[2] = p-&gt;ecx;    p-&gt;manufacturer[12] = 0;  }  if(p-&gt;maxCPUID &lt; 1) return;  /* Identify standard features */  ProcessorCPUID(1,p);  p-&gt;features = p-&gt;edx;  if(p-&gt;features &amp; (1&lt;&lt;4)) p-&gt;hasTSC = 1;  if(p-&gt;features &amp; (1&lt;&lt;15)) p-&gt;hasCMOV = 1;  if(p-&gt;features &amp; (1&lt;&lt;23)) p-&gt;hasMMX = 1;  if(p-&gt;features &amp; (1&lt;&lt;25)) p-&gt;hasSSE = 1;  if(p-&gt;features &amp; (1&lt;&lt;26)) p-&gt;hasSSE2 = 1;  p-&gt;family = (p-&gt;eax &gt;&gt; 8) &amp; 15;  if(p-&gt;family == 0x0F) p-&gt;family |= (p-&gt;eax &gt;&gt; 16) &amp; 0xFF0;  p-&gt;model = (p-&gt;eax &gt;&gt; 4) &amp; 0x0F;  if(p-&gt;model == 0x0F) p-&gt;model |= (p-&gt;eax &gt;&gt; 8) &amp; 0xF0;  p-&gt;steppingId = p-&gt;eax &amp; 0x0F;  /* De facto standard: AMD 3DNow */  ProcessorCPUID(0x80000000U,p);  p-&gt;maxExtendedCPUID = p-&gt;eax;  if(p-&gt;maxExtendedCPUID &gt;= 0x80000001U) {    ProcessorCPUID(0x80000001U,p);    p-&gt;extendedFeaturesAMD = p-&gt;edx;    if(p-&gt;extendedFeaturesAMD &amp; (1&lt;&lt;31)) p-&gt;has3DNow = 1;  }  if(p-&gt;maxExtendedCPUID &gt;= 0x80000004U) {    int *s = (int *)(p-&gt;brand);    ProcessorCPUID(0x80000002U,p);    s[0] = p-&gt;eax; s[1] = p-&gt;ebx; s[2] = p-&gt;ecx; s[3] = p-&gt;edx;    ProcessorCPUID(0x80000003U,p);    s[4] = p-&gt;eax; s[5] = p-&gt;ebx; s[6] = p-&gt;ecx; s[7] = p-&gt;edx;    ProcessorCPUID(0x80000004U,p);    s[8] = p-&gt;eax; s[9] = p-&gt;ebx; s[10] = p-&gt;ecx; s[11] = p-&gt;edx;  }  if(strcmp(p-&gt;manufacturer,"GenuineIntel")==0) {    /* Intel-specific feature recognition */  }  if(strcmp(p-&gt;manufacturer,"AuthenticAMD")==0) {    /* Check extended features for AMD-specific extensions */    /* if(p-&gt;extendedFeaturesAMD &amp; (1&lt;&lt;22)) p-&gt;hasAMDMMX = 1;*/    /* if(p-&gt;extendedFeaturesAMD &amp; (1&lt;&lt;31)) p-&gt;has3DNow = 1; */    /* if(p-&gt;extendedFeaturesAMD &amp; (1&lt;&lt;30)) p-&gt;has3DNowExtensions = 1; */  }  /* Clear out register fields before returning */  p-&gt;eax = p-&gt;ebx = p-&gt;ecx = p-&gt;edx = 0;}</pre><P><A HREF="#rl1">Back to Article</A></P><H4><A NAME="l2">Listing Two</H4><pre>void ClearWorkArea_MMX(char *work) {  _asm {    mov   esi,work    mov   edi,esi    sub   edi,128    pxor  mm0,mm0    mov   cl,15loopTop:    movq  qword ptr [esi],mm0    movq  qword ptr [esi+8],mm0    movq  qword ptr [esi+16],mm0    movq  qword ptr [esi+24],mm0    movq  qword ptr [esi+32],mm0    movq  qword ptr [esi+40],mm0    movq  qword ptr [esi+48],mm0    movq  qword ptr [esi+56],mm0    movq  qword ptr [esi+64],mm0    movq  qword ptr [esi+72],mm0    movq  qword ptr [esi+80],mm0    movq  qword ptr [esi+88],mm0    add   edi,256    movq  qword ptr [esi+96],mm0    movq  qword ptr [esi+104],mm0    movq  qword ptr [esi+112],mm0    movq  qword ptr [esi+120],mm0    movq  qword ptr [edi],mm0    movq  qword ptr [edi+8],mm0    movq  qword ptr [edi+16],mm0    movq  qword ptr [edi+24],mm0    movq  qword ptr [edi+32],mm0    movq  qword ptr [edi+40],mm0    movq  qword ptr [edi+48],mm0    movq  qword ptr [edi+56],mm0    movq  qword ptr [edi+64],mm0    movq  qword ptr [edi+72],mm0    add   esi,256    movq  qword ptr [edi+80],mm0    movq  qword ptr [edi+88],mm0    movq  qword ptr [edi+96],mm0    dec   cl    movq  qword ptr [edi+104],mm0    movq  qword ptr [edi+112],mm0    movq  qword ptr [edi+120],mm0    jne   loopTop    emms  }}</pre><P><A HREF="#rl2">Back to Article</A></P></body></html>