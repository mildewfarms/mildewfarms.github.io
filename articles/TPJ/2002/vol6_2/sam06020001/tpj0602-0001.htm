<html><head><title>Resource Locking Over Networks</title></head><body bgcolor="ffffff"><h1>Resource Locking Over Networks</h1><p>Sean M. Burke<br>  <span class="date">Summer, 2002</span></p><p>"Find a safe part and use it as an anchor"<br>  -- Brian Eno and Peter Schmidt, <i>Oblique Strategies</i></p><p> In my previous <i>The Perl Journal</i> article, "Resource Locking with   Semaphore Files", I showed some of the problems that can appear when simultaneously   running processes try accessing the same file (or other scarce resource) at   the same time; and for a solution, I proposed semaphore files based on Unix   <b>flock</b>. However, that depended on all the tasks in question being on the   same machine, and that machine basically supporting a Unix file-locking model.   In this article, I'll deal with how to do resource locking when we can't   assume any of these things, and where the processes in question might be on   different machines with no common filesystem, and where possibly none of them   are Unixes.<p> <b>Many Ways To Do It</b><p> The thing that makes file locking work correctly is that no two processes   do it at the same time -- if several try, then only one is allowed to succeed.   That's not a characteristic of just file locking, though:<p> <p> <ul>  <li> In many database systems, if several simultaneously connected processes     try to create a record with the same unique identifier, then only one is allowed     to do so, and the rest are told that it wasn't possible.   <li> In most filesystems, the same is true of deleting a file. So if several     processes try deleting the same file at the same time, only one actually gets     told that it succeeded in being the deleter.   <li> Similarly, in any even reasonably sane filesystem, if several processes     try to make a directory "foo" (which doesn't already exist)     at the same time, then only one of them actually succeeds, and the others     are told that it can't <b>mkdir</b> "foo", as "foo"     already exists.   <li> In many filesystem models, you can differentiate opening an existing file     from creating one that wasn't there. So if several processes try, even     at the very same moment, to create a new file named "foo.txt", only     the first one actually succeeds, and the others are told that it can't     be done, because such a file already exists.</ul><p> When doing resource locking over a network, many people use some variation   of the first solution, involving databases. This approach is inconvenient unless   you have a network-accessible database system already set up; and it's   not possible unless you know what operations the database system will treat   like the above operations. Instead, the third way is the one I prefer, and it   works like this:<p> There could be several concurrent processes that want to manipulate some resource   (like reading in a file called "/stuff/big_config.txt", making some   changes, and then re-writing it). And if it would be bad for more than one of   them to be manipulating that resource at the same time, then before any process   deals with that resource, it must try to <b>mkdir</b> "/stuff/big_config_lock".   If that <b>mkdir</b> operation succeeds, then that process has permission to   manipulate that other resource, the "/stuff/big_config.txt" file.   But if a process calls <b>mkdir</b> "/stuff/big_config_lock", and   it doesn't succeed, then we take this to mean that it already exists, and   some other process must be in the middle of accessing the "/stuff/big_config.txt"   resource. Or in Perl terms:<pre>  sub wait_until_lock_dir { my $dir = $_[0]; until( mkdir $dir ) { sleep 2 } return; }  sub unlock_dir { my $dir = $_[0]; warn "Couldn't rmdir $dir: $!" unless rmdir $dir; return; } wait_until_lock_dir("/stuff/big_conf_lock"); print "OK, I've got the lock!\n!'; ... do whatever with /stuff/big_conf.txt ... unlock_dir( "/stuff/big_conf_lock");</pre> <p> Compared to the Unix file-locking model, this model has the following disadvantage.   Suppose, under the Unix file-locking model, that a process opens and locks a   semaphore file, and then dies while manipulating the process. The operating   system will close the file, and thereby end the lock; so under that model, it's   impossible for a file to be locked, unless there is a process that is running   and has it locked. But suppose that instead of a semaphore file, we're   using the existence of a directory to mean "that resource is in use".   If a process starts up, successfully makes the directory, and then dies while   manipulating the process, then there's nothing to automatically make that   directory go away. Unless something comes along and deletes it, every process   that tries to deal with that resource will end up waiting forever for it to   be available. This problem can be coped with in various ways, as I'll describe   later in this article.<p> This use of <b>mkdir</b> as a locking mechanism has two advantages over normal   Unix-ish <b>flock</b>. First off, it works under OSs other than Unix! Second   off, it works across ftp! That is, instead of making the directory on the local   filesystem with <b>mkdir($dirname</b>), you'd log into an ftp server and   <b>$ftp-&gt;mkdir($dirname</b>). The point of doing this is so that ftp server   can essentially serve as a "lock server" for any processes that can   access it.<p> <b>Using ftp mkdir for Resource Locking</b><p> So let's use a concrete example here, such that should be familiar to   all our readers. You're the Chief Technical Officer for KAOS, the worldwide   consortium of supervillians. Among your many evil duties is coordinating the   exchange of the staff phone number directory, <i>phone.tsv</i>. In an ideally   evil world, you would have all the evil resources needed to build a solid network.   But for years now, all available funds have been diverted to the construction   of the giant orbiting death-ray, leaving you with only a ramshackle mess of   legacy systems.<p> Notably, the chief of Henchman Resources, Generalissima Mok, personally updates   the phone directory on a daily basis, as old staff members are killed for their   incompetence and others are promoted. She has learned to do this using FileMaker   on an old Apple laptop running MacOS 7. You once meekly suggested that she use   a Web interface for maintaining the phone database, and the Generalissima reacted   by torturing you with electric shocks until you promised to forget the whole   idea.<p> She did, however, let you write and install a MacPerl program on her laptop   to ftp the <i>phone.tsv</i> file from her Mac to the main server, and she dutifully   runs the program after every evil update to the database. The meat of that program   looks like this:<pre>  use Net::FTP; # The class for FTP connections my($host, $user, $pass, $file) = ( 'ftp.kaos.int', 'phoneacc', 'dmb4ever', 'phone.tsv' ); my $conn = Net::FTP-&gt;new($host) || die "Can't ftp-connect to $host: $@"; $conn-&gt;login($user, $pass) || die "Can't log in as $user\@$host";  $conn-&gt;put($file) || die "Can't ftp-put $file";  $conn-&gt;quit(); undef $conn;</pre> <p> And that program does a fine job of ftp'ing up the file when run. The   problem you quickly run into is that others might be downloading <i>phone.tsv</i>   from the server just as she's uploading it, resulting in their getting   a file that's truncated or garbled. This upsets people, because if they   fail to find some friend's name in the phone directory, they assume he   has been killed for incompetence. This leads to a tense work environment. You   solve the problem of unnerving partial transfers by using a lock directory,   like so:<pre>  ... $conn-&gt;login($user, $pass) || die "Can't log in as $user\@$host";  my $lock_dir = 'phone_lock'; until( $conn-&gt;mkdir($lock_dir) ) { sleep 3 }  # Send it to the remote server $conn-&gt;put($file) || die "Can't put ftp file $file";  $conn-&gt;rmdir($lock_dir);  $conn-&gt;quit(); undef $conn;</pre> <p> The <b>until</b> line waits until it's the one and only process that   succeeds in making the directory <i>phone_lock</i>, which then signals that   it is the only one writing to it. The only other new command that we've   added here is the <b>rmdir</b>, which deletes <i>phone_lock</i>, signaling that   we're no longer staking a claim on this resource, now that we're done   with it.<p> On various hosts that need their own copy of <i>phone.dat</i>, you write Perl   programs to do the downloading. They look exactly like the above program, except   that the <b>put</b> line is replaced with this:<pre>  # Get it from the remote server $conn-&gt;get($file) || die "Can't get ftp file $file";</pre> <p> So if one of those processes logs in when Generalissima Mok is uploading her   <i>phone.tsv</i>, they'll get as far as the <b>until</b> line that tries   to create <i>phone_lock</i>. But that <b>mkdir</b> command will fail, causing   the loop body of <b>sleep 3</b> to be executed, and then it'll try again,   and keep trying until in the meantime the Generalissima will have finished the   upload and <b>rmdir</b>'d the <i>phone_lock</i> directory. Then when this   downloader process tries to call <b>mkdir</b> again, it will succeed, and the   download can continue.<p> Or, going the other way, if the Generalissima's uploader process tries   to log in while some other process is downloading <i>phone.tsv</i>, it will   go into a sleep loop until that process in finished and <b>rmdir</b>'s   the directory.<p> There are three problems with this system:<p> First, the system dutifully stops Generalissima Mok from uploading a <i>phone.tsv</i>   at the same moment that anyone is downloading <i>phone.tsv</i>. But it does   this by stopping any two processes, of any kind, from accessing the file at   the same time. This is a bit broad, because it stops two download processes   from downloading the file at the same time; but two processes downloading the   same file at the same time is harmless. The fact that the system stops this   from happening is a problem only in that it slows things down somewhat; but   I see no way around it.<p> Second, what if a process can't <b>rmdir</b> the lock directory? I've   seen this happen only in a particular case, where a server would let a user   <b>mkdir</b> a directory under an <b>a+rwx</b> directory that was owned by someone   else, but wouldn't let the same user <b>rmdir</b> the directory that he'd   just created! The solution is to run an interactive ftp session before you set   up your lockdir system, to make sure that your ftp host doesn't behave   that way. If it does, then put the lock directory in a directory the account   can access, so that account can <b>rmdir</b> in as needed. This should just   be a simple matter of setting <b>$lock_dir</b> to something different, like   this:<pre>  my $lock_dir = '/evilusers/phoneacc/phone_lock';</pre> <p> And third, what if a process successfully makes the directory <i>phone_lock</i>,   then starts downloading or uploading <i>phone.tsv</i>, but then the ftp connection   dies because of a network outage? In that case, <i>phone_lock</i> would stick   around forever. As I pointed out, this isn't a problem with Unix <b>flock</b>,   but it's sure a problem here. More and more processes would accumulate,   one by one connecting to the remote server and waiting literally forever for   <i>phone_lock</i> to be removed. I see no general solution to this problem;   but in these circumstances, I have used two makeshift solutions. First, I make   the clients give up after a while, by changing the <b>until</b> loop to this:<pre>  my $give_up_after = time() + 10 * 60; # Give up after 10 minutes from now until( $conn-&gt;mkdir($lock_dir) ) { sleep 3; die "Tired of waiting for $lock_dir!" if time() &gt; $give_up_after; }</pre> <p> Second, I decide that a lock should not be any good if it's beyond a   certain age. The simplest way to do this is to crontab a process to run every   few minutes on the ftp server and to forcibly remove that directory if it exists   but is old. This is done on the presumption that any lock directory that old   must be the result of a process that didn't clean up after itself --   since no operation could take that long1. The program to run on the ftp server   is a simple one:<pre>  #!/usr/bin/perl # Delete phone_lock if it's from earlier than six minutes ago use strict; my $dir = "/evil/ftp/phone_lock"; my $six_mins_ago = time() - 6 * 60; if(-e $dir and (stat(_))[9] &lt; $six_mins_ago ) { rmdir $dir or warn "Couldn't rmdir $dir: $!"; } exit;</pre> <p> I arbitrarily chose ten minutes as how long any process would wait for a lock,   and six minutes as the maximum lifetime of a lock file. You'd need to come   up with figures that may be much longer or shorter, depending on the time it   could reasonably take any process to transfer <i>phone.tsv</i>, and the number   of processes that are ever likely to be queued up waiting for a lock at any   one time.<p> Incidentally, you can write a program like the above that kills old directory   files, and have it run not locally, but instead over ftp. But, the mechanics   of asking the age of a directory over ftp are very implementation-dependent,   and this is left as an exercise for the reader.<p> <b>Locksmith Processes</b><p> The real trouble in the above scenario is the possibility of processes creating   a <i>phone_lock</i> directory but failing to remove it. The program I showed   previously that removes too old <i>phone_lock</i> directories will eventually   catch all such cases. But in some situations we can do better. A program going   away without cleaning up its <i>phone_lock</i> directory might be the result   of network trouble, and we can't do much about that. But often it's   the result of the program throwing an exception. That's not very likely   in the above program, because the only thing happening between the <b>mkdir</b>   and the <b>rmdir</b> is a single <b>get</b> or <b>put</b> operation. But suppose   that the program were more complex, pulling down one file, running some sort   of elaborate conversion on it, and then sending it back up; and suppose that   that conversion could potentially run out of system memory, which isn't   a trappable exception -- the OS will simply kill the process. In that case,   there's no way for this process to be sure to <b>rmdir</b> that lock directory.<p> However, if we have mostly Unix-like2 process control where the program is   running, we can delegate the whole business of obtaining and releasing the lock   to a very simple independent subprocess, which I call a "locksmith".   All that a locksmith process needs to do is get the lock (or report that it   can't), and then wait around for one of two things to happen: either the   parent process's signaling that it wants the lock to be released, or the   parent process's untimely demise. When either happens, the locksmith processes   will then tidily remove the lock directory.<p> In very Perlish fashion, "there's more than one way to do it",   but this is the way I implement it: the parent process opens a one-way pipe   from <b>netlock</b>, which is what I've named my locksmith program. The   parent passes the information about how to log in as command-line parameters,   and then <b>netlock</b> prints one line: either something starting with "OK"   to signal that it got the lock, or anything else (consisting of an error message)   to signal that it couldn't get a lock. Then it just waits either for the   parent process to die (which can be discerned with <b>getppid() == 1</b>) or   for the parent process to close the child's pipe to the parent (which can   be detected by a <b>SIGPIPE</b> signal being thrown when the child tries to   write to that pipe).<p> The <b>netlock</b> program is simple enough, introducing few actually new   constructs:<pre>  use strict; # netlock. Call me like: "netlock phonestuff"  my %profile = ( 'phonestuff' =&gt; [ 'phoneacc', 'dmb4ever', 'ftp.xaos.int', 'phone.tsv' ], );  # - - - - - - - - - - - - - - - - use Net::FTP; my $profile = $ARGV[0]; $| = 1; # Don't buffer my STDOUT!  sub bye { print @_, "\n"; exit } # Routine to complain that we couldn't get a lock  bye "What profile?" unless defined $profile; my $auth = $profile{$profile}; bye "I don't know profile $profile'?" unless $auth;  my($user, $pass, $host, $dir) = @$auth; my $ftp = Net::FTP-&gt;new($host) or bye "Can't connect to $$auth[2]"; $ftp-&gt;login($user,$pass) or bye "Can't log into account $user\@$host";  my $quit_flag; $SIG{'PIPE'} = sub {$quit_flag = 1};  my $locked; <b> until($quit_flag or 1 == getppid) {</b> if( $ftp-&gt;mkdir($dir) ) { $locked = 1; last; } sleep 4; # Don't totally hammer the ftp server! } exit unless $locked; # If we aborted early somehow. print "OK\n"; # tell the parent that we got the lock!  # Wait around until the parent says to release, or dies. <b> until($quit_flag or 1 == getppid) {</b> $ftp-&gt;pwd(); # Don't let the connection time out. sleep 5; print STDOUT '.'; }  $ftp-&gt;rmdir($dir); $ftp-&gt;quit(); exit;</pre> <p> This may seem a bit mysterious, but there are only three unusual things here.   First, there's <b>getppid</b>, which returns the process ID of the parent   process (i.e., the process for which <i>netlock</i> is acting as locksmith).   A basic fact to note is that it returns 1 if and only if the original parent   process has died. Second, there is this <b>$SIG{'PIPE'} = sub {$quit_flag =   1}</b> line. This simply tells Perl "from now on, in this process, if you   get a <b>SIGPIPE</b> signal, call this routine that consists just of setting   <b>$quit_flag</b> to 1". And third, we have this mysterious loop that prints   a period to STDOUT every three seconds. Although the parent never reads anything   past the first line, you might expect that the dots do no good. However, we   are printing them for their side effect: if we print anything to a STDOUT (our   pipe to our parent process) and there's no SIGPIPE signal, then our channel   to our parent is alive and well. But, if the parent has closed the pipe, then   the printing to that pipe will throw a SIGPIPE, which will cause <b>$quit_flag</b>   to be set to 1, which will cause the loop to bail out, and the program to <b>rmdir</b>   the directory across FTP, and then quit.<p> As large as that <i>netlock</i> program is, it encapsulates the whole business   of getting a lock, so that the parent process need only call it like so:<pre>  open LOCKER, "/path/to/netlock phonestuff |" or die "Couldn't pipe from 'netlock phonestuff' ?!"; my $status = &lt;LOCKER&gt;; $status = '[netlock startup error]' unless defined $status; die "$status from netlock $lockname" unless $status =~ m/^OK/s;</pre> <p> and then when we want to release the lock, we simply call <b>close(LOCKER)</b>.<p> <b>Object Interface to Locksmith Processes</b><p> Just as in the last article I ended with a class that implemented a simple   class for <b>flock</b> semaphore files, I'll end this article with a class   that implements a simple interface for ftp lock directories as accessed and   tended by a <b>netlock</b> locksmith process.<pre>  package NetLock;  sub new { my($class, $tag) = @_; die "You forgot to specify a tag!" unless defined $tag and length $tag; open my $fh, "perl -S netlock $tag |" or die "Couldn't open netlock $tag: $!"; my $status = &lt;$fh&gt;; $status = '[netlock startup error]' unless defined $status; die "$status from netlock $tag" unless $status =~ m/^OK/s; return bless { 'fh' =&gt; $fh }, ref($class) || $class; }  sub unlock { my $fh = delete $_[0]{'fh'}; return 0 unless defined $fh; close($fh); } 1;</pre> <p> An object of class NetLock represents a lock on a resource as obtained across   ftp by a locksmith process that will delete that lock either when this process   asks it to, or when this process dies. Such an object is made by calling <b>NetLock-&gt;new('tagname')</b>   where <i>tagname</i> is a string, such as "phonestuff", that is in   <i>netlock</i>'s <b>%profile</b> hash of account data. The lock stays around   either for as long as the object is around, or until we dismiss the lock by   calling <b>$lock-&gt;unlock</b>, which will return false if we've already   closed it, or true in the case where it was just now closed.<p> <i>Sean M. Burke (</i><b>sburke@cpan.org</b><i>) quit his job at KAOS to work   on a book for O'Reilly. It's called </i>Perl &amp; LWP<i> and should   be out this summer. Sean would like to thank Arnar Hrafnkelsson, Uri Guttman,   Conrad Heiney, Jim Tranowski, and Ronald Schmidt for help and encouragement   on the locksmith part of this article.</i><p> <ul>1 Incidentally, this doesn't solve the quite serious problem of the aborted   upload leaving a truncated <i>phone.tsv</i> on the server. To deal with this,   the usual approach is to upload your <i>phone.tsv</i> as a randomly named temp   file, then make a lock directory, delete the remote server's old <i>phone.tsv</i>,   and rename your newly uploaded temp file to "phone.tsv", and then   <b>rmdir</b> the temp directory. Corresponding solutions apply in the equally   serious case of aborted downloads leaving a truncated local "phone.tsv". </ul><p> <ul>2 And by "UNIX-like", I do not mean MSWindows. However, as this   article is going to press, Ronald Schmidt has found a brilliantly simple way   to get this working under MSWindows: instead of setting a signal handler and   checking <b>getppid</b> (neither of which are currently implemented under Perl   for MSWindows), you just check the return value of the print STDOUT ".";   If it's a false value, then the pipe has broken, and the while loop should   be exited!</ul></body></html>