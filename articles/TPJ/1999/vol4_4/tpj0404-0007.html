<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>CS-Web: A Lightweight Summarizer for HTML - The Perl Journal, Winter 1999</title>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org">
  <meta name="vscategory" content="Perl">
  <meta name="vsisbn" content="">
  <meta name="vstitle" content=
  "CS-Web: A Lightweight Summarizer for HTML">
  <meta name="vsauthor" content="Tony Rose , Ave Wrigley">
  <meta name="searchdescription" content=
  "In 1997 we wrote CS-Web, a set of Perl programs to collect information from all of Canon's web sites, index it, and make it searchable from the web. We wrote our own solution because at the time the available products were either tools designed for searching the entire web (such as AltaVista), or for searching a single web site.">
  <meta name="vsimprint" content="The Perl Journal">
  <meta name="vspublisher" content="Earthweb">
  <meta name="vspubdate" content="Winter 1999">
  <!-- always update the article title and issue -->

</head>

<body>
  <font face="verdana" size="1">Issue 16, Winter 1999</font>

  <h2 align="center">CS-Web: A Lightweight Summarizer for HTML</h2>

  <h4><i>Tony Rose and Ave Wrigley</i></h4>
  <!-- packages described, if necessary -->

  <div align="center">
    <table border="1" cellspacing="0" cellpadding="5">
      <tr>
        <td align="center" bgcolor="#CCCC99"><b>Packages
        Used</b></td>
      </tr>

      <tr>
        <td>
        HTML::Summary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPAN<br>

        HTML::TreeBuilder&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPAN<br>

        Text::Sentence.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        Part of HTML::Summary<br></td>
      </tr>
    </table>
  </div>

  <p>Canon, like many other large companies, is a multi-national
  organization with multiple (26) web sites, each managed by a
  different part of the company. This is a problem for the typical
  Canon customer, who knows nothing about Canon's internal
  organization and simply wants to find information about their
  cameras or download a new printer driver. They need a single
  clear way to find what they want.</p>

  <p>CS-Web: a search engine for Canon's web space</p>

  <p>In 1997 we wrote CS-Web, a set of Perl programs to collect
  information from all of Canon's web sites, index it, and make it
  searchable from the web. We wrote our own solution because at the
  time the available products were either tools designed for
  searching the entire web (such as AltaVista), or for searching a
  single web site.</p>

  <p>CS-Web consists of a robot, a database, and a web interface
  (written in mod_perl). The robot traverses all of Canon web sites
  and stores a description of each page in the database. The search
  engine queries the database and gives you a list of candidate
  documents, and their descriptions. You can try CS-Web for
  yourself: it is linked from the main "gateway" page for Canon
  (http://www.canon.com/). Alternatively, you can access it
  directly at http://csweb.cre.canon.co.uk/.</p>

  <p>CS-Web presented a variety of challenges, many of which would
  make suitable war stories for TPJ. However, for this article, we
  will focus on one crucial problem: generating the summary of an
  HTML document.</p>

  <h3>META tags</h3>

  <p>Unlike some other search engines, CS-Web doesn't index the
  full text of the document. Instead, it indexes document
  descriptions. When web page authors use the META tag's NAME and
  CONTENT attributes to encode information about the document,
  CS-Web will use it. However, when that information isn't
  provided, CS-Web tries to boil down the text of the document into
  a summary.</p>

  <p>One important limitation on a document's summary is length:
  each web page corresponds to a row in a relational database
  table, and the size of each field in each row is fixed in
  advance. This is because fixed-width fields are much quicker to
  search than variable-width fields. You'll see later how this
  length constraint introduced its own problems.</p>

  <p>If we were deploying CS-Web across a lot of web sites, we'd
  have quickly found that very few web page authors consistently
  provide accurate metadata. In fact, deliberately misleading
  metadata is often included by unscrupulous authors to enhance the
  page's prominence in search engine results.</p>

  <p>However, the outlook for CS-Web was a little more promising.
  Since Canon's webmasters are generally working together, we could
  expect a certain level of integrity, and assume that they were
  not trying to deceive the CS-Web robot. In turn, the CS-Web robot
  could acknowledge this trust: if it found a page description
  within a META tag, it would accept the tag as legitimate.</p>

  <p>So much for metadata, but how could we generate a text
  description from the raw HTML when no metadata are present? By
  encoding some natural language processing techniques in Perl. The
  result was HTML::Summary.</p>

  <h3>HTML::Summary</h3>

  <p>HTML::Summary is available from the CPAN
  (ftp://ftp.perl.org/pub/CPAN/authors/id/A/AW/AWRIGLEY/); version
  0.016 is described in this article. First, you create an
  HTML::Summary object, using the <tt>new()</tt> method. You can
  provide configuration parameters as arguments to
  <tt>new()</tt>:</p>
  <pre>
    my $html_summarizer = new HTML::Summary LENGTH =&gt; 200;
</pre>

  <p>The LENGTH parameter is the maximum length in bytes for the
  generated summary. Next, you need an HTML::Element object
  corresponding to the HTML page that you want to summarize. You
  can generate one with HTML::TreeBuilder:</p>
  <pre>
    my $html_tree = new HTML::TreeBuilder;
    $html_tree-&gt;parse( $html_document );
</pre>

  <p><tt>$html_document</tt> is a string containing the HTML of the
  web page; this could have been read in from a file, or returned
  as the contents of an HTTP request, such as through LWP::Simple's
  <tt>get()</tt> method.</p>

  <p>Finally, you call the <tt>generate()</tt> method of the
  HTML::Summary object, with the HTML::Element object as an
  argument. That returns the summary of the page as a string:</p>
  <pre>
    $html_summary = $html_summarizer-&gt;generate( $html_tree );
</pre>

  <p>That's how you use it. But how does it work?</p>

  <h3>The Summarization Algorithm</h3>

  <p>One of the main tasks before us was generating a good abstract
  of fixed length from arbitrary text. This is known to be a
  important and difficult problem, and a quality solution requires
  sophisticated natural language techniques that can analyze the
  structure of the original text, identify key phrases and
  concepts, and regenerate these in a more succinct format.</p>

  <p>Luckily for us, there are some quick and dirty ways to
  generate summaries. We only needed to provide a gist of the
  original for someone browsing the CS-Web search results. In
  addition, for retrieval purposes, we wanted the summary to
  contain representative keywords.</p>

  <p>One advantage that we had over people trying to generate
  summaries from plain text is that HTML pages contain <i>markup
  information</i> --the HTML tags. Markup tells us about the
  structure of the content, and often about its relative importance
  as well. For example, it is usually clear in HTML pages where
  paragraphs begin and end, and when important text is
  <i>italicized</i>, <b>emboldened</b>, or made into a heading.</p>

  <p>The HTML::Summary module uses a technique known as the
  <i>location method</i> of text summarization. This consists of
  identifying important sentences (based primarily on their
  location in the text), and concatenating them together to produce
  an abstract. A simple example of this would be to take the first
  sentence of every paragraph in an article and string them
  together. This can sometimes be surprisingly effective:</p>

  <blockquote>
    <pre>
   Canon, like many other large companies, is a multi-national
   organization with multiple (26) web sites, each managed by a 
   different part of the company. In 1997 we wrote CS-Web, a 
   set of Perl programs to collect information from all of 
   Canon's web sites, index it, and make it searchable from the 
   web. CS-Web consists of a robot, a database, and a web 
   interface (written in mod_perl). CS-Web presented a variety 
   of challenges, many of which would make suitable war stories 
   for TPJ.
</pre>
  </blockquote>

  <p>The text summarization method used in HTML::Summary is an
  adaptation of the location method. It works as follows:</p>

  <p>&bull; Split into sentences</p>

  <blockquote>
    First, the text is split into sentences. (More about this
    later.)
  </blockquote>

  <p>&bull; Score the sentences</p>

  <blockquote>
    The sentences are scored according to what HTML element they
    appear in, and whether or not they are the first sentence in
    that element. The algorithm here is pretty simple: each element
    has a score. The first sentence in that element gets this
    score; the rest of the sentences get nothing.
  </blockquote>

  <p>&bull; Sort the sentences by score</p>

  <blockquote>
    The sentences are stored in an array of hashes. Each hash
    corresponds to a sentence, and contains information about the
    text in the sentence, its length, the HTML element it appeared
    in, the score given to it, and its original order in the text.
  </blockquote>
  <pre>
    $summary[ scalar( @summary ) ] = {
        'text'          =&gt; $text,
        'length'        =&gt; length( $text ),
        'tag'           =&gt; $tag,
        'score'         =&gt; $score,
        'order'         =&gt; scalar( @summary ),
    };
</pre>

  <blockquote>
    The scores, as described above, are based on the HTML element
    that the sentences appear in. These scores are stored in a
    global hash:
  </blockquote>
  <pre>
    my %ELEMENT_SCORES = (
        'p'         =&gt; 100,
        'h1'        =&gt; 90,
        'h2'        =&gt; 80,
        'h3'        =&gt; 70,
    );

</pre>

  <p>These scores were arrived at by empirical investigation; we
  have no theoretical justification for them.</p>

  <p>&bull; Truncate the list of sentences</p>

  <blockquote>
    Calculate how many sentences are needed to exceed the requested
    summary length.
  </blockquote>

  <p>&bull; Sort the sentences by original order again</p>

  <blockquote>
    Having remembered the original sentence order in the text in
    the hash for that sentence, we can now re-sort the sentences in
    that order.
  </blockquote>

  <p>&bull; Concatenate the sentences to create the summary</p>

  <blockquote>
    Spaces are added between the sentences, since whitespace was
    stripped when the sentences were split.
  </blockquote>

  <p>&bull; Truncate the summary at the requested length</p>

  <blockquote>
    This last step assumes that if you want a summary of 200
    characters, 201 characters are not acceptable . even if it
    means chopping the summary off mid-sentence. This is what we
    wanted in CS-Web. Maybe in other applications a less severe
    approach would be appropriate . it's easy to add more options
    to HTML::Summary, so let us know what you think.
  </blockquote>

  <h3>Sentence Splitting</h3>

  <p>Now for the nitty gritty. The remainder of this article
  focuses on just one aspect of the HTML::Summary: splitting the
  element contents into sentences. Japanese character encodings
  were a particular problem for CS-Web; our approach is described
  in a sidebar: <a href="tpj0404-0007a.html"><b>Truncating Japanese
  Text</b></a>.</p>

  <p>The task of splitting text into sentences seemed like a more
  general problem than its application to text summarization, so
  this is contained in a separate module: Text::Sentence. For the
  moment, this is distributed as part of the HTML::Summary package,
  but we would be interested to learn if there is any interest in
  using this module independently.</p>

  <p>Text::Sentence is basically just a regex. It is has a
  non-object oriented interface that exports one function,
  <tt>split_sentences()</tt>, that takes the text to be split into
  sentences as an argument, and returns a list of the
  sentences.</p>
  <pre>
  sub split_sentences
  {
      my $text = shift;

      return () unless $text;
</pre>

  <p>The function first checks if there really is any text to split
  into sentences; if not, it just returns the empty string.</p>

  <p># $capital_letter is a character set; to account for locale,
  this</p>

  <p># includes all letters for which lc is different from that
  letter.</p>
  <pre>
  my $capital_letter =  
      '[' . 
          join( '', 
            grep { lc( $_ ) ne ( $_ ) } 
             map { chr( $_ ) } ord( "A" ) .. ord( "\xff" )
          ) . 
      ']'
  ;
</pre>

  <p>Although it would be more efficient to compute this regex
  component once at the package level, doing it in
  <tt>split_sentences()</tt> allows the user to change locales
  between calls.</p>

  <p>The next few lines build up the components of the regex that
  split the text into sentences. The first of these components is
  the capital letter found at the start of a sentence. Instead of
  using the character class [A-Z] as you would normally,
  Text::Sentence accounts for locale-specific capital letters. For
  example, in French, a capital A acute (&Aacute;) won't be matched
  by [A-Z]. The method used in Text::Sentence makes use of the fact
  that lc is sensitive to locale settings, and returns a lowercase
  version of all capitalized characters. For more information on
  how Perl handles locales, see the perllocale documentation.</p>
  <pre>
        @PUNCTUATION = ( '\.', '\!', '\?' );
</pre>

  <p>The @PUNCTUATION array is a global variable in Text::Sentence
  containing any punctuation used to indicate the end of a
  sentence. The fact that it's a global means that you're
  encouraged to change it. For example, you might want to add
  locale specific punctuation for the Spanish . &iexcl;':</p>
  <pre>
    my $html_summarizer = new HTML::Summary LENGTH =&gt; 200;
    push( @HTML::Summary::PUNCTUATION, chr( 161 ) ); 
</pre>

  <p>Back to <tt>split_sentences()</tt>:</p>
  <pre>
  # this needs to be alternation, not a character class, 
  # because of multibyte characters
  my $punctuation = '(?:' . join( '|', @PUNCTUATION ) . ')';
</pre>

  <p>As mentioned above, one of our concerns was dealing with
  multibyte character encodings (see the sidebar on <i>Truncating
  Japanese Text</i>). Japanese punctuation characters may be more
  than one character long. For example, an exclamation point in the
  EUC Japanese encoding is "\xA1\xAA".</p>
  <pre>
# return $text if there is no punctuation ... return 
$text unless $text =~ /$punctuation/; 
</pre>

  <p>If these isn't any end-of-sentence punctuation in the text,
  then we might as well return the text now.</p>
  <pre>
 my $opt_start_quote = q/['"]?/;
 my $opt_close_quote = q/['"]?/;
 
 # these are distinguished because (eventually!) I would like to do 
 # locale stuff on quote  characters
 
 my $opt_start_bracket = q/[[({]?/; # }{
 my $opt_close_bracket = q/[\])}]?/;
 
</pre>

  <p>Sentences sometimes have quotation marks or parentheses which
  come before the capital letter at the beginning, or after the
  full stop (period, question mark, or exclamation point) at the
  end. For example, the following sentence:</p>

  <blockquote>
    Larry said "let there be light!" (And there was.)
  </blockquote>is two sentences; the first ends after the second
  double quote. However:

  <blockquote>
    Larry said "let there be light!" (and there was).
  </blockquote>is one sentence. Here is the regex in all its glory:
  <pre>

  my @sentences = $text =~ /
  (
                          # sentences start with ...
      $opt_start_quote    # an optional start quote
      $opt_start_bracket  # an optional start bracket
      $capital_letter     # a capital letter ...
      .+?                 # at least some (non-greedy) chars ...
      $punctuation        # ... followed by any one of !?.
      $opt_close_quote    # an optional close quote
      $opt_close_bracket  # and an optional close bracket
  )
  (?=                     # with lookahead that it is followed by ...
      (?:                      # either ...
          \s+                  # some whitespace ...
          $opt_start_quote     # an optional start quote
          $opt_start_bracket   # an optional start bracket
          $capital_letter      # an uppercase word character 
                               # (for locale sensitive matching)
      |                        # or ...
          \n\n                 # a couple (or more) of CRs
                               #(i.e. a new para)
      |                        # or ...
          \s*$                 # optional whitespace and by end of string
      )
  )
  /gxs
  ;
  return @sentences if @sentences;
  return ( $text );
}
</pre>

  <p>This regex makes use of the lookahead feature of regular
  expressions. In this case, it allows us to specify that a
  sentence must not only start with a capital letter, and end in a
  full stop, but that there must be another capital letter which
  follows the full stop. The only exception to this is when the
  sentence is at the end of a paragraph.</p>

  <p>The lookahead accounts for the whitespace between sentences,
  so it's not part of the matched patterns that end up in the
  <tt>@sentences</tt> array. That's why concatenating the sentences
  won't give you back the exact original text.</p>

  <p>The main problem with trying to split text into sentences is
  that there are several uses for periods, such as
  abbreviations.</p>

  <blockquote>
    Dr. Livingstone, I presume.
  </blockquote>

  <p>This phrase counts as two sentences according to
  Text::Sentence -- the first sentence is three characters long.
  The performance of Text::Sentence could be improved by taking
  into account special cases like honorifics (<tt>Mr., Mrs.,
  Dr.</tt>), common abbreviations (<tt>e.g., etc., i.e.</tt>), and
  so on. However, as with many natural language problems, this
  obeys the law of diminishing returns; a little bit of effort will
  do a decent 90% job, but that last 10% is pretty difficult. For
  our purposes the 90% is good enough.</p>

  <h3>Conclusion</h3>

  <p>We chose to use Perl for CS-Web because of the obvious
  benefits: the LWP modules for web programming, DBD/DBI,
  <tt>mod_perl</tt>, and so on. We found that Perl is also a useful
  tool for doing natural language work. Its text processing
  features, rapid development cycle, and ability to generate
  complex data structures on the fly make it particularly
  appropriate.</p>

  <p>A lot of interesting work in natural language research
  involves analyzing corpus data; collecting statistics about
  language use over large databases of typical usage. The web is an
  obvious rich source of this type of data, and in view of this, it
  is a little surprising how few tools and modules appeared to be
  available in Perl for this field. Certainly, when we were working
  on the Text::Sentence regex, we posted something to a language
  processing mailing list, and there seemed to be quite a lot of
  interest in what we were doing, as well as extensive Perl
  expertise in that community. Hopefully natural language
  processing will become yet another nut for Perl to crack!</p>

  <p>_ _END_ _</p>
  <hr>
  <i>Tony Rose has a background in natural language processing and
  has published widely in the area of speech and language
  technology. He joined Canon Research Centre in November 1996 and
  is now manager of the Information Retrieval Department. Prior to
  this position he worked at Hewlett-Packard Laboratories on
  language modelling for speech recognition. He then completed a
  research fellowship at BT Labs on Internet agents for information
  retrieval. His research interests include natural language
  interfaces and information retrieval.<br>
  <br></i>

  <p><i>Ave Wrigley is webmaster at www.itn.co.uk. He has been
  developing web applications in perl for the last five years, and
  is the author of WWW::Sitemap and co-author of WWW::Robot and
  HTML::Summary. He can be contacted at <a href=
  "mailto:Ave.Wrigley@itn.co.uk">Ave.Wrigley@itn.co.uk</a>.</i></p>
  <!-- end of article -->
  <!-- end of file -->
</body>
</html>
