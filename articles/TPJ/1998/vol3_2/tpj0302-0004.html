<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Learning Japanese - The Perl Journal, Summer 1988</title>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org">
  <meta name="vscategory" content="Perl">
  <meta name="vsisbn" content="">
  <meta name="vstitle" content="Learning Japanese">
  <meta name="vsauthor" content="Tuomas J. Lukka">
  <meta name="searchdescription" content="">
  <meta name="vspublisher" content="Earthweb">
  <meta name="vsimprint" content="The Perl Journal">
  <meta name="vspubdate" content="Summer 1998">
  <!-- always update the article title and issue -->

  <!-- end head -->
</head>

<body>
  <font face="verdana" size="1">Issue 10, Summer 1998</font>

  <h2 align="center">Learning Japanese</h2>

  <h4><i>Tuomas J. Lukka</i></h4>
  <!-- packages described, if necessary -->

  <div align="center">
    <table border="1" cellspacing="0" cellpadding="5">
      <tr>
        <td colspan="2" align="center" bgcolor="#CCCC99">
        <b>Required Packages</b></td>
      </tr>

      <tr>
        <td><b>Package</b></td>

        <td><b>Version</b></td>
      </tr>

      <tr>
        <td>Perl</td>

        <td>5.004 _04</td>
      </tr>

      <tr>
        <td>libwww-perl</td>

        <td>5.14</td>
      </tr>

      <tr>
        <td>MIME-Base64</td>

        <td>2.03</td>
      </tr>
    </table>
  </div>

  <p>I like to learn new languages by plunging into a good book.
  For Italian it was Pinocchio, for English <i>The Moon is a Harsh
  Mistress</i>. I keep a dictionary handy, and I spend many hours
  rechecking words until I remember them. It's tedious, but in the
  end is still a more interesting way to learn than the usual route
  of beginning with very simple phrases and vocabulary and building
  up slowly, reading childish stories about uninteresting
  subjects.</p>

  <p>I tried this with a book on Go strategy written in Japanese,
  and quickly hit a wall. With many languages you can infer the
  meaning of words by what they look like and remember them by how
  they sound. But in everyday Japanese text, there is no way for a
  beginner to know how a given phrase is pronounced until he learns
  the two thousand characters in common use. Furthermore, each
  character can usually be pronounced in two or more ways depending
  on the context (see: <a href="tpj0302-0004a.html">Japanese
  Charecters</a>).</p>

  <p>It might still be possible to learn Japanese with this method,
  but the task is complicated further still by the fact that the
  character dictionaries are not very quick to use - Japanese has
  2000 characters, so you have to find the words graphically, which
  is much more time-consuming. You can't leaf through the
  dictionary as you can with western writing systems.</p>

  <p>So I ended up auditing the Japanese course at the university
  where I work. Even though the teacher made the course as much fun
  as a language course can be, learning kanji was difficult because
  of the feeling of not seeing them in real, interesting
  contexts.</p>

  <h3>THE WEB</h3>

  <p>Eventually I found an invaluable resource for learning and
  using Japanese on the web, <a href=
  "ftp://ftp.monash.edu.au/pub/nihongo" target=
  "resource window">ftp://ftp.monash.edu.au/pub/nihongo</a>. This
  site has two freely available dictionaries that convert Japanese
  to English: <i>edict</i> and <i>kanjidic</i>. There are also
  instructions on how to view and edit Japanese on various
  operating systems.</p>

  <p><b>Listing 2</b>: <a href="tpj0302-0004b.html">Displaying
  Kanji</a></p>

  <p>There were a few Japanese web pages about Go, and I'd visited
  them several times, each time hoping that my proficiency had
  improved enough to let me read them. Each time I found that I
  didn't know enough, and so I came up with an idea: Why not simply
  look up the characters automatically?</p>

  <p>The simplest design I could think of was a CGI script that
  fetches the page and inserts the definitions of the kanji. Now I
  can browse any web page I like, and the kanji are automatically
  translated to English. Perl and CPAN made this nearly as simple
  as it sounds. I called the result wwwkan.pl (see <a href=
  "tpj0302-0004d.html">Listing 4</a>).</p>

  <h3>DICTIONARY DATABASE</h3>

  <p>The dictionaries are fairly large and it would take too long
  to load and parse them again whenever the script is called. There
  are several solutions. You could have a dictionary server that
  sits in memory all the time, responding to queries as they
  arrive, or you could store the dictionary in a hashed database.
  For simplicity I chose the latter. The script which converts the
  dictionary files into hash entries is shown in gendb.pl (see
  <a href="tpj0302-0004c.html">Listing 3</a>).</p>

  <p>The format of the <i>edict</i> dictionary is straightforward:
  first the kanji, then a space, and finally the definition. The
  loop to parse the file:</p>
  <pre>
open DIC, "$dir/edict" or die "Can't open $dir/edict";
while (&lt;DIC&gt;) {
    next if /^#/;
    /^(\S+)\s/ or die("Invalid line '$_'");
    $kanji{$1} .= $_;
}
close DIC;
</pre>

  <p>The second dictionary file, <i>kanjidic</i>, is slightly more
  complicated, as there are several fields on each line explaining
  different aspects of the kanji in question:</p>

  <p align="center"><a href="images/japanese-fig1.gif"><img src=
  "images/japanese-fig1-small.gif" align="center" alt=
  "Figure 1: Screenshot 1" border="0" width="196" height=
  "144"></a></p>

  <p align="center"><font size="-1"><b>Figure 1:</b> The subject
  listing of the Japanese Yahoo web site.</font></p>

  <p>(<i>kanji</i>) <tt>3027 U6328 N1910 B64 S10 I3c7.14 L2248
  P1-3-7 Wae Yai1 Yai2 Q5303.4 MN12082 MP5.0229</tt>
  (<i>hiragana/kata-kana readings</i>) <tt>{push open}</tt></p>

  <p>The various numbers represent different ways of indexing the
  kanji, e.g. <tt>N1910</tt> means that this kanji is number 1910
  in Nelson's <i>Modern Reader's Japanese-English Character
  Dictionary</i> and <tt>Wae</tt> means that the romanized Korean
  reading of this kanji is 'ae'. However interesting this
  information might be, it clutters up our web page, so let's get
  rid of most of it:</p>
  <pre>
s/\s[UNBSMHQLKIOWYXEPCZ][\w-.]*//g;
</pre>

  <p>In the parsing loop, <tt>%kanji</tt> isn't just any hash. It's
  a tied hash:</p>
  <pre>
tie %kanji, AnyDBM_File, 'kanji.dbmx',
               O_CREAT | O_RDWR | O_TRUNC, 0755;
</pre>

  <p>This ties <tt>%kanji</tt> to the file kanji.dbmx using
  AnyDBM_File, a handy module bundled with Perl that lets hashes
  stored on disk appear to be in memory. [<i>Editor's note</i>:
  <a href="tpj0302-0002.html" target="resource window">Infobots and
  Purl (p. 10)</a> does the same. -Jon]</p>

  <p>Adding entries to the database is then as simple as
  saying:</p>
  <pre>
$kanji{$1} .= $_;
</pre>

  <p>This stores the entry in the file. I use the .= operator
  instead of = because there can be multiple entries for different
  meanings of characters or character sequences. After we are done
  with it, we untie <tt>%kanji</tt> to break the connection between
  the hash and the disk file.</p>

  <h3>THE CGI SCRIPT</h3>

  <p>The CGI script <a href="tpj0302-0004d.html">wwwkan.pl.</a>
  uses two different libraries as its frontend and backend:
  libwww-perl (LWP, available on CPAN) is used to fetch the HTML
  document from the server and CGI.pm (provided with the latest
  Perl distribution) to parse the request from the HTTP daemon and
  create the HTML to be returned.</p>

  <p>The script begins with</p>
  <pre>
tie %kanji, AnyDBM_File, "$libdir/kanji.dbmx", O_RDONLY, 0;
</pre>which opens the kanji database created by the other script -
the contents of <tt>%kanji</tt> are read back from the file when
requested.

  <p>Next we print the CGI header and a form for the new URL:</p>

  <p align="center"><a href="images/japanese-fig2.gif"><img src=
  "images/japanese-fig2-small.gif" align="center" alt=
  "Figure 1: Screenshot 1" border="0" width="198" height=
  "146"></a></p>

  <p align="center"><font size="-1"><b>Figure 1:</b> The same page
  viewed through wwwkan.pl.</font></p>
  <pre>
print $query-&lt;header,
  "CONVERTED By TJL's kanji explainer on ", `date`,
  '. Mail comments to lukka@fas.harvard.edu.&lt;P&gt;',
  $query-&gt;startform(),
    "&lt;b&gt;Go To:&lt;/b&gt; ",
    $query-&gt;textfield(-name =&gt; 'url',
                  -default =&gt; 'http://www.yahoo.co.jp/',
                  -size =&gt; 50),
  $query-&gt;submit('Action', 'Doit'),
  $query-&gt;endform,
  "&lt;HR&gt;\n";
  
</pre>

  <p>For more explanation of what is happening, Lincoln Stein's
  documentation in CGI.pm or any of his TPJ columns.</p>

  <p>After printing the form, the script retrieves the web
  page:</p>
  <pre>
$url = $query-&gt;param('url');
$doc = get $url;
</pre>Now we have the HTML code of the page that was specified in
the <tt>url</tt> field of the form in <tt>$doc</tt>.

  <p>The next task is to replace all the links to other HTML files
  with links through our engine.</p>
  <pre>
$h = parse_html($doc);
$h-&gt;traverse(
    sub {
      my ($e, $start) = @_;
      return 1 unless $start;
      my $attr = $links{lc $e-&gt;tag} or return 1;
      my $url = $e-&gt;attr($attr-&gt;[0]) or return 1;
      $e-&gt;attr($attr-&gt;[0],
      ($attr-&gt;[1] ? <b>getlink</b>($url) : <b>abslink</b>($url)));
   },
1);
</pre>

  <p>See the HTML::Parse documentation for further details. The
  anonymous subroutine <tt>(sub { ... })</tt> merely checks whether
  this tag has a URL field, using the hash that we initialized at
  the beginning of the program:</p>
  <pre>
# 0 = absolute, 1 = relative
%links = ( a      =&gt; ['href', 1],
           img    =&gt; ['src', 0],
           form   =&gt; ['action', 1],
           link   =&gt; ['href', 1],
           frame  =&gt; ['src', 1]);
</pre>

  <p>The anonymous subroutine in the call to
  <tt>$h-&gt;traverse</tt> rewrites any URLs that appear on the
  page. URLs that are believed to contain text are rewritten with
  <tt>getlink()</tt> so that my translation engine filters them.
  URLs that represent images are replaced with absolute (prefaced
  with <tt>http://</tt>) links by the <tt>abslink()</tt>
  subroutine.</p>
  <pre>
sub abslink {
    return (new URI::URL($_[0]))-&gt;abs($url)-&gt;as_string;
}

sub getlink {
    my $url_to = (new URI::URL($_[0]))-&gt;abs($url);
    my $proxy_url = new URI::URL($my_url);
    $proxy_url-&gt;query_form(url =&gt; $url_to-&gt;as_string);
    return $proxy_url-&lt;as_string;
}
</pre>

  <p>After modifying the tags in the parsed form, this line
  retrieves the modified HTML:</p>
  <pre>
$doc = $h-&gt;as_HTML;
</pre>

  <p>Next, the climactic ending of the script:</p>
  <pre>
for ( split "\n", $doc ) {
    s/((?:<b>[\x80-\xFF][\x40-\xFF]</b>)+)/explainstr($1)/ge;
    print;
}
</pre>

  <p>This converts the text into explained kanji one line at a
  time. The regular expression matches one or more Japanese
  char-acters: each is stored in two bytes with the highest bit in
  the first byte set. The <tt>/e</tt> modifier is used to replace
  them with the output of the <tt>explainstr()</tt> subroutine,
  which converts a string of kanji into an English explanation:</p>
  <pre>
sub explainstr {
    my $str = @_;
    my $res = "";
    my ($pos, $mlen, $s);
    for ( $pos = 0; $pos &lt; length($str); $pos += $mlen ) {
        my $expl;
        $mlen = 20;
        while ( !defined($expl =
            $kanji{$s=(substr(($str),$pos,$mlen))})
                                and $mlen &gt; 2) {
            $mlen -= 2;
        }
        $res .= $s;
        if (defined $expl) {
            $res .= " &lt;small&gt;&lt;[[[".($expl)."]]]&gt;&lt;/small&gt; ";
         }
    }
    return $res;
}
</pre>

  <p>The inner loop is necessary because we wish to find the
  longest match available in the dictionary. (We want to translate
  "word processor", not the shorter matches "word" and
  "processor".)</p>

  <h3>TAKING IT A STEP FURTHER</h3>

  <p>This design is good if you don't know any Japanese, but once
  you've learned the basic characters (e.g. 'one', 'large'...), it
  gets tedious to see their definitions over and over again. We
  need a way to categorize the difficulty of characters, and
  luckily, the Japanese Ministry of Education has done most of our
  work for us. They have divided kanji into grades for school. The
  kanjidic file contains the grade number of each kanji, so we can
  include an option that disables translation below a particular
  grade. This can be achieved with the regex <tt>/G([0-9])/</tt> in
  the <i>explainstr</i> loop and checking <tt>$1</tt> to see
  whether we should explain this character.</p>

  <p>Of course, different people have different interests. For
  exam-ple, I have learned several terms relating to Go but far
  fewer that relate to, say, art history. It would be nice to be
  able to provide a detailed list of what kanji I know. It is easy
  to envision CGI interfaces to a personalized database containing
  the kanji that you know, but let's KISS (Keep It Simple, Stupid)
  for now. The easiest solution is to write the numbers of the
  kanji I know into a file. As a bonus, I can use the same file to
  generate a selection of words from <i>kanjidic</i> and
  <i>edict</i> to use with the <i>kdrill</i> program to drill
  myself on the kanji I should know.</p>

  <p>Also, some Japanese pages use an alternate encoding called
  Shift-JIS. To handle both encodings without degrading
  perfor-mance, I adapted the code used by the <i>xjdic</i> program
  (from the Monash archive) and made it into an XS module,
  available from my author directory in the CPAN.</p>

  <p>Even though all these changes would be useful, they are fairly
  trivial so I shall not present the code here - by the time this
  issue goes to press I expect to have a module available at:
  <a href=
  "http://www.perl.com/CPAN/modules/by-authors/Tuomas_J_Lukka"
  target=
  "resource window">http://www.perl.com/CPAN/modules/by-authors/Tuomas_J_Lukka</a>.</p>

  <h3>CONCLUSION</h3>

  <p>This tool has proven itself quite useful - I am able to keep
  up my study of Japanese by reading interesting material. The
  effort that went into making these scripts was not large; only
  about 5 hours to get the most complicated (messy) version, and a
  few more to clean them up for TPJ.</p>

  <p>There are several problems with this approach. The most
  serious is that images of characters cannot be translated - you
  have to resort to a traditional dictionary (I recommend xjdic
  from the Monash archive). Another problem is the fact that
  Japanese inflects verbs and has short particles all over the
  sentence (e.g. <img src="images/char1.gif" width="17" height="21"
  border="0" alt="">, the subject marker, <img src=
  "images/char2.gif" width="20" height="21" border="0" alt="">, the
  object marker, <img src="images/char3.gif" width="19" height="22"
  border="0" alt="">, the word for 'with', and so on). Therefore,
  the displayed text given by <a href=
  "tpj0302-0004d.html">wwwkan.pl.</a> is sometimes spurious. A good
  rule of thumb is that all entries that are one or two hiragana
  characters should be viewed with suspicion.</p>

  <p>As a teaser, I might mention that my study of Japanese is
  related to my work on a Go-playing program, which I'm writing
  mostly in Perl (PDL for the speed-critical parts - <a href=
  "http://pdl.perl.org" target=
  "resource window">http://pdl.perl.org</a>) for speed-critical
  parts) but that is a story for another time.</p>

  <p>__END__</p>
  <hr>
  <i>Tuomas J. Lukka is currently a Junior Fellow at Harvard
  University, working on computer learning and physical chemistry.
  He spends his time writing programs, playing music, and pondering
  molecular quantum mechanics.</i> <!-- end of article -->
   <!-- end of file -->
</body>
</html>
