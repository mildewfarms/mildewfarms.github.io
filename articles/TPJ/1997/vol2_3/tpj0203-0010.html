<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Webpluck - The Perl Journal, Fall 1997</title>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org">
  <meta name="vscategory" content="Perl">
  <meta name="vsisbn" content="">
  <meta name="vstitle" content="Webpluck">
  <meta name="vsauthor" content="Peter Prymmer">
  <meta name="searchdescription" content=
  "The promises of smart little web agents that run around the web and grab things of interest to you have gone unfulfilled. Like me, you probably have a handful of web pages that you check on a regular basis, and if you had the time, you'd check many more.">
  <meta name="vsimprint" content="The Perl Journal">
  <meta name="vspublisher" content="Earthweb">
  <meta name="vspubdate" content="Fall 1997">
  <!-- always update the article title and issue -->

  <!-- end head -->
</head>

<body bgcolor="#FFFFFF">
  <font face="verdana" size="1">Issue 7, Fall 1997</font>

  <h2 align="center">Webpluck</h2>

  <h4><i>Ed Hill</i></h4><!-- packages described, if necessary -->
  The promises of smart little web agents that run around the web
  and grab things of interest to you have gone unfulfilled. Like
  me, you probably have a handful of web pages that you check on a
  regular basis, and if you had the time, you'd check many more.

  <p>Listed below are a few of the bookmarks that I check on a
  regular basis. Each of these pages has content that changes every
  day, and it is, of course, the content of these pages that I am
  interested in - not their layout, nor the advertising that
  appears on the pages.</p>

  <p>Dilbert (of course)
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CNN
  US News<br>
  Astronomy Picture of the Day
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C|Net's
  NEWS.COM<br>
  The local paper (The Daily
  Iowan)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ESPNET
  Sportszone</p>

  <p>These pages are great sources of information. My problem is
  that I don't have time to check each one every day to see what is
  there or if the page has been updated. What I want is my own
  personal newspaper that is built from the sources listed
  above.</p>

  <h3>Similar Tools</h3>

  <p>This is not an original idea, but after spending many hours
  searching for a tool to do what I wanted, I gave up. Here are the
  contenders I considered, and why they didn't do quite what I
  wanted.</p>

  <p>First there is the "smart" agent. This little gremlin roams
  the net trying to guess what you want to see using either some AI
  technique or other algorithm. Firefly (<a href=
  "http://www.firefly.com/" target=
  "resource window">http://www.firefly.com/</a>) is an example; you
  tell it you are interested in a particular topic and it points
  you at a list of sites that others have ranked. When I first
  looked at Firefly, it suggested that since I was interested in
  "computers and the internet," I should check out "The Amazing
  Clickable Beavis"<br>
  (<a href="http://web.nmsu.edu/~jlillibr/ClickableBeavis/" target=
  "resource window">http://web.nmsu.edu/~jlillibr/ClickableBeavis/</a>).</p>

  <div align="center">
    <img src="images/beavis.gif" width="101" height="223" border=
    "0" alt="beavis">
  </div>

  <p>This is why I don't have much confidence in agents. Besides, I
  know what I want to see. I have the URLs in hand. I just don't
  have the time to go and check all the pages every day.</p>

  <p>The second type of technology is the "custom newspaper." There
  are two basic types. CRAYON ("Create Your Own Newspaper,"
  headquartered at <a href="http://www.crayon.net/" target=
  "resource window">http://www.crayon.net/</a>), is one flavor of
  personalized newspaper. CRAYON is little more than a page full of
  links to other pages that change everyday. For me, CRAYON just
  adds to the problem, listing tons of pages that I wish I had time
  to check out. I was still stuck clicking through lists of links
  to visit all the different pages.</p>

  <p>Then there are sites like My Yahoo (<a href=
  "http://my.yahoo.com/" target=
  "resource window">http://my.yahoo.com/</a>), a single page whose
  content changes every day. This is very close to what I needed -
  a single site with all of the information I need. My Yahoo
  combines resources from a variety of different sources. It shows
  a one-line summary of an article; if it's something that I find
  interesting, I can click on the link to read more about it. The
  only problem with My Yahoo is that it's restricted to a small set
  of content providers. I want resources other than what Yahoo
  provides.</p>

  <p>Since these tools didn't do exactly what I wanted, I decided
  to write my own. I figured with Perl, the LWP library, and a
  weekend, I could throw together exactly what I wanted. Thus
  webpluck was born. My goal was to write a generic tool that would
  automatically grab data from any web page and create a
  personalized newspaper exactly like My Yahoo. I decided the best
  approach was to define a regular expression for each web page of
  interest. webpluck uses the LWP library to retrieve the web page,
  extracts the content with a regular expression tailored to each,
  and saves it to a local cache for display later. Once it has done
  this for all the sources, I use a template to generate my
  personal newspaper.</p>

  <h3>How To Use Webpluck</h3>

  <p>I don't want this article to turn into a manual page (since
  one is provided), but here is a brief summary of how to use
  webpluck. You first create a configuration file containing a list
  of targets that define which pages you want to read, and the
  regular expression to match against the contents of that page.
  Here is an example of a target definition that retrieves
  headlines from the CNN US web page.</p>
  <pre>
name     cnn-us 
url      http://www.cnn.com/US/ 
regex    &lt;h2&gt;([^\&lt;]+)&lt;\/h2&gt;.*?&lt;a href=\"([^\"]+)\" 
fields   title:url 
</pre>These definitions define the following: the name of the file
to hold the data retrieved from the web page; the URL of the page
(if you point at a page containing frames, you need to determine
the URL of the page that actually contains the content); the Perl
regular expression used to extract data from the web page; and the
names of the fields matched in the regular expression that you just
defined. The first pair of parentheses in the regex field matches
the first field, the second pair matches the second, and so on. For
the configuration shown, (<tt>[^\&lt;]+</tt>) is tagged as the
title and (<tt>[^\"]+</tt>) is tagged as the <tt>url</tt>. That
<tt>url</tt> is the link to the actual content, distinct from the
<tt>url</tt> definition on the second line, which is the starting
point for the regex.

  <p>Running <i>webpluck</i> with the target definition above
  creates a file called <i>cnn-us</i> in a cache directory that you
  define. Here's the file from March 25th, 1997:</p>
  <pre>
title:Oklahoma bombing judge to let 'impact witnesses' see trial 
url:http://www.cnn.com/US/9703/25/okc/index.html
&#13;title:Simpson's attorneys ask for a new trial and lower damages 
url:http://www.cnn.com/US/9703/25/simpson.newtrial/index.html 
&#13;title:U.S. playing low-key role in latest Mideast crisis 
url:http://www.cnn.com/WORLD/9703/25/us.israel/index.html 
&#13;title:George Bush parachutes -- just for fun 
url:http://www.cnn.com/US/9703/25/bush.jump.ap/index.html 
</pre>

  <p>An OJ story. Who would have guessed?</p>

  <p>As you might expect, everything depends on the regular
  expression, which must be tailored for each source. Not everyone,
  myself included, feels comfortable with regular expressions; if
  you want to get the most use out of webpluck, and you feel that
  your regular expression skills are soft, I suggest buying Jeffrey
  Friedl's book <i>Mastering Regular Expressions</i>.</p>

  <p>The second problem with regular expressions is that as
  powerful as they are, they can only match data they expect to
  see. So if the publisher of the web page you are after changes
  his or her format, you'll have to update your regular expression.
  <i>webpluck</i> notifies you if it couldn't match anything, which
  is usually a good indication that the format of the target web
  page has changed.</p>

  <p>Once all the content has been collected, webpluck takes those
  raw data files and a template file that you provide, and combines
  them to create your "dynamic" HTML document.</p>

  <p><i>webpluck</i> recognizes a new "tag" in your template file
  that has been defined called <tt>&lt;clip&gt;</tt>. It replaces
  these tags with data from the raw data files that you have
  created with webpluck. Everything else in the template file is
  passed through as is. Here is an example of a segment in my daily
  template file (again using the CNN US headlines as an
  example).</p>
  <pre>
&lt;clip name="cnn-us"&gt; 
&lt;li&gt;&lt;a href="url"&gt;title&lt;/a&gt; 
&lt;/clip&gt; 
</pre>

  <p>This is replaced with the following HTML (the lines have been
  split to make them more readable):</p>
  <pre>
&lt;li&gt;&lt;a href="http://www.cnn.com/US/9703/25/okc/index.html"&gt;\ 
Oklahoma bombing judge to let 'impact witnesses' see trial&lt;/a&gt; 
&#13;&lt;li&gt;&lt;a href="http://www.cnn.com/US/9703/25/simpson.newtrial/index.html"&gt;\ 
Simpson's attorneys ask for a new trial and lower damages&lt;/a&gt; 
&#13;&lt;li&gt;&lt;a href="http://www.cnn.com/WORLD/9703/25/us.israel/index.html"&gt;\ 
U.S. playing low-key role in latest Mideast crisis&lt;/a&gt; 
&#13;&lt;li&gt;&lt;a href="http://www.cnn.com/US/9703/25/bush.jump.ap/index.html"&gt;\ 
George Bush parachutes -- just for fun &lt;/a&gt; 
</pre>

  <p>I personally use <i>webpluck</i> by running one <i>cron</i>
  job every morning and one during lunch to re-create my "daily"
  page. I realize <i>webpluck</i> could be used for a lot more than
  this; that's left as an exercise for the reader.</p>

  <h3>How Webpluck Works</h3>

  <p>Now on to the technical goodies. For those who don't know what
  the LWP library is - learn! LWP is a great collection of Perl
  objects that allows you to fetch documents from the web. What the
  CGI library does for people writing web server code, LWP does for
  people writing web client code. You can find out more about LWP
  at http://www.sn.no/libwww-perl/, and you can download LWP from
  the CPAN at <a href=
  "http://www.perl.com/CPAN/modules/by-module/LWP/" target=
  "resource window">http://www.perl.com/CPAN/modules/by-module/LWP/</a>.</p>

  <p><i>webpluck</i> is a simple program. Most of the code takes
  care of processing command line arguments, reading the
  configuration file, and checking for errors. The guts rely on the
  LWP library and Perl's powerful regular expressions. The
  following is part of the main loop in <i>webpluck</i>. I've
  removed some error checking to make it smaller, but the real guts
  are shown below.</p>
  <pre>
use LWP; 
&#13;$req = HTTP::Request-&gt;new( GET =&gt; $self-&gt;{'url'} ); 
$req-&gt;header( Accept =&gt; "text/html, */*;q=0.1" ); 
$res = $main::ua-&gt;request( $req ); 
&#13;if ($res-&gt;is_success()) { 
   my (@fields)  = split( ':', $self-&gt;{'fields'} ); 
   my $content   = $res-&gt;content(); 
   my $regex     = $self-&gt;{'regex'}; 
&#13;   while ($content =~ /$regex/isg) { 
     my @values = ($1,$2,$3,$4,$5,$6,$7,$8); 
&#13;     # URL's are special fields; they might be 
     # relative, so we check for that
&#13;     for ($i = 0; $i &lt;= $#fields; $i++) { 
&#13;       if ($fields[$i] eq "url") { 
          my $urlobj = new URI::URL($values[$i], $self-&gt;{'url'})
          $values[$i] = $urlobj-&gt;abs()-&gt;as_string();
       } 
       push(@datalist, $fields[$i] . ":" . $values[$i]); 
     } 
     push( @{$self-&gt;{'_data'}}, \@datalist ); 
  } 
} 
</pre>

  <p>The <tt>use LWP</tt> imports the LWP module, which takes care
  of all the web related tasks (fetching documents, parsing URLs,
  and parsing robot rules). The next three lines are all it takes
  to grab a web page using LWP.</p>

  <p>Assuming <i>webpluck</i>'s attempt to retrieve the page is
  successful, it saves the document as one long string. It then
  iterates over the string, trying to match the regular expression
  defined for this target. The following statement merits some
  scrutiny:</p>
  <pre>
while ( $content =~ /$regex/isg ) { 
</pre>

  <p>The <tt>/i</tt> modifier of the above regular expression
  indicates that it should be a case-insensitive match. The
  <tt>/s</tt> modifier treats the entire document as if it were a
  single line (treating newlines as whitespace), so your regular
  expression can span multiple lines. <tt>/g</tt> allows you to go
  through the entire document and grab data each time the regular
  expression is matched, instead of just the first.</p>

  <p>For each match <i>webpluck</i> finds, it examines the fields
  defined by the user. If one of the fields is <tt>url</tt> (the
  only field that isn't arbitrary), it's turned into an absolute
  URL - specifically, a URI::URL object. I let that object
  translate itself from a relative URL to an absolute URL that can
  be used outside of the web site from where it was retrieved. This
  is the only data from the target page that gets massaged.</p>

  <p>Lastly I take the field names and the data that corresponds to
  each field and save that information. Once all the data from each
  matched regular expression is collected, it's run through some
  additional error checking and saved to a local file.</p>

  <h3>The Dark Side Of The Force</h3>

  <p>Like any tool, <i>webpluck</i> has both good and bad uses. The
  program is a sort of web robot, which raises some concerns for
  myself (as the author) and for users. A detailed list of these
  considerations can be found on The Web Robots Page located at:
  <a href=
  "http://info.webcrawler.com/mak/projects/robots/robots.html"
  target=
  "resource window">http://info.webcrawler.com/mak/projects/robots/robots.html</a>.
  A few points listed in the Web Robot Guide to Etiquette stand
  out:</p>

  <p><b>Identify yourself.</b> <i>webpluck</i> does identify itself
  as "webpluck/2.0" to the remote web server. This isn't a problem
  now since few people use <i>webpluck</i> at the moment, but it
  could be if sites decide to block my program.</p>

  <p><b>Don't overload a site.</b> Since <i>webpluck</i> only
  checks a finite set of web pages that you explicitly define (it
  doesn't tree-walk sites), this isn't a problem. Just to be safe,
  <i>webpluck</i> pauses for a small time period between retrieving
  documents. It should only be run once or twice a day - don't
  launch it every five minutes to ensure that you constantly have
  the latest and greatest information. If instant notification is
  critical to you, consider push technologies like Pointcast or
  Marimba's Castanet.</p>

  <p><b>Obey robot exclusion rules found in /robots.txt.</b> This
  is the toughest rule to follow. Since <i>webpluck</i> is
  technically a robot, I should be following the rules set forth by
  the /robots.txt file. However, since the data that I am after
  typically changes every day, some sites have set up specific
  rules telling robots not to index their pages.</p>

  <p>In my opinion, <i>webpluck</i> isn't your typical robot. I
  consider it more like your average web client. I'm not building
  an index, which I think is the reason that these sites tell
  robots not to retrieve the pages. If <i>webpluck</i> followed the
  letter of the law, it wouldn't be very useful since it wouldn't
  be able to access many pages that change their content. For
  example, CNN has this in their robot rules file:</p>
  <pre>
     User-agent: * 
     Disallow: / 
</pre>

  <p>If <i>webpluck</i> were law-abiding, it wouldn't be able to
  retrieve any information from CNN, one of the main sites I check
  for news. So what to do? After reading the Robot Exclusion
  Standard (<a href=
  "http://info.webcrawler.com/mak/projects/robots/norobots.html"
  target=
  "resource window">http://info.webcrawler.com/mak/projects/robots/norobots.html</a>),<br>

  I believe <i>webpluck</i> doesn't cause any of the problems meant
  to be prevented by the standard. Your interpretation may differ;
  I encourage you to read it and decide for yourself.
  <i>webpluck</i> has two options (--naughty and --nice) that
  instruct it whether to obey the robot exclusion rules found on
  remote servers. This is my way of deferring the decision to
  you.</p>

  <p>Just playing nice as a web robot is only part of the equation.
  Another consideration is what you do with the data once you get
  it. There are obvious copyright considerations. Copyright on the
  web is a broad issue; I'm just going to mention a few quandaries
  raised by <i>webpluck</i>. I don't have the answers.</p>

  <p>Is it okay to extract the URL from the Cool Site of the Day
  home page and jump straight to the "Cool Site?" The Cool Site
  folks don't own the URL, but they would certainly prefer that you
  visit their site first.</p>

  <p>Is it okay to retrieve headlines from CNN? What about URLs for
  the articles?</p>

  <p>How about grabbing the actual articles from the CNN site and
  redisplaying them with your own layout?</p>

  <p>And for all of these tasks, does it matter if they're for your
  own personal use as opposed to showing it to a friend, or
  redistributing it more widely?</p>

  <p>One newspaper in the Shetland Islands has sued a rival
  newspaper for setting up links to their web site. The May 1997
  issue of WIRED lists another site being sued, called TotalNews
  (<a href="http://www.totalnews.com/" target=
  "resource window">http://www.totalnews.com/</a>). Some of
  TotalNews' web pages point to news stories written by other news
  providers - but TotalNews sells its own ads in a frame around
  them.</p>

  <p>Obviously, people have different opinions of what is right and
  what is wrong. I personally don't have the background, knowledge,
  or desire to try to tell you what to do. I merely want to raise
  the issues so you can think about them and make your own
  decisions.</p>

  <p>For a final example of a potential problem, let's take a look
  at Dilbert. Here's the target I have defined for Dilbert at the
  time of this writing.</p>
  <pre>
name     dilbert 
url      http://www.unitedmedia.com/comics/dilbert/ 
regex    SRC=\"?([^&gt;]?\/comics\/dilbert\/archive.*?\.gif)\"?\s+
fields   url 
</pre>

  <p>The cartoon on the Dilbert page changes every day, and instead
  of just having a link to the latest cartoon (todays-dilbert.gif),
  they generate a new URL everyday and include the cartoon in their
  web page. They do this because they don't want people setting up
  links directly to the cartoon. They want people to read their
  main page - after all, that's where the advertising is. Every
  morning I find out where today's Dilbert cartoon is located,
  bypassing all of United Media's advertising. If enough people do
  this, United Media will probably initiate countermeasures. There
  are at least three things that would prevent webpluck (as it
  currently works) from allowing me to go directly to today's
  comic.</p>

  <ul>
    <li>They could write a CGI program that you have to access to
    get at the comic strip. This CGI program can take all kinds of
    steps to try and see if you should have access to the image
    (e.g. checking <tt>Referer</tt> headers, or planting a cookie
    on you). But I think that any such countermeasure can be worked
    around with a clever enough <i>webpluck</i>.</li>

    <li>They can embed the advertising in the same image as the
    cartoon. That'll work for Dilbert, but not for other pages
    where the content is plain HTML.</li>

    <li>They can move away from HTML to some other display format
    such as Coda (<a href="http://www.randomnoise.com/" target=
    "resource window">http://www.randomnoise.com/</a>). Coda takes
    over an entire web page with a single view in which Java-based
    UI elements and text are displayed inside. This approach makes
    the information supplied by such a page totally useless to any
    web robot, and for that matter anyone without a Java-based
    browser. People seem to like Coda anyway.</li>
  </ul>

  <p>Most funding for web technology exists to solve the needs of
  content providers, not users. If tools like <i>webpluck</i> are
  considered a serious problem by content providers, steps will be
  taken to shut them down, or make them less useful.</p>

  <p>It isn't my intent to distribute a tool to filter web
  advertising or steal information from web pages so that I can
  redistribute it myself, but I'm not so na&iuml;ve to think that
  this can't be done. Obviously, anyone intent on doing these
  things can do so; <i>webpluck</i> just makes it easier. Do what
  you think is right.</p>

  <p>You can find more information about webpluck at <a href=
  "http://strobe.weeg.uiowa.edu/~edhill/public/webpluck/" target=
  "resource window">http://strobe.weeg.uiowa.edu/~edhill/public/webpluck/</a>.
  The program is also on the TPJ web site.</p>

  <p>_ _END_ _</p>
  <hr>
  <i>Ed Hill (ed-hill@uiowa.edu) is a Unix Systems Administrator at
  the University of Iowa</i> <!-- end of article -->
   <!-- end of file -->
</body>
</html>
