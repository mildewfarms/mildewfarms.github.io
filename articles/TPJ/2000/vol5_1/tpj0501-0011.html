<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Perl and Morphology - The Perl Journal, Spring 2000</title><!-- end head -->
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org">
  <meta name="vscategory" content="Perl">
  <meta name="vsisbn" content="">
  <meta name="vstitle" content="Perl and Morphology">
  <meta name="vsauthor" content=
  "Khurshid Ahmad and Duncan C. White">
  <meta name="searchdescription" content=
  "Development of Natural language processing which means how computer programs understand or generate the written natural language.">
  <meta name="vsimprint" content="The Perl Journal">
  <meta name="vspublisher" content="Earthweb">
  <meta name="vspubdate" content="Spring 2000">
  <!-- always update the article title and issue -->

  <!-- link to the previous and next documents relative to the current document -->
</head>

<body>
  <font face="verdana" size="1">Issue 17, Spring 2000</font>

  <h2 align="center">Perl and Morphology</h2>

  <h4 align="left"><i>Khurshid Ahmad and Duncan C. White</i></h4>
  <!-- packages described, if necessary -->

  <div align="center">
    <table width="350" border="1" cellspacing="0" cellpadding="5">
      <tr>
        <td bgcolor="#CCCC99" align="middle">
        <b>&nbsp;&nbsp;Packages Used:&nbsp;&nbsp;</b></td>
      </tr>

      <tr>
        <td>Perl 5.005.....................................CPAN<br>
        BritishNational Corpus......http:info.ox.ac.uk/bnc/</td>
      </tr>
    </table>
  </div>

  <p><b>Introduction</b></p>

  <p>Natural Language Processing (<i>NLP</i>) is a branch of
  Artificial Intelligence involving the development of computer
  programs to understand or generate documents written in "natural
  language" &mdash; that is, any human language, like English,
  Hebrew, Swahili, or Xhosa. Creating programs that exhibit full
  understanding of natural language has long been a goal of AI.</p>

  <p>Here are some typical NLP applications:</p>

  <p>&bull; Word assistance to users. For example, a human might
  ask: "What is the adverb form of 'accident'?, and the computer
  might reply: "'accidentally' is probably the word you want,
  although 3% of users spell it 'accidently'".</p>

  <p>&bull; A smarter web search engine, or a <i>knowbot</i>
  (knowledge robot), that lets you search for a keyword such as
  <tt>compute</tt> to retrieve all documents that contain that
  keyword, or conceptually related keywords like
  <i>computationally</i>.</p>

  <p>&bull; A smart document categorizer that reads a series of
  documents and sorts them into different categories based upon
  disproportionate use of particular keywords ("this document
  appears to be about nuclear physics, because it mentions keywords
  like <tt>atom</tt> and <tt>nucleus</tt> far more than average
  documents").</p>

  <p>&bull; A smart document summarizer that summarizes one or more
  documents into a more compact form and presents a digest (see
  <i>CS-Web: A Lightweight Summarizer for HTML</i>, in TPJ
  #16).</p>

  <p>All of these programs require some understanding of natural
  language. For perfect summarization or perfect translation, we'd
  need <i>total</i> understanding, and that's far too difficult
  because of the intricacies of human communication.</p>

  <p>Fortunately, we don't need complete language understanding to
  obtain some utility. Even a little knowledge of language is
  useful for some applications, and in this spirit, let's start at
  the lower levels and pick one small part of natural language
  which we can process relatively easily &mdash; the structure of
  <i>single words</i>.</p>

  <p><b>Morphology: Word Form And Structure</b></p>

  <p>Natural languages like English often express shades of meaning
  by changing the form of a single word, such as by adding a prefix
  or a suffix to an existing word. Adding <tt>s</tt> to
  <tt>computer</tt> changes the meaning from "a single computer" to
  "multiple computers": a noun-to-noun change. We can also move
  between nouns, verbs, and adjectives with similarly simple
  changes (<tt>move</tt> becomes <tt>movement</tt>). The study of
  the <i>form</i> of words is called <i>morphology</i>.</p>

  <p>This article shows how to implement a program that uses
  morphological rules to generate many related words from one given
  word. This will be useful in search tools, because when the user
  searches for <tt>computer</tt>, the search engine can now find
  documents with related words like <tt>compute</tt>,
  <tt>computing</tt>, and <tt>computationally</tt>.</p>

  <p>So let's look at four common morphological processes and how
  we might model them in Perl. The first is called
  <i>inflection</i>, in which a word is transformed to indicate the
  tense (for example, <i>look</i> becomes <i>looks</i>,
  <i>looking</i>, and <i>looked</i>) or the number (for example,
  <i>husky</i> becomes <i>huskies</i>).</p>

  <p>As soon as we start thinking about such transformations from a
  Perl perspective, we can't help but think of regular expressions:
  if you have the verb <i>look</i> and you want to generate the
  present participle (<i>looking</i>), this regular expression
  springs to mind:</p>
  <pre>
s/$/ing/  # add "ing" to $_
</pre>

  <p>In this spirit, Table 1a shows some typical inflections that
  pluralizes nouns, and Table 1b shows some tense inflections on
  verbs.</p>

  <p>Unfortunately, English has many irregularities, ranging from
  whether or not to suppress a trailing <tt>e</tt>, up to the
  impressive past tense of <tt>go</tt>: <tt>went</tt>, with no
  letters in common!</p>

  <p>Sometimes two different rules can apply to a word, but only
  one of the inflections is actually correct. Is the plural form of
  <tt>omnibus</tt> really <tt>omnibi</tt> according to the
  <tt>s/us$/i/</tt> rule? Or is it actually <tt>omnibuses</tt>
  because of <tt>s/us$/uses/</tt>? In some cases (<tt>geniuses</tt>
  and <tt>genii</tt>, <tt>syllabuses</tt> and <tt>syllabi</tt>)
  both inflections are permitted. How can a program know what to
  do?</p>

  <p>We'll downgrade our rules to <i>rules of thumb</i> &mdash; or,
  as AI researchers call them, <i>heuristics</i>. We'll accept that
  they will sometimes be wrong, and later we'll develop a way to
  filter out incorrect applications.</p>

  <table width="600">
    <tr>
      <td colspan="4"><b>Table 1a: Some rules for pluralizing
      English nouns.</b><br></td>
    </tr>

    <tr>
      <td width="25%">regular</td>

      <td width="25%">dog, cat, computer</td>

      <td width="25%">dogs, cats, computers</td>

      <td width="25%"><tt>s/$/s/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in s, x, z, sh, ch</td>

      <td width="25%">gas, tax, wish</td>

      <td width="25%">gases, taxes, wishes</td>

      <td width="25%"><tt>s/$/es/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in f</td>

      <td width="25%">calf, elf, dwarf</td>

      <td width="25%">calves, elves, dwarves</td>

      <td width="25%"><tt>s/f$/ves/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in o</td>

      <td width="25%">bamboo, folio<br>
      domino, torpedo</td>

      <td width="25%">bamboos, folios<br>
      dominoes, torpedos</td>

      <td width="25%"><tt>s/o$/os/<br>
      s/o$/oes/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in y</td>

      <td width="25%">sky, husky<br>
      decoy, donkey</td>

      <td width="25%">skies, huskies<br>
      decoys, donkeys</td>

      <td width="25%"><tt>s/y$/ies/<br>
      s/y$/ys/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in sis</td>

      <td width="25%">crisis, analysis</td>

      <td width="25%">crises, analyses</td>

      <td width="25%"><tt>s/sis$/ses/</tt></td>
    </tr>

    <tr>
      <td width="25%">ending in us</td>

      <td width="25%">radius, genius<br>
      omnibus, genius</td>

      <td width="25%">radii, genii<br>
      omnibuses, geniuses</td>

      <td width="25%"><tt>s/us$/i/<br>
      s/us$/uses</tt></td>
    </tr>

    <tr>
      <td width="25%">irregular</td>

      <td width="25%">man</td>

      <td width="25%">men</td>

      <td width="25%"><tt>s/an$/en/</tt></td>
    </tr>
  </table><br>

  <table width="600">
    <tr>
      <td colspan="8"><b>Table 1b: Some rules for inflecting
      English verbs</b><br></td>
    </tr>

    <tr>
      <td valign="top">regular</td>

      <td>look<br>
      walk<br>
      decide<br>
      watch</td>

      <td>looks<br>
      walks<br>
      decides<br>
      watches</td>

      <td>looking<br>
      walking<br>
      deciding<br>
      watching</td>

      <td>looked<br>
      walked<br>
      decided<br>
      watched</td>

      <td><tt>s/$/s/<br>
      s/$/s/<br>
      s/$/s/<br>
      s/$/es/</tt></td>

      <td><tt>s/$/ing/<br>
      s/$/ing/<br>
      s/e$/ing/<br>
      s/$/ing/</tt></td>

      <td><tt>s/$/ed/<br>
      s/$/ed/<br>
      s/e$/ed/<br>
      s/$/ed/</tt></td>
    </tr>

    <tr>
      <td valign="top">irregular</td>

      <td>see<br>
      flee<br>
      come<br>
      go<br>
      take</td>

      <td>sees<br>
      flees<br>
      comes<br>
      goes<br>
      takes</td>

      <td>seeing<br>
      fleeing<br>
      coming<br>
      going<br>
      taking</td>

      <td>saw<br>
      fled<br>
      came<br>
      went<br>
      took</td>

      <td><tt>s/$/s/<br>
      s/$/s/<br>
      s/$/s/<br>
      s/$/es/<br>
      s/$/s/</tt></td>

      <td><tt>s/$/ing/<br>
      s/$/ing/<br>
      s/$/ing/<br>
      s/$/ing/<br>
      s/e$/ing/</tt></td>

      <td><tt>s/ee$/aw/<br>
      s/ee$/ed/<br>
      s/ome$/ame/<br>
      s/go$/went/<br>
      s/take$/took/</tt></td>
    </tr>
  </table>

  <p>The second morphological process is called <i>derivation</i>,
  and involves taking a base word (the <i>stem</i>) and
  transforming it from one part of speech into another (for
  instance, from the verb <tt>ignite</tt> to the noun
  <tt>ignition</tt>). Table 2 shows some typical derivations.</p>

  <p><b>Table 2: Examples of derivational morphology</b></p>

  <table>
    <tr>
      <td width="30%"><u>NOUN</u></td>

      <td width="30%"><u>DERIVATION</u></td>

      <td width="30%"><u>CLASS</u></td>
    </tr>

    <tr>
      <td width="30%">nation</td>

      <td width="30%">national</td>

      <td width="30%">adjective</td>
    </tr>

    <tr>
      <td width="30%">nation</td>

      <td width="30%">nationalize</td>

      <td width="30%">verb</td>
    </tr>

    <tr>
      <td colspan="3">&nbsp;</td>
    </tr>

    <tr>
      <td width="30%"><u>VERB</u></td>

      <td width="30%"><u>DERIVATION</u></td>

      <td width="30%"><u>CLASS</u></td>
    </tr>

    <tr>
      <td width="30%">slow</td>

      <td width="30%">slowly</td>

      <td width="30%">adverb</td>
    </tr>

    <tr>
      <td width="30%">observe</td>

      <td width="30%">observation</td>

      <td width="30%">noun</td>
    </tr>

    <tr>
      <td width="30%">observe</td>

      <td width="30%">observable</td>

      <td width="30%">adjective</td>
    </tr>

    <tr>
      <td width="30%">nationalize</td>

      <td width="30%">nationalization</td>

      <td width="30%">noun</td>
    </tr>

    <tr>
      <td width="30%">develop</td>

      <td width="30%">development</td>

      <td width="30%">noun</td>
    </tr>

    <tr>
      <td colspan="3">&nbsp;</td>
    </tr>

    <tr>
      <td width="30%"><u>ADJECTIVE</u></td>

      <td width="30%"><u>DERIVATION</u></td>

      <td width="30%"><u>CLASS</u></td>
    </tr>

    <tr>
      <td width="30%">national</td>

      <td width="30%">nationally</td>

      <td width="30%">adverb</td>
    </tr>

    <tr>
      <td width="30%">thick</td>

      <td width="30%">thickness</td>

      <td width="30%">noun</td>
    </tr>
  </table>

  <p><i>Nominalization</i> is an especially important type of
  derivation &mdash; deriving a noun from a verb or adjective.
  Let's look at one example of derivation in detail &mdash; the
  verb <i>observe</i>. As well as the usual inflectional tenses
  (<i>observing</i>, etc.), we derive the adjective
  <i>observable</i> and, by nominalization, the noun
  <i>observation</i>, the plural inflection <i>observations</i>,
  another adjective <i>observational</i>, and an adverb
  <i>observationally</i>.</p>

  <p>There are two other major morphological processes in which
  words are formed in English: <i>compounding</i> (<tt>ear</tt> +
  <tt>ache</tt> = <tt>earache</tt>, <tt>work</tt> + <tt>load</tt> =
  <tt>workload</tt>) and affixation (<tt>dis</tt> +
  <tt>arrange</tt> = <tt>disarrange</tt>). Compounding combines two
  words; affixation combines a word with a prefix or suffix. These
  actions can also be expressed in regular expressions
  (<tt>s/^/ear/</tt>), but for the purposes of this article we'll
  ignore compounding and affixation.</p>

  <p>To summarize, we can transform words by adding certain
  prefixes or suffixes to a stem, or by joining two stems together.
  Salton (1989) estimated that there are about 75 prefixes and
  around 250 suffixes in English. Most importantly for us, Perl
  regular expressions are perfectly suited to performing all these
  transformations.</p>

  <p>Now let's build a set of derivation and inflection rules and
  then use these rules to analyze a <i>corpus</i>, a collection of
  carefully selected documents. Several large corpuses,
  representative of different types of documents, have been built
  &mdash; a typical representative corpus of everyday written
  language is the British National Corpus. Details about the BNC
  research project can be found on the web at <a href=
  "http://info.ox.ac.uk/bnc/" target=
  "resource window">http://info.ox.ac.uk/bnc/</a>. It is a
  collection of over 4,000 different documents with around 100
  million words in over 6 million sentences. The number of distinct
  words &mdash; the <i>vocabulary</i> &mdash; is a little under
  400,000. We'll use it for the examples below.</p>

  <p><b>Morphological Analysis And Perl</b></p>

  <p>We now present a series of Perl programs performing simple
  morphological inflections and derivations on single words. Our
  starting point is a series of documents, forming our corpus.
  Since we're working only with single words, we don't need to
  retain the original word order in the documents. All we need is a
  list of the words that appear in the documents, and how many
  times each word appears.</p>

  <p>Our first step, therefore, is to transform the documents into
  a <tt>word frequency</tt> list which tells us how many distinct
  word forms there are, and how many times each form occurs. The
  rest of the programs can then operate on the word frequency
  lists, which are much smaller and more easily managed than the
  documents themselves.</p>

  <p><b>Constructing A Word Frequency List</b></p>

  <p>Our first program, <tt>mkfreq</tt>, creates a word frequency
  list from a set of documents. The only hard part is defining what
  constitutes a word. One definition might be a series of
  alphanumeric characters: that is, anything not matching
  <tt>/[\W_]+/</tt>.</p>

  <p>This regular expression isn't perfect: it splits abbreviations
  like <i>don't</i> into two words. Let's live with that, and
  instead of calling the things identified by our regular
  expression words, we'll call them <i>tokens</i>. We'll routinely
  lowercase all tokens, since "work" and "Work" shouldn't be
  treated as different words.</p>

  <p>Listing 1 shows <tt>mkfreq</tt>. We use a hash to store the
  word frequencies, and display two simple but useful measures of
  the linguistic diversity in the documents &mdash; the total
  number of tokens and the vocabulary. The frequency list is
  displayed in descending order of frequency using a custom sort
  order. For cryptographic applications, a knowledge of the most
  frequent words in the English language is useful. Looking at the
  most frequent ten words in the British National Corpus, we
  see:</p>

  <table>
    <tr>
      <td width="50%"><u>Word</u></td>

      <td width="50%"><u>Frequency</u></td>
    </tr>

    <tr>
      <td width="50%"><tt>the</tt></td>

      <td width="50%">6,198,660</td>
    </tr>

    <tr>
      <td width="50%"><tt>of</tt></td>

      <td width="50%">3,112,676</td>
    </tr>

    <tr>
      <td width="50%"><tt>and</tt></td>

      <td width="50%">2,695,784</td>
    </tr>

    <tr>
      <td width="50%"><tt>to</tt></td>

      <td width="50%">2,672,790</td>
    </tr>

    <tr>
      <td width="50%"><tt>a</tt></td>

      <td width="50%">2,241,128</td>
    </tr>

    <tr>
      <td width="50%"><tt>in</tt></td>

      <td width="50%">1,996,243</td>
    </tr>

    <tr>
      <td width="50%"><tt>that</tt></td>

      <td width="50%">1,150,942</td>
    </tr>

    <tr>
      <td width="50%"><tt>it</tt></td>

      <td width="50%">1,091,584</td>
    </tr>

    <tr>
      <td width="50%"><tt>is</tt></td>

      <td width="50%">1,004,574</td>
    </tr>

    <tr>
      <td width="50%">Total</td>

      <td width="50%">22,164,381</td>
    </tr>
  </table>

  <p>Some observations: These ten words are responsible for 21.6%
  of the corpus. They're all common determiners, conjunctions,
  pronouns, and prepositions, carrying no information about the
  subject of a document. Most search engines exclude these
  uninformative words from their indexes. Excluded words are called
  <i>stopwords</i>.</p>

  <p>The frequency list can also help us out by identifying
  misspelled words; we'd expect to find uncommon spelling mistakes
  at the bottom of our frequency list. We might decide to exclude
  all tokens with a frequency below some threshold (say, one or two
  occurrences) in order to exclude many spelling mistakes (and,
  inevitably, some legitimate words) from our processing. In the
  British National Corpus discussed above, 154883 tokens occur
  exactly once and 49465 tokens occur exactly twice. Together,
  these comprise 204348 tokens out of the total vocabulary of
  381807 tokens &mdash; 53% percent of the vocabulary occurs less
  than three times!</p>

  <p>Excluding low-frequency tokens is a simple addition to
  <tt>mkfreq</tt>. We omit that here, but it's in the version of
  <tt>mkfreq</tt> on the TPJ website. We also omit tokens with
  digits.</p>

  <p><b>Morphological Inflections And Derivations</b></p>

  <p>Some derivation rules apply only to particular word categories
  &mdash; you wouldn't try to find the plural of a verb. However,
  there's no algorithmic way of identifying whether an English word
  is a verb or noun. No rules or even heuristics are possible.</p>

  <p>Ideally, we'd have some way to take a word and find out which
  category it belonged to. However, this would require a knowledge
  base containing every base word from the corpus. Constructing and
  maintaining such a knowledge base would be a major
  undertaking.</p>

  <p>So, let's turn the problem on its head: how far can we get
  with <tt>no knowledge base whatever?</tt> Could we, in fact, make
  reasonably accurate derivations based only upon our corpus? If we
  have no knowledge of word categories, then we must start by
  applying all derivation rules to all tokens in the corpus, and
  then see if there's any way of eliminating the duds.</p>

  <p><b>Representing A Single Rule</b></p>

  <p>A rule is basically an <tt>s///</tt> regular expression.
  However, we need to store a collection of rules in an array and
  apply them all to a token. Having decided to ignore affixation
  and compounding, if we look back to Tables 1a, 1b, and 2, we
  notice that all our rules for inflection and derivation are of
  the form <tt>s/ending$/suffix/</tt>.</p>

  <p>This suggests that we can represent each rule as an
  ending-suffix pair with the meaning:</p>

  <div>
    If <i>token</i> ends with <i>ending</i>, replace the ending
    part of <i>token</i> with <i>suffix</i>.
  </div>

  <p>Given this, we could write a function that attempts to apply a
  rule pair (<i>ending</i>, <i>suffix</i>) to a given <i>token</i>,
  returning either the derived token or the empty string (if the
  token does not end in <i>ending</i>). This subroutine,
  <tt>derive()</tt>, is shown below.</p>
  <pre>
sub derive {                # Make a single derivation
    my ( $t, $r, $a ) = @_;
    if ( $r ) {
        return "" unless $t =~ /.$r$/;
        $t =~ s/$r$//;
    }
    return "$t$a";
}
</pre>

  <p>We embedded this into a test harness that we named
  <tt>applyruletotoken</tt>, which, at its core, is simply
  this:</p>
  <pre>
        my $d = derive( @ARGV ); 
        print "Derivation is '$d'\n" if $d;
        print "No derivation\n" unless $d;
</pre>

  <p>We can now perform a few simple tests:</p>

  <table>
    <tr>
      <td width="50%">Command</td>

      <td width="50%">Response</td>
    </tr>

    <tr>
      <td width="50%"><tt>applyruletotoken look '' ing</tt></td>

      <td width="50%">Derivation is <tt>looking</tt></td>
    </tr>

    <tr>
      <td width="50%"><tt>applyruletotoken look 's' sing</tt></td>

      <td width="50%">No Derivation</td>
    </tr>

    <tr>
      <td width="50%"><tt>applyruletotoken take 'e' ing</tt></td>

      <td width="50%">Derivation is <tt>taking</tt></td>
    </tr>
  </table>

  <p><b>Representing Many Rules</b></p>

  <p>The next step is to figure out how to represent all the
  derivation rules so that we can apply all of them to each token
  and determine which succeed. For convenience we'll collapse our
  rule pair (<i>ending, suffix</i>) into a single string:
  <tt>-ending+suffix</tt>, or just <tt>+suffix</tt> if
  <i>ending</i> was empty. Figure 3 shows a function
  <tt>setuprules</tt> which creates an array of rules
  <tt>@rule</tt> and then processes it into an array of
  removal-strings <tt>@remove</tt> and an array of addition-strings
  <tt>@add</tt>. Only a few rules are shown, although we routinely
  use many more.</p>
  <pre>
# Set up the derivation rules and processed forms.
sub setupRules {    
    my ( $r, $a );
    @rule =      # A rule is a disguised pair of the form -R+A or +A
    (
      "+s", "+es", "-us+i", "-is+es",      # singular -&gt; plural noun
      "+ion", "+tion", "+sion", "-te+tion", # verb -&gt; noun
      "-ify+ification", "+ment",
      "+ness", "+fulness", "+ful", "+ic", "+al",  # noun -&gt; adjective
      "-e+ing", "-s+ssing", "+ing", "+ed",     # verb -&gt; tense forms
      "+ly",                           # adjective -&gt; adverb
    );
    @add = @remove = ();
    foreach (@rule) {
        ( $r, $a ) = ( $1, $2 ) if /^-(\w+)\+(\w+)$/;
        ( $r, $a ) = ( "", $1 ) if /^\+(\w+)$/;
        push( @remove, $r );
        push( @add, $a );
    }
}
</pre>

  <p>We had to make a few small changes to <tt>derive()</tt>, and
  our new version is shown below. Whenever it finds a valid
  derivation, it calls <tt>handlederivation()</tt>, providing the
  token, the rule, and the derived token.</p>
  <pre>
# Our updated derive, which now calls handlederivation()
sub derive {  
    my ( $t ) = @_;
    my ( $i, $a, $r, $d );

    for ( $i = 0; $i &lt; @add; $i++ ) {
        $a = $add[$i];
        $r = $remove[$i];
        $d = $t;
        if ( $r ) {
            next unless $t =~ /.$r$/;
            $d =~ s/$r$//;
        }
        $d = "$d$a";
        handlederivation( $t, $rule[$i], $d );
    }
}

sub handlederivation {
    my ( $token, $rule, $derivation ) = @_;
    print "$token\t$rule\t$derivation\n";
}
</pre>

  <p>We embedded these subroutines into a test harness called
  <tt>applyallrules</tt>, whose heart is simply this:</p>
  <pre>
 setuprules();
 derive( @ARGV );
</pre>

  <p>A test such as <tt>applyallrules look</tt> will generate the
  following output:</p>
  <pre>
        look    +s              looks
        look    +es             lookes
        look    +ion            lookion
        look    +tion           looktion
        look    +sion           looksion
        look    +ment           lookment
        look    +ness           lookness
        look    +fulness        lookfulness
        look    +ful            lookful
        look    +ic             lookic
        look    +al             lookal
        look    +ing            looking
        look    +ed             looked
        look    +ly             lookly
</pre>

  <p>As we would expect, this generates some good derivations
  (<i>looks, looking, looked</i>) but the majority of the
  derivations are not real English words.</p>

  <p><b>Telling Good From Bad</b></p>

  <p>We need some way to distinguish the real English words from
  the bogus, and this is point at which an English word knowledge
  base could be used. We could use an external wordlist such as the
  Unix dictionary <tt>/usr/dict/words</tt>, rejecting any
  derivation that doesn't appear, but many uncommon words (and even
  some common ones) aren't included. Augmenting a wordlist with a
  large collection of specialized words is another major
  endeavor.</p>

  <p><b>The Key Insight!</b></p>

  <p>Fortunately, we have another wordlist available that is a
  source of exactly the specialized words that we need &mdash; the
  frequency list derived from the corpus! Using this insight, we
  propose the following method:</p>

  <p>&bull; Take an existing token in the frequency list.</p>

  <p>&bull; Apply all derivation rules to that token as above.</p>

  <p>&bull; Reject any derivation that is not itself present in the
  frequency list.</p>

  <p>Implementing It</p>

  <p>Our new system, <tt>filterallrules</tt>, is just
  <tt>applyallrules</tt> with two changes: the
  <tt>readfreqfile()</tt> subroutine shown below reads a frequency
  list into <tt>%freq</tt>, and <tt>derive()</tt> calls
  <tt>handlederivation()</tt> only if the word is in the frequency
  list: <tt>handlederivation( $t, $rule[$i], $d ) if
  $freq{$d};</tt>.</p>
  <pre>
sub readfreqfile {          # Read the frequency list
    my ( $filename ) = @_;
    my ( $line, $word, $count );

    open ( IN, $filename ) || die "can't open $filename\n";
    %freq = ();
    while ( $line = &lt;IN&gt; ) {
        last unless $line =~ m/^=======/;
        chomp $line;
        $totaltokens = $1 if $line =~ m/= Total tokens:\s+(\d+)/;
        $vocab = $1 if $line =~ m/= Vocabulary:\s+(\d+)/;
    }
    while ( $line = &lt;IN&gt; ) {
        chomp $line;
        next if $line =~ /^\d/;
        ( $word, $count ) = split( /\s+/, $line );
        $freq{$word} = $count;
    }
    close IN;
}
</pre>

  <p>Applying this to <tt>look</tt> with a frequency file derived
  from the British National Corpus now correctly sifts out the true
  English words from the duds:</p>
  <pre>
        look    +s              looks
        look    +ing    looking
        look    +ed             looked
</pre>

  <p>These inflections are rather straightforward and obvious.
  Consider a more complex example &mdash; the derivations of
  <tt>approximate</tt>:</p>
  <pre>
        approximate             +s              approximates
        approximate             -te+tion        approximation
        approximate             -e+ing          approximating
        approximate             +ly             approximately
</pre>

  <p>This technique is not perfect. There are two main types of
  false derivations produced: first, when two completely unrelated
  English words happen to match a derivation rule (for example, if
  the <tt>+s</tt> rule is applied to <tt>asses</tt>, we get
  <tt>assess</tt>). There are surprisingly few cases of this type,
  however, because English has strong morphological
  constraints.</p>

  <p>The second and more common type of false derivation arises
  from specialized terms, proper names, foreign words, and spelling
  mistakes in our corpus. For example, the electrical term
  <tt>AC</tt> is lowercased to <tt>ac</tt> and then is used to
  generate seemingly related words such as <tt>aced</tt> and
  <tt>action</tt>. This is inevitable without deeper knowledge
  about the words we're processing.</p>

  <p><b>Applying The Derivation Process To All Tokens</b></p>

  <p>The final step is to apply this derivation process not just to
  one token, but to all tokens within the corpus, and to display
  the results in a pleasing format. To apply the derivation rules
  to all tokens, we define a new subroutine called
  <tt>deriveall()</tt> that iterates over all tokens:</p>
  <pre>
# Call derive() for every token in the frequency table
sub deriveAll {   
    my ( $t );
    %b2d = ();
    foreach $t (sort(keys(%freq))) {
        derive( $t );
    }
}
</pre>

  <p>To tabulate the results, we redefine
  <tt>handlederivation()</tt> as shown below. The successive calls
  to <tt>handlederivation()</tt> now build a hash called
  <tt>%b2d</tt> (base-to-derivations) that maps each base token to
  an array of all of its derivations. We use references so that our
  hash can have many values for each key.</p>
  <pre>
sub handlederivation {
    my ( $token, $rule, $derivation ) = @_;
    my ( $ref );
    $ref = $b2d{$token};
    $ref = $b2d{$token} = [ ] unless defined $ref;
    push( @$ref, $derivation );
}
</pre>

  <p>Once <tt>deriveall()</tt> completes, the entire <tt>%b2d</tt>
  hash has been constructed. To display all the derivations, we
  define a subroutine named <tt>tabulate</tt>, which displays each
  word and its frequency:</p>
  <pre>
# tabulate the results in the %b2d array.
sub tabulate {
    my ( @b, $base, $list, $t );
    # @b contains all the base tokens (the keys of %b2d)
    @b = sort(keys(%b2d));

    foreach $base (@b) {
        # $list is a reference to an array of derivations
        $list = $b2d{$base};

        $f = $freq{$base};
        print "$base/$f\t";

        foreach $t (@$list ) {
            $f = $freq{$t};
            print "$t/$f\t";
        }
        print "\n";
    }
}
</pre>We assembled a final test harness called <tt>tabulate</tt>
which did little more than this:
  <pre>
 readfreqfile( $ARGV[0] );
 setuprules();
 deriveall();
 tabulate();
</pre>

  <p>Now the user can receive a summary of the derivations
  obtained, where each token is displayed followed by its
  frequency. Here is an excerpt of the output:</p>
  <pre>
        approximate/39  approximation/258       approximating/2 approximately/35
        argument/3      arguments/3     
        arise/34        arises/19               arising/41      
        arrange/4       arrangement/17          arranging/1
        artificial/10   artificially/1  
        aspect/2        aspects/4       
        assess/23       assessment/10           assessing/10    assessed/7
        assistant/3     assistants/1    
        assume/68       assumes/10              assuming/69     
        assumption/49   assumptions/19
        asymptotic/17   asymptotics/8
</pre>

  <p><b>Summary</b></p>

  <p>We have managed to obtain remarkably accurate heuristic
  derivations by taking a set of real texts, turning them into a
  frequency distribution, applying a series of very simple addition
  and substitution rules to every token, and rejecting derivations
  that do not appear in the frequency distribution. Some
  derivations are wrong, but a high percentage are accurate. The
  results are certainly good enough to enable Web searches for
  morphologically related words &mdash; a significant improvement
  over a dumb keyword search.</p>

  <p><b>Future Work</b></p>

  <p>To enhance our system, more rules could be added (we routinely
  use a much bigger set, and are augmenting them all the time).
  More categories of replacement rules (such as affixation and
  compounding) should be added too.</p>

  <p>We could make <tt>mkfreq</tt> retrieve an arbitrary web page,
  discard its HTML tags, and generate its frequency list using the
  LWP::Simple module, exactly as shown by Jon Orwant and Dan Gruhl
  in TPJ #13. A version that does this (<tt>webmkfreq</tt>) is
  available from the TPJ web site.</p>

  <p>Reading in the entire token frequency file cost a lot of time
  and memory, so we implemented it as a DBM file instead. This
  significantly speeds up programs that access only a few frequency
  elements (such as plural and adverb), but slows down programs
  like <tt>tabulate</tt> that access a very large number of
  frequency elements many times. We concluded that in our era of
  large high-speed computers and bountiful memory, reading a token
  frequency file into memory is realistic.</p>

  <p>One could easily add the ability to include known word lists
  such as Unix's <tt>/usr/dict/words</tt>, so that we could include
  entries for derivations of a base token not present in the
  corpus.</p>

  <p>We have also implemented two small applications called
  <tt>plural</tt> and <tt>adverb</tt>, which use expanded
  single-purpose sets of rules to report on the probable plural and
  adverb forms (if any) of a given word. It answers the very first
  question we posed in this paper:</p>
  <pre>
Adverb form of accident: accidently (3.5%) accidentally (96.5%)
</pre>

  <p><b>_ _END_ _</b></p>
  <hr>
  <i>Prof. Khurshid Ahmad (<a href=
  "http://www.mcs.surrey.ac.uk/showstaff?K.Ahmad" target=
  "resource window">http://www.mcs.surrey.ac.uk/showstaff?K.Ahmad</a>)
  has just become the head of the Department of Computing at the
  University of Surrey, and is the head of the Artificial
  Intelligence research group. He and his colleagues do AI work in
  several research areas, with NLP one of the main focuses. He also
  teaches several lecture courses on AI.</i><br>
  <i>Duncan C. White (<a href=
  "http://www.mcs.surrey.ac.uk/showstaff?D.White" target=
  "resource window">http://www.mcs.surrey.ac.uk/showstaff?D.White</a>)
  is a programmer and Unix Systems Administrator in the Department
  of Computing with a thorough knowledge of Unix, C, Perl, and CGI
  programming on the web. He also teaches an introductory Computer
  Architecture and Operating Systems lecture course and is writing
  this up in book form at present. His non-computing interests span
  the natural world &mdash; botany, geology, and evolution. His
  only known idiosyncrasy is a profound hatred of Macs, although
  writing silly information about his idiosyncrasies might qualify
  too...</i> <!-- end of file -->
</body>
</html>
