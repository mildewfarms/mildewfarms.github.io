<html><head><title>Mar03: Perl in High Performance Computing Environments</title></head><body BGCOLOR="#ffffff" LINK="#0000ff" VLINK="#330066" ALINK="#ff0000" TEXT="#000000"><!--Copyright &#169; The Perl Journal--><h1>Perl in High Performance Computing Environments</h1><p><i>The Perl Journal</i> March 2003</p><h3>By Moshe Bar</h3><I>Moshe is a systems administrator and operating-system researcher and has a M.Sc and a Ph.D. in computer science. He can be contacted at moshe@moelabs.com.</I><hr><p>Linux has brought High Performance Computing (HPC) to the masses. Until a few years ago, only government agencies and big corporations could afford to crunch numbers with Crays and other supercomputers. Today, small companies or even individuals can build a cheap cluster of commodity Linux boxes and run compute-intensive applications. </p><p>Two distinct clustering paradigms exist for HPC: Beowulf-style and Single-System-Image-style (SSI). SSI clusters like openMosix (http://www.openMosix.org/) or openSSI (http://www.openSSI.org/) connect <i>n</i> Linux boxes to look like one giant, single computer with <i>n</i> CPUs. In the November <i>TPJ,</i> I showed how to make use of the simple but powerful Perl parallel fork manager module to create parallel environments. The parallel fork manager works best in SSI environments because each new forked child is automatically sent to a new cluster node for execution. </p><p>For more classic computing problems, such as fast Fourier transformations (FFTs), it is often more efficient to use the Beowulf approach. Beowulf technologies such as message passing interface (MPI) or parallel virtual machines (PVM) are based on external libraries implementing a virtual parallel computer paradigm. Programmers need to modify their applications to use parallelization directives to split up computational loops across multiple nodes. </p><p>Traditionally, programming languages such as Fortran and C have been used for number-crunching applications, in part due to the multitude of mathematical and engineering libraries available to programmers. In fact, HPC applications are quite often written ad hoc; that is, they are written by scientists for a particular, temporary problem and then discarded to be replaced by new ad hoc programs, taking the output of the previous program as input for the next step. </p><p>To these developers, speed of development is of prime importance. Fortran and C, however, do not lend themselves easily to fast development or ad hoc programming. Perl, on the other hand, is ideal for prototyping and ad hoc programming. Naturally, people have been devising ways to use Perl for number crunching. </p><p>Perl and/or scripting language opponents will be quick to point out that interpreted languages have, by definition, a speed disadvantage compared to compiled languages. In my own personal experience, this is only true where pure mathematical performance is concerned. However, as soon as you add some I/O and other outer-world interaction, Perl quickly gains ground in comparison. Additionally, given the ample computing power available on today's CPUs, a few percentage points advantage in cycle efficiency is not going to make such a big difference in execution time. </p><p>There are a number of parallel computing modules out there for Perl. The most widely used are the <i>Parallel::Pvm</i> and <i>Parallel::Mpi</i> modules from CPAN.</p><h3>Using the Parallel Modules </h3><p>Before hacking away on a number-crunching application, one should first install either PVM and/or MPI, and the corresponding Perl modules from CPAN. For some help in setting up the PVM environment, see http://help-site.com/c.m/prog/lang/perl/cpan/ 05/parallel/parallel-pvm/_d11729. For help with MPI setup, see http://www.jics.utk.edu/MPI/MPIguide/MPIguide.html.</p><p>Once your cluster is installed, say with PVM, you start the virtual parallel computer with simple statements like this:</p><PRE>Use Parallel::Pvm;Parallel::Pvm::addhosts("foo", "bar");</PRE><p>Once your cluster is up and running, you register your program to be a PVM implicitly by calling any PVM function or explicitly by doing something like this:</p><PRE>$mytask = Parallel::Pvm::mytask;</PRE><p>The next step would be to let an executable run on, say, 16 virtual hosts. In PVM lingo, you call that "spawning" executables. You do that by executing a line like this:</p><PRE>($ntasks, &amp;tids) = Parallel::Pvm::spawn("executable', 16,,argv); </PRE><p>In this example, the scalar <i>$ntasks</i> will hold the number of children spawned and <i>$tids</i> will hold the task ID's of the children. There are several arguments for the spawn function, obviously. The documentation of the module goes into great lengths to explain every single function's use. </p><p>Just like in MPI, you may want to send messages to instances of an executable running on a remote node. In <i>Parallel::Pvm,</i> you do that with a code sequence:</p><PRE>Parallel::Pvm::initsend ;Parallel::Pvm::pack(1.333,"sample_message");Parallel::Pvm::pack(100);Parallel::Pvm::send($dtid,999);</PRE><p>The first statement makes sure we have a <i>send</i> buffer container, and then we fill it with a double, string, and integer value, respectively. Once we fill the container, we send it to a particular task, <i>$tid</i>, and we tag this message with <i>999</i>. </p><p>On the receiving end of this message, you unpack the content in reverse order by executing the following statements: </p><PRE>Return_code = Parallel::Pvm::recv;$int_t = Parallel::Pvm::unpack;($double_t,$str_t) = Parallel::Pvm::unpack;</PRE><p>There are dozens of other options and functions in <i>Parallel::Pvm</i>. You can build-in fault tolerance by respawning lost children, for instance. You can even provide parallel I/O or send nonblocking messages. </p><p>If you need to write a number-crunching application fast, then <i>Parallel::Pvm</i> is certainly worth considering. It's easy to use and powerful. </p><h3><i>Parallel::MPI::Simple</i></h3><p>In MPI (similar to PVM), the central idea is to have several instances of the same executable running on various nodes, and use message passing for coordination among the instances. Just as with PVM, installing the MPI libraries is as easy as typing "rpm -install" under Linux (or through similar means under other operating systems). Perl programs making use of the <i>Parallel:: MPI::Simple</i> module should be launched with the standard MPI command:</p><PRE>mpirun -np 2 perl script.pl</PRE><p>MPI is very simple.  These six functions allow you to write many programs: </p><p>MPI_Init </p><p>MPI_Finalize </p><p>MPI_Comm_size </p><p>MPI_Comm_rank </p><p>MPI_Send </p><p>MPI_Recv</p><p>In fact, in most cases, you don't even need communication between the nodes. Take, for instance, a simple program to calculate <IMG SRC="pi14.gif" ALT="" WIDTH="7" HEIGHT="7"> on any number of nodes available. For that, we use a discrete integration (Simpson's rule) under the curve <i>x</i>*<i>x</i>+<i>y</i>*<i>y</i>=1, a circle of radius 1 for 0&lt;<i>x</i>&lt;1, and multiply by 4.</p><p>Using MPI (see <A NAME="rl1"><A HREF="#l1">Listing 1</A>), you first initialize MPI, then indicate the number of divisions. Now, within the integration loop, each node can compute a constant cycle of divisions. Finally, we assemble the results at the end at one of the instances. </p><h3>Conclusion</h3><p>For quick and not-so-dirty development of number-crunching applications, Perl (with the appropriate modules) can be an intelligent choice. Because relatively few people use this technique as of yet, there is still a lot of room for research in this field, particularly in the benchmarking area. Please get back to me with samples of your own parallel Perl programs, as I plan to return to this subject in a future article. </p><p><b>TPJ</b></p><H4><A NAME="l1">Listing 1</H4><pre>#!/usr/bin/perluse lib qw(/usr/local/perlmod/Parallel-MPI-0.03/contrib/cpi/../../blib/arch/usr/local/perlmod/Parallel-MPI-0.03/contrib/cpi/../../blib/lib);$|=1;use Parallel::MPI qw(:all);sub f {    my ($a) = @_;    return (4.0 / (1.0 + $a * $a));}my $PI25DT = 3.141592653589793238462643;MPI_Init();$numprocs = MPI_Comm_size(MPI_COMM_WORLD);$myid =     MPI_Comm_rank(MPI_COMM_WORLD);#printf(STDERR "Process %d\n", $myid);$n = 0;while (1) {    if ($myid == 0) {        if ($n==0) { $n=100; } else { $n=0; }        $startwtime = MPI_Wtime();    }    MPI_Bcast(\$n, 1, MPI_INT, 0, MPI_COMM_WORLD);    last if ($n == 0);    $h   = 1.0 / $n;    $sum = 0.0;    for ($i = $myid + 1; $i &lt;= $n; $i += $numprocs) {        $x = $h * ($i - 0.5);        $sum += f($x);    }    $mypi = $h * $sum;    MPI_Reduce(\$mypi, \$pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);    if ($myid == 0) {        printf("pi is approximately %.16f, Error is %.16f\n",               $pi, abs($pi - $PI25DT));        $endwtime = MPI_Wtime();        printf("wall clock time = %f\n", $endwtime - $startwtime);    }}MPI_Finalize();</pre><P><A HREF="#rl1">Back to Article</A></P></body></html>