<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v13, i03: NetBackup Performance Reports with MySQL/Perl</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><span class="navbarLink"><a href="a1_f1.htm">Figure 1</a></span></td>
		<td><span class="navbarLink"><a href="a1_l1.htm">Listing 1</a></span></td>
		<td><span class="navbarLink"><a href="a1_l2.htm">Listing 2</a></span></td>
		<td><span class="navbarLink"><a href="a1_l3.htm">Listing 3</a></span></td>
		<td><a href="../../../../source/SA/2004/mar2004.tar"><b class=codeListing>mar2004.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <h1><b><img src="a1.gif" width="200" height="167" align="right">NetBackup 
              Performance Reports with MySQL/Perl</b></h1>
            <p> <i>Jerry Uanino</i>
            <p> Backups are always a problem. Often, even in large companies where 
              you would expect such an important task to be a high priority, backups 
              are given to a new guy, an intern, or anyone willing to pay attention. 
              Many excellent systems administrators began their careers changing 
              tapes and monitoring or restarting failed backups.
            <p> If you oversee a large installation and backups are not your primary 
              responsibility, it's a daunting task to determine not only 
              that the backups are succeeding, but also that they are occurring 
              consistently and efficiently. In this article, I'll describe 
              a technique for monitoring the performance of a Veritas NetBackup 
              Datacenter system. Although this technique is specific to a NetBackup 
              installation, the same concepts can be used to monitor any backup 
              system.
            <p> The main concept is to run a collector script that queries the 
              NetBackup catalog for backup statistics and stores that information 
              in a MySQL database. A second program then uses the backup data 
              in the MySQL database to generate a chart with Perl and the help 
              of GD::Graph::bars. Alternatively, you can use SQL commands to query 
              the MySQL database directly and produce any type of custom report. 
              Figure 1 shows some example output.
            <p> Performance data gathered for each backup would include the backup 
              ID, the duration, the client host, the schedule type (full or incremental), 
              the amount of data backed up, the number of files, the kilobytes 
              per second, the policy used, the date the backup started, and the 
              server that initiated the backup. This data can then be queried 
              to help answer such questions as:
            <p> 
            <p> Which system is the best performer?
            <p> 
            <pre>
mysql&gt; select client,kbpersec from backperf order by kbpersec desc limit 1;
+------------+----------+
| client     | kbpersec |
+------------+----------+
| somehost   |    24612 |
+------------+----------+
</pre>
            Which systems back up the most data?
            <p> 
            <pre>
mysql&gt; select client,kbytes from backperf order by kbytes desc limit 1;
+---------+-----------+
| client  | kbytes    |
+---------+-----------+
| ct01sts | 746335072 |
+---------+-----------+
</pre>
            What is the average number of files being backed up?
            <p> 
            <pre>
mysql&gt; select avg(numfiles) from backperf;
+---------------+
| avg(numfiles) |
+---------------+
|   102661.9459 |
+---------------+
</pre>
            Other questions we could answer include: How long is client X taking 
            to back up? What is the median/average backup time? How many files 
            are the poor performers backing up? Which are the good performers?
            <p> These kinds of questions can be answered by analyzing the data 
              shown in the previous examples. Figure 1 shows kilobytes/sec, which 
              helps identify which systems might need attention and provides a 
              good report to check every morning.
            <p> <b>The Setup</b>
            <p> Now let's get to the nitty-gritty. The first piece in this 
              setup is a MySQL database. This article assumes you have a MySQL 
              server running. Using nbperf.sql (Listing 1), you can create a MySQL 
              table called "backperf" to store your backup performance 
              data.
            <p> 
            <pre>
#mysql -u root -p
mysql&gt; create database nbperf
(control-d)
#cat nbperf.sql | mysql -u root -p nbperf
</pre>
            At this point you have a database (nbperf) with one table (backperf).
            <p> The next piece is to put data into the table on a regular basis. 
              Ideally, this would occur as a cron job sometime after the backups 
              are complete (early morning in most cases). Because the collector 
              can run as often as you like, scheduling depends on your environment. 
              The collector, co_backperf.pl (Listing 2), will need to be modified 
              to fit your system environment. Set the <b>$database</b> and <b>$hostname</b> 
              variables to the database you created in the above steps on the 
              MySQL server. You will also need to modify <b>@servers</b> to contain 
              the NetBackup servers on which you want to collect data (i.e., "server1" 
              and "server2"). Going through the listing, the next important 
              thing to note is the <b>$cmd variable</b>. "bpimagelist" 
              is the guts of the output we want to store; every time it runs, 
              it produces a listing of everything in the NetBackup catalog since 
              the "-d" option and includes several important fields. 
              Typical output of this command looks like this:
            <p> 
            <pre>
IMAGE clientname 0 0 6 clientname_1054339491 policyname 0 *NULL* root
schedulename 0 5 1054339491 1324 1062374691 0 0 2659008 113017 1 1 0
policyname_1054339491_FULL.f *NULL* *NULL* 0 1 0 0 0 *NULL* 0 0 0 0 0 0 0
*NULL* 0 0 0
</pre>
            Although this looks confusing, co_backperf.pl will parse it. If you 
            want to get a better "user" view of this data, you can add 
            the "-U" flag to the command (but don't change this 
            in the script). If the output of this command contains lines with 
            "IMAGE", a split occurs on whitespace and several fields 
            are noted. In particular, we grab these:
            <p> 
            <pre>
$backupid  = $fields[5];
$seconds   = $fields[14];
$client    = $fields[1];
$schedtype = $fields[11];
$kb        = $fields[18];
$num_files = $fields[19];
$kbpersec  = ($kb/$seconds);
$policy    = $fields[6];
$backdate  = $fields[13];
$backserver= $host;
$backdate  = &amp;humantime($backdate);
</pre>
            After noting the important fields, co_backperf.pl will insert an entry 
            into the database for this backup id (field 5 is the backup id). Running 
            the script a second time will produce several failed inserts. This 
            is because entries that are in the database will still appear in the 
            catalog for some time until they expire, but the failed insert will 
            prevent duplicate entries. This is done by a primary key set on the 
            backupid field. Thus, even if the NetBackup catalog expires an entry, 
            its historical performance data is still kept in the MySQL database. 
            Additionally, one could write a co_driveindex.pl that would run bperror 
            commands to collect the drive on which a specific backup id occurred. 
            This is handy for checking the balancing of your drive usage for multiple 
            tape drive configurations. (The drive_index field is unused in this 
            article.)
            <p> Note that I call the primitive rsh in my script, but you can use 
              ssh or any other method. If you run the script locally on the backup 
              server, you could remove the call altogether. One significant improvement 
              would be to switch the command calls to use Net::SSH::Perl or a 
              similar module.
            <p> Next, I will cover the representation of this data, which makes 
              for good management meetings. mkimage_backperf.pl (Listing 3) creates 
              an image using GD::Graph and a few other CPAN modules. In the listing, 
              you'll notice <b>$database</b>, <b>$hostname</b>, <b>$user</b>, 
              and <b>$password</b> must modified as with co_backperf.pl. Then, 
              we issue the following query:
            <p> 
            <pre>
SELECT backdate, client, kbpersec, policy,
 (to_days(now()) - to_days(backdate)) as age
 FROM nbperf WHERE
 (to_days(now()) - to_days(backdate)) &lt; 2
 $policy_hunt
 ORDER BY policy DESC
</pre>
            Here we query the backup date, the client, the kilobytes per second, 
            the policy that was used, and the "age", which is calculated 
            with some MySQL functions. This produces a list of rows for backups 
            that occurred within the last 24 hours. The order in which they will 
            appear in the chart is by policy name, descending. Each row is used 
            to build the X and Y axis of the chart, using the GD::Graph to produce 
            the result (see Figure 1). That's it! From this point, you could 
            modify mkimage_backperf.pl to have a different y_max_value if your 
            drives are slower or faster than my drives. The resulting image will 
            be written to an img/ directory, so you need to create that directory 
            before running the script (see Figure 1). Note that, if you don't 
            want all this fancy charting, you could simply run this on the backup 
            server:
            <p> 
            <pre>
/opt/openv/netbackup/bin/admincmd/bperror -all -hoursago 24  \
  |grep "wrote backup" |  \
awk {'print $1 " " $15 " " $20'}  | sed 's/,//g'
</pre>
            This command would give you the backup jobs and speeds from the last 
            24 hours. You could then store or chart these results as you like. 
            You could also use Excel or another charting program once you have 
            the data in the database.
            <p> <b>Assumptions</b>
            <p> You'll need to install the Perl modules GD::Graph and DBI/DBD::mysql 
              and have a MySQL installation available for use. Also, note that 
              the kilobytes per second that is calculated does not take into account 
              whether a tape needs to be switched. This field is calculated as 
              kilobytes backed up / seconds. You could run bperror commands and 
              change your collector to use the actual tape speeds, but in my case 
              a library switches tapes rather rapidly and the value is close enough 
              to get a good picture. Besides, if a backup were slow because a 
              drive wasn't free, or the tape wasn't found for a while, 
              I'd want to know.
            <p> <b>Alternatives</b>
            <p> Veritas has a product called NetBackup Advanced Reporter, which 
              provides reporting capabilities for backup data. I did not review 
              this portion of NetBackup since I didn't know it existed until 
              after I began creating my own scripts. I think the product requires 
              a separate license, and users looking for a more supportable solution 
              might check into this product. The co_backperf.pl would break if 
              future versions of NetBackup changed the format of the output, but 
              then so would a lot of other systems administration scripts that 
              have been written around NetBackup over the years.
            <p> <b>Conclusion</b>
            <p> Although Veritas NetBackup provides various command-line utilities 
              for acquiring backup statistics, there is almost always the need 
              to tie this data into another database or use open source tools 
              to do something with the data. MySQL acts as a good starting point 
              for storing the data because of its simplicity and availability. 
              Perl makes sense for parsing the output of the various NetBackup 
              commands, and GD::Graph works well to make simple graphs. The most 
              difficult part of this setup is getting GD::Graph to install on 
              a Solaris system but the module is used widely enough that you can 
              find help to work through the quirks.
            <p> <i>Jerry Uanino holds a B.S. in Computer Information Systems from 
              Marist College and has worked for IBM, US Internetworking, Enterasys 
              Networks, and most recently Factset Research Systems. He can be 
              reached at: <b>juanino@yahoo.com</b>.</i>
          </table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
