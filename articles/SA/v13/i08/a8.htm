<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v13, i08: Computing Environment Crisis Management</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>
<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><a href="../../../../source/SA/2004/aug2004.tar"><b class=codeListing>aug2004.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <h1><B><img src="a8.gif" width="200" height="167" align="right">Computing 
              Environment Crisis Management</b></h1>
            <p> <I>Debby Hungerford</i>
            <p> Power out? Servers not responding? Phone ringing off the hook? 
              Monitoring systems sending lots of email? If any of these or other 
              obvious signs of trouble are beginning to wreak havoc at your place 
              of work, it's time to reach for that Big Red "Crisis Cookbook" and 
              follow the procedures.
            <P> <B>What?</b>
            <p> You don't have a big red book? The procedures aren't documented? 
              The systems administration team hasn't been trained on crisis management? 
              Read on...
            <P> I am part of an R&amp;D systems administration group, and our 
              crisis management has evolved into a streamlined and well-organized 
              procedure. In this article, I will share some tips about how to 
              be prepared for a crisis.
            <P> The best time to prepare for a crisis is before you've had one. 
              But, the best way to hone your crisis procedures is by experiencing 
              a crisis. So, if you don't have enough crises to hone your procedures, 
              you can set up some drills.
            <P> The main success factors to great crisis management are (not in 
              any particular order):
            <P> 
            <P> 1. Solid and evolving procedures
            <P> 2. Good and simple documentation discipline that is well maintained
            <P> 3. Central points of entry for work orders (a ticketing system), 
              phone calls (a hotline), and urgent needs (on-call system)
            <P> 4. "Battle tactics" leadership
            <P> 5. Timely and succinct communication within the systems administration 
              team to the right external people
            <P> 6. Good contacts and relationships with departments you depend 
              on, such as facilities and that large Information Systems group 
              that provides network infrastructure and support
            <P> 7. Checklists with document pointers
            <P> 8. Responsive and trained people who really care about the customers
            <P> 
            <P> I'm going to touch on most of these points and really focus on 
              a couple of them.
            <P> In the Engineering Computer Services group at Apple Computer, 
              we implemented two major tools to help us manage in the event of 
              a crisis:
            <P> 
            <P> 1. A crisis cookbook
            <P> 2. Outages and post-downtime checklists
            <P> <B>Big Red Crisis Cookbook</b>
            <p> We have identical, red crisis cookbooks in each of three key locations:
            <P> 
            <P> 1. The office area
            <P> 2. Lab
            <P> 3. Main server room
            <P> 
            <P> The crisis cookbook has everything from soup to nuts. Some examples 
              of information include:
            <P> 
            <P> 1. Contact information for the systems administration group, other 
              key people, and certain vendors (e.g., the vendor with whom we store 
              off-site backup tapes)
            <P> 2. Downtime information: checklists, notification procedures, 
              bring-up order
            <P> 3. Important how-to documents: file server quick reference guide, 
              our own failover and creation procedures for key servers and services, 
              restore and snapshot procedures, backups schedule, etc.
            <P> 4. Hosts files (sorted by hostname, hostfile format, and location)
            <P> 5. Diagrams of our server room cabinets and contents
            <P> 6. Terminal server configuration (sorted by port and by hostname)
            <P> 7. Building maps of areas in which we have users
            <P> 8. A CD of our documentation tree
            <P> 9. A pad and pen
            <P> 
            <P> A successful crisis cookbook rests on sound documentation procedures. 
              All documents should contain a header and footer. Our headers are 
              a brief description of the document's purpose. Our footers are a 
              standard format, listing level of confidentiality, the pathname 
              of the document, the author's email address, date created, and date(s) 
              modified.
            <P> All documentation is reviewed by the systems administration group 
              as it is put into production. Most documents are flat-text files. 
              The author emails the group with a pointer to the document (new 
              or updated), and the manager ensures that it is reviewed by at least 
              one person. Documentation that impacts the entire team is reviewed 
              in a team meeting.
            <P> <B>Updates and Tests</b>
            <p> How do we keep the cookbook updated, and how does the team stay 
              fresh about the crisis cookbook's contents?
            <P> A person in the group is designated as being responsible for maintaining 
              the crisis cookbook. A cron job submits a work order every quarter, 
              reminding us that it needs to be reviewed and possibly updated.
            <P> We have a cookbook test every 3 to 6 months. Each team member 
              writes a list of the documents they remember are in the crisis cookbook. 
              They also list documents that they think should be in it but perhaps 
              aren't. Team members get a point for each right answer and extra 
              points if they've identified a document that isn't in the book but 
              should be. The winner gets a prize (like something from the company 
              store that the manager buys), and the loser gets a booby prize. 
              One Halloween, the booby prize was wearing a silly light-up spooky 
              tie. It's important to make the process both fun and worthwhile. 
              We've seen marked improvements in document identification since 
              implementing regular tests.
            <P> Probably the most important aspect of the crisis cookbook, and 
              crisis management, is having good checklists.
            <P> Our checklist serves several purposes. The outages portion of 
              the checklist gets us started with the following steps:
            <P> 
            <P> 1. Notifying our management
            <P> 2. Opening a ticket with INFORMATION SYSTEMS or Facilities if 
              the outage is network or power related
            <P> 3. Escalating notification in those groups as appropriate
            <P> 4. Notifying other key people via voicemail if email isn't working
            <P> 5. Doing walk-arounds
            <P> 
            <P> The event leader ensures someone is always there to answer the 
              hotline phone, while one or two designated people work on and track 
              the problem, and then the rest of the team usually does the walk-arounds.
            <P> Walk-arounds are really important. If the network or power is 
              down or out, the best way to get information across is human contact. 
              In a walk-around, the systems administrators each take a couple 
              of floors in our two primary buildings and spread the word to as 
              many people as they find. The event leader keeps the roving systems 
              administrators updated by cell phone as to status, and calls them 
              back when everything is supposedly working. At that point, it's 
              time to go into the next phase, which is verifying that the environment 
              is back up and running.
            <P> The next phase, verifying the environment, is covered with a downtime 
              checklist. Here are some of the key components of this process:
            <P> 
            <P> 1. A checklist coordinator is identified to lead the team through 
              the event.
            <P> 2. We communicate the "all clear" to our management and notify 
              them that we're beginning the verification process.
            <P> 3. Again, we ensure that someone is manning the phone.
            <P> 4. We test our work order system.
            <P> 5. We check all points of entry for work orders or calls from 
              users and process them.
            <P> 6. We check/clear NFS, printers and plotters, servers, the job 
              management system, CAD tools, data management, backups, licenses, 
              databases, email, shares, cron, MRTG, the Web, etc.
            <P> 7. We communicate the "all clear" to users.
            <P> 8. We line up a recovery crew for the evening or next morning 
              if appropriate.
            <P> 9. We conduct a post mortem or lessons learned. All post mortems 
              are documented, and action items are put into the ticketing system 
              if they last more than a day.
            <P> 10. The environment bring-up order is also documented on the checklist.
            <P> 
            <P> I've been in R&amp;D Unix systems administration and management 
              for more than 20 years VLSI Technology, MIPS/SGI, Octel/Lucent, 
              and now at Apple Computer. I'm here to tell you that bad things 
              are still going to happen, but good crisis management will help 
              you deal with them much more successfully.
            <P> <I>Debby Hungerford came up in the trenches as a lone systems 
              administrator at VLSI Technology in the 1980's. She helped develop 
              some of the early standardization and consistency philosophies for 
              R&amp;D Unix environments used in Silicon Valley. Further senior 
              systems administration and management experience was gained by rebuilding 
              the environments and R&amp;D Unix System Administration groups at 
              MIPS Computer Systems and Octel. Debby now works at Apple Computer 
              in the same line of work. She can be contacted at: <B>hungerford@apple.com</B>.</I></table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
