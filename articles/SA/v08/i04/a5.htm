<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v07, i04: Designing for Disaster</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#990000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center><table width=600 cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">

<tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class=navbarLink>Article</span></td>
		<td><span class=navbarLink><a href="a5_l1.htm">Listing 1</a></span></td>
		<td><a href="../../../../source/SA/1999/apr99.tar"><b class=codeListing>apr99.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
<td valign=top width=440 bgcolor="#ffffff">
<table width=100% cellpadding=15 cellspacing=0 border=0>
<tr><td valign=top>
<! -- Insert Content ------ >




<H1>Designing for Disaster</H1><p>
<P>Chad Barker and Mark Frisbey
<P>
In today's competitive world, the time to recover from a disaster is of paramount importance. Downtime converts directly to lost revenue. Too much lost revenue, and the recovery process may be futile if the organization's financial resiliency is exceeded by the loss. Thus, it is essential that computing environments be designed with disaster recovery in mind. The extra effort invested in solid system design can make the difference between the system administration group's having a bad day, and the entire company's employees looking for new jobs. This article explores such a disaster-minded design effort. Although our specifics relate to an HP-UX environment, the basic issues apply to any OS.<p>
Downtime can involve three components: 1) replacement of hardware, 2) reconstruction of the disk layout, and 3) restoration of the data. The speed of data recovery is a function of the speed of the recovery system and the size of the files. Filesystems with many small files take longer to recover than filesystems with large files. The time to recover also depends on whether the recovery system is local or remote. If it is remote, the amount of network traffic will affect restoration time significantly. The time to get from the hardware replacement to the restoration of data is determined by the complexity of the I/O subsystem layout. The old adage "time is money" becomes a reality.<p>
Cheaper and larger disk drives coupled with logical volumes and multiple I/O channels have increased the intricacy of machines dramatically. Decisions are made to optimize the I/O performance with the possible disadvantage of increasing the time to restore the data. Administrators have typically used either SAM or the command-line interface. SAM on earlier versions of the OS did not provide the flexibility some machines needed. It is also very clumsy on an ASCII terminal. The command-line syntax could be lengthy, complex, and extremely susceptible to typos (some of us were not hired for our typing skills).<p>
The machine on which our disaster recovery programs were developed has 52 disks, 7 volume groups, 31 logical volumes, and 6 I/O interfaces. We built the machine using the command-line syntax, which took about four hours. When we tested the programs, that time was reduced to about 20 minutes. One factor that affected the time was whether we used <B>hfs</B> or <B>vxfs</B> filesystems. <B>vxfs</B> filesystems build much more quickly than <B>hfs</B>.<p>
The driving force to develop a simpler approach was the increased complexity of the activity. There were a myriad of things that could be incorporated into such a project, but we focused on a small subset to get started. We originally thought to include volume groups that contained parts of the OS, but if we allowed the user to remove a volume group and it contained a portion of the OS, that could create a new set of problems. So we decided to not pursue that avenue. We thought tying into a backup system would provide a great addition. If we could automate the rebuilding of the volume groups and the restore, it would reduce the recovery time and ease the burden on the administrator because he or she would not have to babysit the entire process. With any tape media, there is always the possibility of a tape failure. When that happens, there is not much you can do except go back to another tape and possibly lose some data.<p>
We refer here to the person using the program as the user. Note that if you are upgrading from HP-UX 10.X to 11.00 and use this program, you will have to edit the <B>volumegroup.devs</B> files, because the device addresses will be different on HP-UX 11.00. Some of the underlying factors that we considered and addressed in the design phase are listed below:<p>
<p>
1. Simple to use interface so an individual unfamiliar with the machine could rebuild it<p>
2. Program must be able to run on an ascii terminal<p>
3. Allow the administrator to create configurations from a template<p>
4. Allow the program to generate the configuration files on an existing machine<p>
5. Support building and destruction at the volume group level<p>
6. Allow the building and destruction of all volume groups (non-OS)<p>
7. Allow for extra swap spaces<p>
8. Allow for extra crash dump spaces (11.0 only)<p>
9. Tie into a backup system (local and remote)<p>
<p>
The directory structures associated with the program are:<p>
<p>
data - This directory contains the configuration files for the machine.<p>
temp - This is the tmp directory.<p>
files - This directory contains the base files and setup files.<p>
logs - This is where the logs are stored.<p>
<p>
There are many files associated with the program, and they are located in the README file in the files directory. The main files associated with the program and their functions are:<p>
<p>
<B>AutoConf</B> - Auto-generates the configuration files<p>
<B>bld.dump</B> - Builds the extra crash dump spaces<p>
<B>bld.sys</B> - Builds the logical volumes<p>
<B>build</B> - Builds the volume groups<p>
<B>confVG</B> - Used to create new volume group or edit and existing one<p>
<B>get_dump.info</B> - Gets the dump info from an existing machine<p>
<B>restmon</B> - Monitors the restore of data returning the percent complete and the approximate time to complete the restore<p>
<B>restoreVG</B> - Restores the volume group<p>
<B>rmlv</B> - Removes logical volumes<p>
<B>rmvg</B> - Removes the volume group<p>
<B>restmenu</B> - The menu system for the restore<p>
<B>setup</B> - The main program<p>
<B>swaprm</B> - Removes extra swap spaces<p>
<p>
The best way to describe what the program does is to walk through the various options. The submenus will not be covered here, but those are self-explanatory.<p>
<B>Main Menu</b><p>1. Configure the Volume Groups Information.<p>
2. Auto configure information for volume groups already on the machine.<p>
3. Build all the Volume Groups.<p>
4. Build a selection of the Volume Groups.<p>
5. Remove all the Volume Groups.<p>
6. Remove a selection of the Volume Groups.<p>
7. Restore data for logical volumes.<p>
8. EXIT<p>
<B>Option 1</b><p>This option allows the user to create a new volume group or modify an existing volume group's information. The program will allow the user to edit the basic templates for the volume group device file and the logical volume file (<B>volumegroup.devs</B> and <B>volumegroup.info</B>, respectively). If there is extra crash dump space needed, the process is manual at this time. The user will have to copy the <B>dump.info</B> from the files directory to the data directory and edit the file. Note that if the crash space is in an existing volume group, care should be taken because the crash dump space must be contiguous. For this reason, if a crash dump space is configured, the crash dump space will be done first to allow for turning off access to the defined disk. Upon completion of adding the dump space, the access is turned on. Listing 1 shows the format of the configuration files.<p>
<B>Option 2</b><p>This option allows the user to build the configuration files on an existing machine. All volume groups will be recorded. The OS volume groups are stored in the data/OS directory and the rest will be in the data directory. If there is a crash dump space, it will create all the correct entries in the <B>dump.info</B> file (11.00 only). RAID systems using alternate links are handled by separating the primary devices into the <B>volumegroup.devs</B> file. The alternate links will be in the <B>volume-groupname.alt</B>. This prevents the program from doing a <B>pvcreate</B> on the alternate link and causing the controller to fail over. If there are logical volumes defined that are not in the <B>/etc/fstab</B> file, they are stored in a file called <B>lv.ex</B>.<p>
<B>Options 3 and 4</b><p>These options allow the user to build all of the volume groups or a subset of volume groups. The rebuild is based on the configuration files for each of the volume groups. The entries for the <B>/etc/fstab</B> are generated as the logical volume is built. If the user has edited the logical volume configuration file and has defined more space than exists, the program will not make the logical volumes that are beyond the disk boundaries of the volume group. If the info file has been edited, and the number of drives to stripe on is less than the number of disks in the volume group, the program will inform you of this error and brings up a <B>vi</B> session to correct the problem. The user would then remove the volume group and remake it.<p>
<B>Options 5 and 6</b><p>These options allow the user to remove all of the volume groups or remove a subset of volume groups. If the volume group contains a secondary swap space, the user will be asked if the machine can be rebooted at this time. If the reboot is chosen, the entry in the <B>/etc/fstab</B> file will be removed and the machine rebooted. If the reboot cannot happen, then the program will not perform the task and return to the menu. Upon reboot the program will have to be restarted, and it will continue with the removal process.<p>
<B>Option 7</b><p>This option allows the user to restore a volume group(s) from tape. At this point, the only backup system supported is standard HP-UX <B>dump</B>/<B>restore</B>, <B>vxdump</B>/<B>vxrestore</B> and IBM's ADSM. In 11.00, the <B>-s</B> option for <B>vxrestore</B> does not work. Extra time is needed to rewind and fast-forward the tape. The limitation is that <B>dump</B> fits on one tape, and <B>bdf</B> is stored in <B>/df.out</B>. This file must be in place before starting the restore. If the user has edited the <B>volumegroup.info</B> files and moved filesystems from one volume group to another, the program will prompt the user that the filesystem does not exist in the <B>/df.out</B> file. It will then bring up a<B> vi</B> session, and the user can make the necessary changes. If no changes are made to the <B>/df.out</B> file, the logical volume in question will not be restored, and an entry in the error log will be made. As with any restore, there is no way to tell how long it will take. We designed a program to show the percent complete compared to the entry in the <B>/df.out</B> file, the current transfer rate, and the approximate time to complete the restore. The time to complete the restore of each logical volume is stored in the log file. Multiple tapes could be used with some modification.<p>
Our program was originally designed and built on a K450 with 128 Gb of disks and six I/O channels. Included in the disk space are two RAID systems. The disk sizes varied from 4.2 Gb to 4.55 Gb and the rpm of the disks were 7200 and 10000. The program was tested extensively on this system. For further testing, it was moved to a C160 with 14 Gb of disk space. The initial setup had parts of the OS in both volume groups. The auto-configure was run, and the configuration files were modified to reflect all of the OS that was contained in the vg00 volume group. We then installed 11.00, restored the configuration directory, and built the system. The machine had one non-OS volume group with six logical volumes striped on four disk drives. The time to recover from the building of the volume group and logical volumes to the completion of the restoration of 5.2 Gb of data was 33 minutes using a local tape drive. We then removed the volume group and restarted the rebuild using a remote 20 Gb, 8-mm tape drive.<p>
The time to recover was about two hours and fifty-nine minutes. Another test run was performed in which we changed the volume group to non-striped drives. The striped drive configuration restored about 11% faster. It is apparent that a local subsystem will recover faster than a remote one. The local restore ran five times faster than the remote restore and three times faster than the ADSM restore.<p>
This program may require some tailoring to work in every custom setup, but it will provide the means to recover most system configurations. The programs can be easily modified to fit your environment. The future enhancements we are considering include the ability to drill down to the single logical volume level for removal and building and adding different backup systems into the restore. A priority system will be devised to handle the order in which logical volumes are made when they are mount points within another filesystem.<p>
<B>Known Problem</b><p>If a filesystem has a mount point in another filesystem, the order in which the logical volumes are made is important, otherwise there will be an error during the mounting of the filesystem. This is especially true if the logical volumes are in different volume groups.<p>
<B>Warnings and Notes</b><p>If you have filesystem that mounts in other filesystems, make sure they are done in the correct order. The program will produce an error if they are done in the wrong order.<p>
If you have parts of the OS in different volume groups, make sure that you include the volume groups in the OS pattern in the <B>setup.env</B> file.<p>
This program may not work with some specific configurations. It has been tested on several different machines with different configurations and it seems to work well. Before using this program, however, the user should test the program in the environment to ensure its reliability prior to using it in production. n</BODY>
<h1>About the Author</h1><p>
This work was supported by the U.S. Department of Energy under DOE Idaho Operations Office Contract DE-AC07-94ID13223.
<p>
Chad Barker is in his junior year at the University of Idaho pursuing a B.S. in Computer Science. His 1998 summer internship was done at Idaho National Engineering and Environmental Lab.
<P>
Mark Frisbey received a B.S.C.S. from the University of Idaho and has worked for the Idaho National Engineering and Environmental Lab for the past 10 years in the Operating Systems Support group. He is currently the technical leader for the HP-UX support.




<! -- Begin Content ------ >
<br>&nbsp;<br>
</table></center></td>
<! -- End Content ------ >

</table><!/center>
<! -- End MASTER TABLE -- >

</body></html>