<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v12, i04: NetWorker Savegroup Summarizer -- A Legato NetWorker Reporting Tool</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><a href="../../../../source/SA/2003/apr2003.tar"><b class=codeListing>apr2003.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <h1><b><img src="a4.gif" width="200" height="167" align="right">NetWorker 
              Savegroup Summarizer -- A Legato NetWorker Reporting Tool</b></h1>
            <p> <i>John Stoffel</i>
            <p> NetWorker Savegroup Summarizer (NSS) came about when we replaced 
              our old four-drive, 8mm (5-GB capacity!) tape jukebox with a new 
              tape jukebox holding two DLT7000 tape drives, driven by a dual-CPU 
              Sun Ultra 2 server. At the time, we were running Legato's NetWorker 
              Backup software version 4.2.x to back up about 100 clients of mixed 
              types, mostly UNIX with some PC and NetWare clients. I wanted to 
              get a better idea of exactly how much data we were backing up, how 
              long backups took to complete, and where the performance bottlenecks 
              were. I also wanted to see how much real improvement we were getting 
              in our backup performance after the upgrade to the new hardware.
            <p> Legato NetWorker is a powerful tool to manage backup needs, but 
              it has some frustrating limitations when it comes to reporting useful 
              statistics from a completed backup. The 5.x version was especially 
              limited in regard to reports you could get from the base product. 
              See Example 1 for a sample savegroup notification report. At the 
              time this program was first started, the NetWorker Management Console 
              product was not available, and the GEMs reporting tool was an expensive 
              add-on. (See the sidebar for an overview of Legato.)
            <p> After looking around and asking on the NetWorker mailing list 
              (see Resources), I ended up writing my own code as presented here.
            <p> The goals of the code were as follows:
            <p> 
            <p> 1. Email the administrator a concise daily report detailing overall 
              status of the backups completed each night.
            <p> 2. Report statistics on how much data was backed up and how long 
              it took.
            <p> 3. Report on the largest saveset backup(s) in terms of the amount 
              of data, as well as the saveset backup(s) that took the most time, 
              since these are not always the same.
            <p> 4. Keep the report in plain ASCII format, 80 columns maximum width, 
              so that reports are useful even on dumb terminal displays.
            <p> 
            <p> See Example 2 for a sample report generated by NSS.
            <p> <b>Problem Description</b>
            <p> The initial implementation of NSS just took the raw information 
              stored in the /nsr/logs/messages log file and parsed out the needed 
              information. This worked at first, because I could first look for 
              the matching lines that signified the start of a savegroup report, 
              and then look for the matching end of the report. Then some post-processing 
              would be done on the data to make it more presentable.
            <p> This process broke, however, when we had multiple savegroups running 
              at different times and with different clients that overlapped in 
              their start/end times. Since the messages file was just an ever-growing 
              file, it was impossible to tell which savegroup a line of output 
              belonged to without knowing lots of internal details of each savegroup. 
              Because I didn't want to do inquiries to NetWorker's internal database, 
              and because all the information I need was in the standard savegroup 
              notification, I wrote the savegroup notifications to a file and 
              ran them through NSS by hand. This was automated fairly quickly 
              with a procmail recipe.
            <p> Over time, NSS has been expanded with extra features, such as 
              a tape report to show how many tapes in each tape pool are full, 
              partially full, or empty. This is a useful check to ensure that 
              you have enough tapes for the next night's run. The limitation here 
              is that you must make a call to the <b>mminfo</b> command, which 
              may or may not be available on the system running the report. Another 
              useful feature is the ability to save easily parsed summaries of 
              the backups for each savegroup recording such details as:
            <p> 
            <p> 
            <ul>
              <li> Start and end time 
              <li> Time taken to save 
              <li> Number of clients 
              <li> Number of failed clients 
              <li> Size of the backup in kilobytes 
              <li> Backup level
            </ul>
            <p> The server and savegroup names are implicit in the default directory 
              structure and filenames used. See the online usage of NSS for details, 
              and Example 3 for a sample log file.
            <p> The eventual goal is to write a plotting (nss-plot, see the resources) 
              program to graph the output in a nice overview format. This project, 
              however, has been going very slowly due to time constraints and 
              limitations of the various plotting tools.
            <p> <b>Breakdown of NSS and How It Works</b>
            <p> The first step of the script is the parsing of the various command-line 
              options. You can change all settings and actions from the command-line 
              switches, so you can embed the script inside another script if need 
              be.
            <p> <b>State Machines</b>
            <p> The core stage is to process STDIN and to scan the input for the 
              start and end markers that determine the savegroup report. This 
              is really the heart of the code, and can be thought of as a simple 
              finite state machine. Any good book on computer theory and programming 
              will describe state machines in more detail, but one definition 
              is:
            <p> 
            <p> 
            <ul>
              <li> An initial state 
              <li> A set of possible input 
              <li> A set of new states that may result from the input 
              <li> A set of possible actions or output events that result from 
                a new state
            </ul>
            <p> This is a very useful way of thinking when you are trying to write 
              a program (which is just a fancy state machine) to parse a set of 
              data to turn it into a more useful representation. Humans are very 
              good at pattern matching, but as seen when trying to write a regular 
              expression to handle all the myriad possibilities to pull out that 
              one piece of data, computers are not as good at pattern recognition, 
              or more accurately, exception handling in pattern matching.
            <p> This is where the power of Perl comes in, since we have both a 
              need for regular expressions to match and find the markers that 
              determine which state we are in, as well as the regular expressions 
              that are used to pull out the needed data inside each section. Luckily, 
              Legato has been quite static with the format of the savegroup reports 
              over three major versions of the software.
            <p> Another useful feature of Perl 5.x is the addition of complex 
              data structures, so we can do hashes of hashes to store the various 
              bits of information that are pulled from the savegroup report. This 
              document cannot really go into this in any great depth, so I would 
              recommend that you see the Perl documentation, especially the <b>perldsc</b> 
              and <b>perlreftut</b> man pages, or the O'Reilly &amp; Associate 
              books <i>Programming Perl</i> (3rd Edition) by Larry Wall, Tom Christiansen, 
              and Jon Orwant and <i>Advanced Perl Programming</i> by Sriram Srinivasan.
            <p> Breaking down the problem into smaller steps is just one technique 
              that can be used to make the problem more manageable. In general, 
              when processing a random stream of input, looking for data, you 
              will have the following states and transitions that are possible 
              as you scan through the input stream.
            <p> 
            <p> 
            <ul>
              <li> I have not found the data I need. 
              <li> I have found the data I need. 
              <li> I have found a different set of data I need. 
              <li> I am at the end of the data I need. 
              <li> No more data to read.
            </ul>
            <p> Transitions are how you move from one state to another, and are 
              really just how you indicate to the computer what to do on the next 
              step of input.
            <p> When parsing input, you can choose the chunk size, such as characters, 
              words, lines, paragraphs, etc. It can be tricky to determine which 
              state to be in (i.e., what to do with the input) when states span 
              multiple chunks. In such situations, it makes sense to break those 
              spans into multiple states. So, when you find Chunk A, you know 
              to look for Chunk B, or you fail and reset to the state before you 
              found Chunk A, or just go back to a default state.
            <p> For example, say you are reading input one line at a time, but 
              the markers you need to match against are split across two lines, 
              with the markers being "foo" and "bar", and the end marker being 
              "END", or the "EOF". Some basic Perl code to handle this can be 
              seen in Figure 3. Note that while we are reading our input, the 
              variable <b>$state</b> keeps track of the state we are in, which 
              tells us what we are expecting for input. This lets us skip the 
              processing of other states that don't apply or that might cause 
              problems if we can't tell what to do with the input without knowing 
              where we are in the processing. Also notice how we jump out of the 
              various "if-then" constructs and back up to the top of the loop 
              to read another line of input.
            <p> <b>Parsing Input in Perl</b>
            <p> Using this type of programming, we can process almost any type 
              of input. When parsing a NetWorker savegroup report, there are four 
              possible states the input can be in:
            <p> 
            <p> 1. We haven't found the start of the savegroup report.
            <p> 2. We've found the header and parsed the info from it.
            <p> 3. We're done with the header but now we have "Unsuccessful Save 
              Sets" to read in and process.
            <p> 4. We've found the "Successful Save Sets" marker and we're processing 
              them.
            <p> 
            <p> In state one, we are looking for the marker that will drop us 
              into state two, where we will process the actual savegroup. This 
              lets us skip mail headers or other extraneous information.
            <p> In state two, we have matched the regular expression that tells 
              us we have found the start of the savegroup report. The code currently 
              supports both Legato NetWorker and "Solstice Backup", which is Sun's 
              name for their OEM'd version from Legato.
            <p> In this state, we pull out information on the number of clients, 
              the savegroup name, the start (or restart) time, and the end time. 
              We might also find out the name of the backup server if we're lucky. 
              This is one of the more frustrating limitations of the savegroup 
              report format, because it does not explicitly state the name of 
              the backup server.
            <p> So, we need state three to help figure out the name of the backup 
              server, because it's not explicitly stated in the savegroup report 
              anywhere. But, because we know that all indexes are saved on the 
              server, we can look for lines that mention "index save" and pull 
              the server name information from there. State three then may or 
              may not be used, depending on the savegroup report and whether there 
              were any failures.
            <p> The main work is handled in the fourth state, which is where we 
              process the individual client saveset (think filesystems) reports. 
              Again, we try to determine which host is really the server. Note 
              that when the server is finished writing all of a client's savesets 
              to tape, it will write the client index on the server to tape as 
              well, so we also look for that information. As each client's information 
              is read in, we find: client name, saveset (directory), level, total 
              amount of data written, the scale used for this measurement (e.g., 
              kilo-, mega-, giga- or terabytes), the time it took to write the 
              data, and the number of files saved.
            <p> There is no finish state. I assume that the input will continue 
              to match in the fourth state, but this is handled by skipping any 
              input that doesn't match the NetWorker report format, so processing 
              continues until the EOF.
            <p> One key step during the processing of client saveset reports in 
              state four is to take the amount of data saved and scale it into 
              kilobytes, so that it's all consistent. This simplifies other steps 
              further in the processing and printing of reports. (Note that I 
              use the standard 1024 bytes in a kilobyte, not the marketing driven 
              version that uses 1000 bytes in a kilobyte.)
            <p> Perl's ability to handle complex data structures is also a key 
              element here, because it lets us store the data we've parsed in 
              a hash of hashes of hashes. This could have been done using <b>split()</b> 
              and <b>join()</b> to build long keys to hold the information, or 
              using multiple parallel hashes, each for just one piece of info. 
              Using a complex data structure contains all the data in one structure 
              for each use. The basic data structure is as follows:
            <p> 
            <pre>Client --&gt; Saveset Name --&gt; Level
                                   --&gt; Total Data Written
                                   --&gt; Time to Write Data
                                   --&gt; Number of Files Written</pre>
								   
            <p> Each level of the above structure is a hash, pointing to one or 
              more sub-hashes as needed. The first level is the name of the client, 
              and since each client can have multiple Savesets, that leads to 
              the second level of the hash. At the third level, we could have 
              gone to a fixed array to hold the information, but continuing the 
              use of hashes serves two purposes. One, it's self documenting -- 
              no need to remember that index 2 is the number of files written 
              by the Client in that particular saveset. Two, the sorting and report 
              generation functions are simpler and most consistent since the entire 
              data structure is just hashes.
            <p> <b>Post-Processing and Reporting</b>
            <p> After all the data has been parsed, we process the data to pull 
              out the start and end times, as well as sum the total amount of 
              data written to tape across all savesets from all clients. This 
              is a simple step, since we already have all the sizes in kilobytes, 
              so we simply sum them up by both level and the overall total.
            <p> Once this is done, we must determine which scale to use when showing 
              the reports. Generally, I like to use the biggest scale possible, 
              since if we have written gigabytes of data to tape, it's not too 
              informative to know how many megabytes it is.
            <p> After that, we can pick and choose among the various reports and 
              output them. When printing reports in Perl, most people think of 
              using formats as a quick and dirty option, but there are some limitations 
              to this method that I found frustrating -- mostly dealing with empty 
              line compaction. So, I only use formats for the main summary section, 
              and instead use <b>printf()</b> for the various extra reports.
            <p> These other reports are fairly self-explanatory, but I will look 
              at how a couple of them work. The <b>print_top_n_size()</b> function 
              will sort and display the clients that wrote the most data. This 
              is broken into two steps -- the first of which goes through all 
              the clients and totals up the size of all savesets written by that 
              client. The data is then put into a temporary hash. The second step 
              is to print out the header of the report and loop through the temporary 
              hash holding the client totals, printing until we either run out 
              of clients or reach the maximum number of clients to show. Generally, 
              only the top five or ten clients are interesting in terms of the 
              amount of data written.
            <p> In contrast, the report for the top N hosts by time is broken 
              down by saveset, since a client with a very small amount of data 
              could have a problem and take a large amount of time to write that 
              data. This is interesting data and should be shown for troubleshooting 
              purposes. The general structure of the report is the same though, 
              where a first loop through the data builds a temporary hash of the 
              needed info, then the header is printed and the report is written.
            <p> One suggestion would be to include the size of the data written, 
              as well as the time, but I haven't felt a need for this, and it 
              has not been requested. There is also an issue with the report width, 
              since I am trying to keep the entire report less than 80 columns 
              in width if at all possible.
            <p> <b>Setting Up and Using NSS</b>
            <p> To run these scripts, you need a reasonably up-to-date version 
              of Perl (see resources) and the Time::Local and Getopt::Long libraries, 
              both of which come standard with Perl 5.000 and newer.
            <p> Edit the file to make sure that you have the correct path to your 
              locally installed version of Perl. You can also edit the first few 
              lines that specify the default directories for where the logs and 
              the savegroup input should be saved. These can also be specified 
              on the command line with the <b>-L</b> and <b>-O</b> options, respectively.
            <p> You can feed the raw Savegroup summaries directly to NSS via STDIN 
              to get a nice report, as shown in Example 2. This is very useful 
              for testing or just running off a quick report to make sure things 
              are working correctly for your site.
            <p> Another technique would be to use procmail to filter incoming 
              savegroup notification emails and forward the summaries, while saving 
              the raw savegroup notification to a mail folder. This is slightly 
              trickier, but eliminates the worry that you'll lose the raw notifications 
              sent to your sys admins. See Figure 2 for an example procmail recipe; 
              you will probably have to tweak it to recognize the format of email 
              sent to you. In this example, the email is sent from SERVER, and 
              it is saved in a mail folder in the user's Mail/ directory. For 
              a more complete discussion of procmail and how to write recipes, 
              see the resources section.
            <p> You can call NSS directly from NetWorker so that only summaries 
              get emailed to the sys admins. Here are instructions for Legato 
              NetWorker 5.1.1 on Solaris. They might be slightly different depending 
              on which version of NetWorker and which OS you are running.
            <p> 
            <p> 1. Start up the /usr/bin/nsr/nwadmin GUI and connect to your NetWorker 
              server.
            <p> 2. Click on Customize -&gt; Notifications.
            <p> 3. A new window will pop up with a list of notifications. Scroll 
              down and highlight "Savegroup completion".
            <p> 4. Edit the action to be as follows:
            <p> 
            <pre>
   /path/to/nss -o -l -s 10 -t 10 -T -m "admins@foo.com"
</pre>
            5. Click on the "Apply" button.
            <p> 
            <p> The above options deserve some explanation, as shown in Figure 
              1. Note, however, that you can see the online help with an explanation 
              of all the arguments by passing the <b>-h</b> flag to <b>nss</b> 
              when you run it from the command line.
            <p> The <b>-o</b> option tells NSS to write the savegroup notification 
              to the default saveinput directory, as specified in the source code, 
              or by the <b>-O</b> option. The filename format option, <b>-F</b>, 
              defaults to "%S/%G-%D" where %S is the backup server name, %G is 
              the savegroup name, and %D is the date of the savegroup notification. 
              This lets you log the data from multiple backup servers, each with 
              multiple savegroups into a central and consistent directory structure. 
              If necessary, you can recreate reports by running NSS on the file(s) 
              as needed.
            <p> The <b>-l</b> option is similar, but it turns on the logging of 
              the summary data to be plotted later in some sort of data analysis 
              tool, such as plot-nss. It also has a companion option, <b>-L</b> 
              for which directory to log the data to.
            <p> The <b>-s</b> and <b>-t</b> options can be used with or without 
              optional numbers. They turn on the reporting of the top N (default 
              5) savesets in terms of size and time, respectively.
            <p> The <b>-T</b> option turns on the tape report. Please note that 
              this option can only show you the current status of tape use at 
              the time the report is run. So, if you run the report immediately 
              after the savegroup notification is sent by NetWorker, you will 
              get a reasonably accurate status of the tapes remaining at that 
              time. If you run this report later, you may encounter problems, 
              because it depends on the permissions of the user running the report 
              in order to use the <b>mminfo</b> command to extract information 
              from NetWorker. Such access might not be permitted to general users.
            <p> The <b>-m</b> option is obvious, because it is the email address 
              to which to send the Savegroup Summary. If this is left off, the 
              default of STDOUT is used. Its companion option is <b>-S</b> &lt;string&gt;, 
              which specifies the subject of the Savegroup Summary being emailed. 
              It defaults to the following: "Backups %E: %S - %G", where %S and 
              %G are the same as the <b>-F</b> option, and the %E gives the status 
              of the backup that completed, either "SUCCEEDED" or "FAILURES". 
              The idea here is that if you miss reading email for several days, 
              you can sort your inbox by "Subject:", and all the successful reports 
              can be disposed of quickly. In this way, you can focus on the failures, 
              since they are the most important.
            <p> Plans for future work include dynamic sizing of column widths 
              in reports based on client hostname length, plotting/visualization 
              tool for the data saved with the <b>-l</b> option, handling newer 
              versions of NetWorker, and adding support for Veritas NetBackup.
            <p> <b>Conclusion</b>
            <p> NSS is still under development as time and energy permit, but 
              the basic layout has stabilized over the past year because it does 
              what I need it to do without muss or fuss.
            <p> In this article, I've tried to provide both a useful script and 
              some pointers on the concepts you can use to write your own application 
              for parsing arbitrary input and generating useful reports.
            <p> <b>Resources</b>
            <p> NSS Homepage -- <b>http://jfs.ecotarium.org/sources/nss</b>
            <p> Procmail -- <b>http://www.procmail.org</b>
            <p> Legato -- <b>http://www.legato.org</b>
            <p> NetWorker Users Mailing List -- <b>http://listmail.temple.edu/archives/networker.html</b>
            <p> <i>John Stoffel attended Worcester Polytechnic Institute where 
              he earned a degree in computer science and spent way too much time 
              doing Theatre and Rock'n'Roll lighting on the side. He currently 
              works as a senior UNIX sys admin for a not-so-large-anymore major 
              telecommunications company. He is also a Board member of the USENIX 
              Sage Certification program at <b>http://www.sage-cert.org</b>. He 
              can be reached at: <b>john@stoffel.org</b>.</i>
          </table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
