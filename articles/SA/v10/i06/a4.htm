<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v10, i06: Secure, Automated File Distribution</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><a href="../../../../source/SA/2001/jun2001.tar"><b class=codeListing>jun2001.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
<td valign=top width=440 bgcolor="#ffffff">
<table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <H1><img src="a4.gif" width="200" height="174" align="right">Secure, 
              Automated File Distribution</h1>
            <p> <i>Tim Maletic</i>
            <p> There are countless ways to send a file from one host to another. 
              But what if you want your systems to do it without your intervention? 
              There are still many answers: use the Berkeley notion of trust (implemented 
              via the r-commands and <b>hosts.equiv</b> or <b>.rhosts</b> files), 
              embed clear-text passwords into scripts or pre-load them into memory, 
              or use anonymous authentication. But what if you want to do it securely? 
              And what if you want to do it efficiently?
            <p> I recently ran into this problem when I wanted to push a set of 
              files to all the UNIX systems at my site. <i>Sys Admin</i> has presented 
              solutions to this type of problem before, but none have sufficiently 
              tackled the security issues. Jim McKinstry is rather explicit about 
              the security shortcomings of his file replication strategy, which 
              includes use of SUID programs, embedded clear-text passwords, and 
              the r-commands (<i>Sys Admin</i>, February 1998), while Robert Blader's 
              techniques focus on physically separated networks (<i>Sys Admin</i>, 
              October 1999). Michael Watson recently presented a brief introduction 
              to using SSH for automated file distribution (<i>Sys Admin</i>, 
              February 2001). However, Watson neglects the possibility of using 
              of public-key authentication, leaving his strategy open to the age-old 
              problems of automated password authentication (see Libes).
            <p> In my situation, the files to distribute were the master-configuration 
              files for a UID 0 process that runs on every host -- the solution 
              had to be highly paranoid. To prevent a rogue server from masquerading 
              as the true master server, I decided that some form of cryptographic, 
              host-to-host authentication was required. Symmetric key cryptography, 
              with a shared secret key on each host, is risky because the compromise 
              of one host breaks the entire system. An asymmetric, public key 
              system would alleviate this threat. The Secure Shell protocol was 
              a good fit -- it is widely ported and resides in user space 
              (as opposed to most IPSec implementations). I didn't want to 
              get into kernel modifications for my entire site. SSH, however, 
              turned out to be only part of the solution.
            <p> <b>The Ingredients</b>
            <p> I considered a number of options, including <b>rdist</b>, NFS, 
              Coda, <b>ftp</b>, SSL, IPSec, and <b>stunnel</b>, but none of them 
              satisfied all of my requirements. The solution I settled on uses 
              Rsync (<b>http://rsync.samba.org</b>) to connect to a chrooted, 
              unprivileged account via OpenSSH (<b>http://www.openssh.com</b>). 
              An Rsync daemon can serve files from a chrooted directory, but it 
              can't do strong host-to-host authentication. <b>scp</b>, part 
              of the SSH suite, can do strong authentication, but its performance 
              doesn't scale because it has no built-in mechanism for copying 
              only the differences between source and target files. Rsync can 
              use SSH for its transport layer, but SSH requires a shell account 
              on the SSH server. My breakthrough came when I finally read the 
              <b>contrib/README</b> file in the OpenSSH distribution -- Ricardo 
              Cerqueira has contributed a patch for chrooting SSH accounts. I 
              have since learned that this functionality is built into the SSH 
              product from SSH Communications Security, Inc. (<b>http://www.ssh.com</b>).
            <p> With these ingredients, we can create a highly secure architecture 
              for automated file distribution. Master copies of the files live 
              under the home directory of a specially-created, unprivileged account 
              on a SSH server. I'll call this user "ssync". Slave 
              copies of the files can reside on any other SSH-capable system that 
              can hit port 22 (the registered SSH port) of the server, even if 
              it crosses an untrusted network. Client systems run a regularly 
              scheduled Rsync command to update the slave files. For the following, 
              let's assume the client-side Rsync runs as root, because we 
              want to preserve the ownership of arbitrary master files (but it 
              can run as any user that can write the slave files to their destination 
              on the client). These Rsync sessions connect over SSH to the server 
              as the ssync user, and the SSH daemon is configured to <b>chroot()</b> 
              to ssync's home directory. The <b>chroot()</b> imprisons the 
              ssync user in his home directory, effectively changing that directory 
              into the root directory, and is an extra precaution in the event 
              that a client system becomes compromised. (Several <i>Sys Admin</i> 
              articles have covered <b>chroot</b> in various contexts. See "Securing 
              Apache" by Kyle Dent, May 1999, for an introduction to <b>chroot</b>.) 
              Public keys are distributed during the initial installation to allow 
              for password-less logins. And since Rsync only transfers the differences 
              of changed files, these sessions can run frequently, relative to 
              the amount of data to sync and its rate of change.
            <p> <b>Mix Well</b>
            <p> I will later analyze the security of this model in some detail. 
              In the meantime, let's look at the practical side of putting 
              these pieces together. We'll build a network consisting of 
              the hosts "sol", "mercury", "venus", 
              ..., "pluto". sol will run <b>sshd</b> and host the master 
              copies of the files under the home directory of the ssync user. 
              mercury, venus, and the rest of the planets will run Rsync over 
              SSH as root to connect to the ssync account on sol.
            <p> You'll need SSH and Rsync on all the planets, while sol will 
              need an SSH server, and statically linked versions of both Rsync 
              and a shell for the chrooted environment. Our examples will use 
              OpenSSH v2.3.0, and Rsync v2.4.6. OpenSSH requires OpenSSL (<b>http://www.openssl.org</b>) 
              and Zlib (<b>http://www.info-zip.org/pub/infozip/zlib</b>) for their 
              cryptographic and compression libraries, respectively.
            <p> OpenSSH, OpenSSL, and Zlib build easily on a wide variety of platforms. 
              See their documentation for details. (I've had a hard time 
              with other applications finding OpenSSL if it is installed in a 
              custom location, so you may save some frustration by letting it 
              use its preferred <b>/usr/local/ssl</b>.) If you are new to the 
              Secure Shell, familiarize yourself with the client's and server's 
              copious options and get a copy of <i>SSH, The Secure Shell: The 
              Definitive Guide</i>, by Barrett and Silverman. The default client 
              and server configurations in OpenSSH are sufficient for our purposes, 
              but I recommend setting:
            <p> 
            <pre>
Protocol 2
PermitRootLogin no
</pre>
            in sol's <b>sshd_config</b> file. The first option disables support 
            for any clients running less than SSH Protocol Version 2. SSH Protocol 
            2 improves upon its earlier incarnations in several respects, and 
            should be required where possible. The second option modifies the 
            default PermitRootLogin behavior, which may bypass your <b>/etc/securetty</b> 
            configuration (especially if you use OpenSSH's <b>src/contrib/sshd.pam.generic</b>, 
            which leaves out a reference to <b>pam_securetty.so</b>). After all, 
            you should never log in directly as root anyway. I also recommend 
            modifying each client's <b>ssh_config</b> to enable <b>StrictHostKeyChecking</b>. 
            This will prevent an <b>ssh</b> client from connecting to a host whose 
            private key has changed, unless the client explicitly overrides this 
            safety feature at the command line.
            <p> Fire up <b>sshd</b> on sol and test logging in with the <b>ssh</b> 
              client from the planets. If you've enabled <b>StrictHostKeyChecking 
              </b>by default, you'll have to manually exchange keys, or temporarily 
              urn it off with the <b>-o StrictHostKeyChecking=no</b> option to 
              the <b>ssh</b> client. If you run into problems, remember to try 
              running <b>sshd</b> in debug mode (<b>-d</b>) and the client in 
              verbose mode (<b>-v</b>). Once the installation is complete, you'll 
              need to create key pairs for root on each of the clients:
            <p> 
            <pre>
root@venus# ssh-keygen -d -P ""
</pre>
            This will create a DSA key pair with an unencrypted private key in 
            the default locations <b>~/.ssh/id_dsa</b> and <b>id_dsa.pub</b>. 
            An encrypted private key requires a passphrase for each use. This 
            is obviously more secure, but hard to automate. We'll have to 
            rely on filesystem protections for our private keys -- more on 
            this later.
            <p> Create the ssync user on sol. Its <b>/etc/passwd</b> entry should 
              look something like:
            <p> 
            <pre>
ssync:x:111:99::/usr/local/ssync:/bin/sh
</pre>
            For our example, the master copies of the distribution files will 
            live under <b>/usr/local/ssync</b> on sol, so this becomes ssync's 
            home directory. Create that directory, as well as <b>/usr/local/ssync/.ssh/</b>. 
            Now concatenate each planet's <b>id_dsa.pub</b> file and place 
            the result in ssync's <b>~/.ssh/authorized_keys2</b> file on 
            sol. This should be sufficient for passwordless logins to the ssync 
            account. Test it from a planet; you should see something like:
            <p> 
            <pre>
root@jupiter# ssh ssync@sol "uname -n; whoami"
sol
ssync
root@jupiter#
</pre>
            You can lock this new account now; you won't be using the password 
            anyway.
            <p> The final twist for the SSH configuration is forcing <b>sshd</b> 
              to <b>chroot()</b> the ssync user. Apparently, this can be done 
              through the use of the "ChrootUser" configuration directive 
              in SCS, Inc.'s SSH implementation. With OpenSSH, you need to 
              apply a contributed patch to <b>src/session.c</b>. The patch, unfortunately, 
              is slightly out of date, but it is simple enough that you'll 
              find it easy to insert the changes into the newer <b>session.c</b>. 
              After fixing <b>session.c</b>, rerun the <b>make</b> and copy the 
              resulting <b>sshd</b> into its production directory. Stop and restart 
              <b>sshd</b>.
            <p> Your new <b>sshd</b> will <b>chroot()</b> a user when it encounters 
              the magic token "<b>/./</b>" in the sixth field of their 
              /etc/passwd entry (the home directory). So modify the ssync account 
              on sol with a password entry such as:
            <p> 
            <pre>
ssync:x:111:99::/usr/local/share/./:/.ssh/ash.static
</pre>
            The directory to the left of the "<b>.</b>" in the sixth 
            field is ssync's real home directory, and the directory to the 
            right of the "<b>.</b>" is ssync's home directory relative 
            to the <b>chroot</b>. We'll try to keep the special configuration 
            files as contained as possible by keeping ssync's statically 
            linked shell in its <b>~/.ssh</b> directory, along with its <b>authorized_keys2</b> 
            and environment files. From the perspective of the real root, our 
            directory should look like:
            <p> 
            <pre>
root@sol: /usr/local/ssync &gt;ls -al
total 28
drwxr-xr-x   7 root     root        4096 Nov 29 09:20 .
drwxr-xr-x  19 root     root        4096 Feb 20 12:59 ..
dr-x------   2 ssync    root        4096 Jan 17 10:58 .ssh
drwxr-xr-x   2 root     root        4096 Feb  9 09:42 bin
drwxr-xr-x   4 root     root        4096 Jan 13 09:29 etc
drwxr-xr-x   2 root     root        4096 Nov  6 16:04 lib
drwxr-xr-x   4 root     root        4096 Nov 29 09:20 platform
root@sol: /usr/local/ssync &gt;ls -al .ssh/
total 2064
dr-x------   2 ssync    root        4096 Jan 17 10:58 .
drwxr-xr-x   7 root     root        4096 Nov 29 09:20 ..
-r-xr-xr-x   1 root     root      275556 Nov  8 13:08 ash.static
-rw-r--r--   1 root     root       12044 Jan 17 10:58 authorized_keys2
-rw-r--r--   1 root     root          11 Nov  8 13:08 environment
-r-xr-xr-x   1 root     root     1801225 Nov  8 13:08 rsync
root@sol: /usr/local/ssync &gt;cat .ssh/environment
PATH=/.ssh
root@sol: /usr/local/ssync &gt;
</pre>
            We use the environment file (documented in the <b>ssh(1)</b> man pages) 
            to set ssync's <b>$PATH</b> so that it can find its shell, a 
            statically linked version of the simple ASH shell that I found installed, 
            by default, on my Red Hat 6.2 system.
            <p> To get the above to work, I discovered that <b>sshd</b> and ssync 
              must agree as to the full path to ssync's shell. Because I 
              didn't want a copy of <b>ash.static</b> getting mixed up with 
              the bulk of my distribution directory, I made the change at the 
              real root: I created a <b>.ssh</b> directory under the real <b>/</b>, 
              and copied <b>ash.static</b> there.
            <p> With that cheap hack out of the way, we're now ready for 
              testing:
            <p> 
            <pre>
root@neptune: / &gt;ssh ssync@sol
Last login: Sat Feb 01 20:01:49 2001 from uranus.example.net
$ pwd
Cannot exec /bin/pwd
$ ls
ls: not found
</pre>
            Oh yeah, all we have to work with is the static ash shell. What can 
            we do with shell built-ins?
            <p> 
            <pre>
$ echo *
bin etc lib platform
$ echo .*
. .. .ssh
$ cd ..
$ echo *
bin etc lib platform
</pre>
            Great! We can't <b>cd</b> out of <b>/usr/local/ssync</b>.
            <p> 
            <pre>
$ cd bin
$ echo foo &gt; test
cannot create test: permission denied
</pre>
            And our file modes and owners forbid write-access to the ssync user.
            <p> 
            <pre>
$ exit
Connection to sol closed.
root@neptune: / &gt;
</pre>
            Now <i>that</i> is a limited environment. But it is enough for Rsync.
            <p> Fetch the Rsync sources from <b>rsync.samba.org</b>. Build and 
              install on the planets, as per the traditional:
            <p> 
            <pre>
user@saturn$ cd /usr/local/src/rsync-2.4.6
user@saturn$ ./configure ; make
user@saturn$ /bin/su
root@saturn# make install
</pre>
            (See the <b>src/README</b> file for details.) For sol, however, skip 
            the <b>make install</b>, and modify the configuration step for static 
            linking:
            <p> 
            <pre>
user@sol$ LDFLAGS="-static" ./configure ; make
</pre>
            Then copy the resulting Rsync binary to ssync's<b> ~/.ssh</b> 
            directory.
            <p> <b>Serve with Lime Twist</b>
            <p> We're finally ready for a real test. On the planets, create 
              the destination directory. Let's make it the same as the master 
              copy: <b>/usr/local/ssync</b>. Now we should be able to run Rsync 
              to retrieve the first batch of files -- the contents of <b>/usr/local/ssync/.ssh</b>.
            <p> 
            <pre>
root@mars# rsync -avz --delete --rsh=ssh ssync@sol:/.ssh /usr/local/ssync
receiving file list ... done
.ssh/
.ssh/ash.static
.ssh/authorized_keys2
.ssh/environment
.ssh/rsync
.ssh/
wrote 80 bytes  read 678888 bytes  271587.20 bytes/sec
total size is 2088836  speedup is 3.08
root@mars#
</pre>
            (If you run into problems at this stage, try removing the <b>chroot</b> 
            restriction from the ssync account to isolate the problem.) See the 
            <b>rsync(1)</b> man pages to become familiar with its many options. 
            Above, we're using the following options:
            <p> 
            <p> <b>-a</b> -- Archive mode (recurse, preserve modes, owners, 
              etc.)
            <p> <b>-v</b> -- Verbose (just for testing)
            <p> <b>-z</b> -- Compress
            <p> <b>--delete</b> -- Delete files on target that aren't on source
            <p> <b>--rsh</b> -- Path to ssh client
            <p> 
            <p> The "<b>-a</b>" and "<b>--delete</b>" options 
              will ensure that the slave directories will exactly replicate the 
              master. This kind of configuration is useful when you want the master 
              to be completely authoritative for the content of the slaves. For 
              example, this could be used to simulate a push of configuration 
              files to all of your hosts. It's only a simulated push, because 
              really each slave system would be regularly Rsyncing to the master. 
              I've had success using cron to schedule an Rsync script that 
              runs every five minutes. To prevent hitting the SSH server all at 
              once, the script sleeps for a pseudo-random amount of time between 
              1 and 180 seconds before launching the Rsync.
            <p> This technique also has applications to static Web content distribution. 
              First, public Web sites could pull their static content from a trusted 
              intranet system. This would save Web authors publishing to or authoring 
              on vulnerable, external hosts. It would also provide an automated 
              update of the site from a trusted copy in the event of defacement 
              (unless they root your box, in which case they'll most likely 
              disable your cron scripts!). Second, such an arrangement fits naturally 
              into Web-clustering strategies, where content synchronization is 
              already a problem. We could sync a 10-node cluster as easily as 
              a single host.
            <p> <b>Don't Drink and Drive</b>
            <p> The proposed file distribution strategy demands a high price in 
              initial configuration, but it pays big dividends on security. Let's 
              think about some possible attack scenarios.
            <p> First of all, anonymous users on separate systems (i.e., neither 
              sol nor one of the planets) will have no access to our files, because 
              they can't authenticate to the SSH server. Because we've 
              locked the ssync account, password authentication isn't an 
              option. Authenticating, therefore, requires a copy of a private 
              DSA key that corresponds to one of the public keys in ssync's 
              <b>authorized_keys2</b> file, and those should only be readable 
              by <b>root@[planet]</b> (or by anyone with access to your backup 
              tapes, which is a good reason to regularly rotate your SSH keys).
            <p> For users with local access to one of the planets, root's 
              private key is protected by the mode 700 <b>.ssh</b> directory. 
              (OpenSSH creates that directory mode 700 -- make sure it stays 
              that way!) While users won't be able to access sol's ssync 
              account, they may very well be able to access the files you are 
              distributing. Set their access modes and owners appropriately on 
              sol, and Rsync's "<b>-a</b>" option will preserve 
              those settings on the planets.
            <p> Version 2 of the SSH protocol will thwart all but the most determined 
              packet-level attacks. There are currently no known vulnerabilities 
              (though there are several in protocols 1.x). Passive packet sniffing 
              will yield no passwords or other data, and active attacks such as 
              session hijacking are prevented as well. Both DNS and the more difficult 
              IP spoofing are blocked by SSH's host authentication. If a 
              rogue server IP spoofs as sol, the planet's SSH clients will, 
              at the very least, complain loudly that sol's host key has 
              changed. If this is a concern, configure your SSH clients to refuse 
              such a connection with the <b>StrictHostKeyChecking</b> option, 
              and manually initialize your clients' <b>known_hosts2</b> files.
            <p> If an attacker gains root access to one of the planets, they'll 
              have one of the golden private keys, and will be able to access 
              the ssync account on sol. However, we've chrooted that account, 
              and (take another look at the file permissions listed above) the 
              ssync user has no write-access to any file or directory. All they'll 
              be able to do with the ssync account is update your file set. (Of 
              course, the client system is hosed, and you've got serious 
              problems, but there is no threat to the distribution system as a 
              whole.)
            <p> You may be wondering whether this public-key authentication really 
              buys us any extra security when we're leaving the private key 
              unencrypted. How is this more secure than embedded clear-text passwords, 
              when in either case, the game is over when an attacker gets the 
              right file? The answer is that public-key thereby restricts access 
              to SSH clients (assuming we haven't done anything silly, like 
              allowing <b>.rhosts</b> files). We can also take additional steps, 
              such as configuring the SSH server to only accept connections from 
              a fixed set of client IP addresses. Then an attacker won't 
              be able to authenticate from arbitrary points on the network. (You'll 
              also get the illusory feeling of safety from knowing how much more 
              difficult it is to shoulder-surf a public DSA key than a clear-text 
              password.)
            <p> The real risk is a root-level compromise of the master host. The 
              severity of this risk depends on what you're distributing. 
              If it is Web content, your Web content is corruptible and no longer 
              trustworthy. If it is host configuration files, your hosts are corruptible 
              and no longer trustworthy. In the latter case, you must protect 
              the master at all costs. Like a Kerberos key server, it should run 
              the fewest services possible, and those should be secured.
            <p> <b>Hangover</b>
            <p> The major weakness of the above strategy is the management complexity 
              when scaling beyond a handful of distribution filesets. Each new 
              directory structure to replicate will need to be analyzed to determine 
              whether or not it can use the ssync account on sol. Perhaps the 
              master files will need to reside on another host, or their security 
              requirements or filesystem location may dictate using another unprivileged 
              account. So, each new fileset to distribute may require the configuration 
              of <b>sshd</b> and the unprivileged account on a new system. Its 
              scalability within a single fileset, on the other hand, is another 
              strength of our solution. It should perform well as the number of 
              files and the number of clients rise.
            <p> <b>References</b>
            <p> Barrett, Daniel J. and Richard Silverman. <i>SSH, The Secure Shell: 
              The Definitive Guide</i>.O'Reilly &amp; Associates.
            <p> Blader, Robert. "File Transfer and Verification Between Non-Connected 
              Networks". <i>Sys Admin</i>, October 1999.
            <p> Libes, D. "Handling Passwords with Security and Reliability 
              in Background Processes" Proceedings of the Eighth USENIX System 
              Administration Conference (LISA VIII), pp. 57-64, San Diego, CA, 
              September 19-23, 1994, <b>http://www.nist.gov/msidlibrary/doc/ \ 
              libes94d.ps</b>).
            <p> McKinstry, Jim. "File Replication". <i>Sys Admin</i>, 
              February 1998.
            <p> Watson, Michael. "Replacing rdist and ftp with scp and Associated 
              Utilities". <i>Sys Admin</i>, February 2001
            <p> <i>Tim Maletic was a doctoral candidate in Philosophy before he 
              started down the path of true enlightenment. He is now a Senior 
              UNIX System Administrator for Priority Health, specializing in Information 
              Security. He can be reached at: <b>tmaletic@alumni.indiana.edu</b>.</i>
          </table></table><br>&nbsp;<br>

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
