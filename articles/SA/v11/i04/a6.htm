<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v11, i04: Using Lib C and I/O and Performance</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><span class="navbarLink"><a href="a6_f1.htm">Figure 1</a></span></td>
		<td><span class="navbarLink"><a href="a6_f2.htm">Figure 2</a></span></td>
		<td><a href="../../../../source/SA/2002/apr2002.tar"><b class=codeListing>apr2002.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <H1><img src="../../resource/storage.gif" width="150" height="150" align="right">Using 
              Lib C and I/O and Performance</h1>
            <p> <i>Henry Newman</i>
            <p> I will be writing a new column for <i>Sys Admin</i> magazine on 
              storage and I/O related topics. To start, I will discuss how I/O 
              works from the application. Applications generally make requests 
              to create files in at least two ways via POSIX standards. One way 
              is that the file is created/opened with the <b>open</b>(2) system 
              call, and I/O is done directly to the system via calls to the raw 
              device, volume manager, and file system (<b>read</b>(2), <b>write</b>(2), 
              <b>pread</b>(2), <b>pwrite</b>(2), or the POSIX standard asynchronous 
              I/O routines <b>aio_read</b>(3RT), <b>aio_write</b>(3RT), <b>lio_ 
              listio</b>(3RT)). With the other method, the file is created/opened 
              with <b>fopen</b>(3), and I/O is performed via the standard C library 
              package (<b>fread</b>/<b>fwrite</b>/<b>fprint</b>).
            <p> The <b>fopen</b>(3) method is more common and uses direct system 
              calls because it has few restrictions. This article describes how 
              an application accesses data through <b>fopen</b>() and the standard 
              C library and shows how you can improve the performance of that 
              application by increasing the efficiency of I/O.
            <p> <b>Using the C Library</b>
            <p> C library functions that read or write data to a file opened through 
              <b>fopen</b>(3), such as <b>fread</b>(3), <b>fwrite</b>(3), or <b>fprint</b>(3), 
              or <b>fprintf</b>(3), make a request to the operating system in 
              a size defined usually in the include file <b>/usr/include/stdio.h</b>. 
              The table below lists the default request sizes for various operating 
              systems. The request size from the <b>fread</b>(3)<b>/fwrite</b>(3)<b>/fprint</b>(3) 
              is set by the size of the buffer being passed.
            <p> In the C library I/O environment, however, this request size does 
              not affect the way data is actually read from or written to the 
              disk. <b>fopen</b>(3), which uses the C library package (libc.a), 
              moves the data from the user data space to a library buffer for 
              each opened file. The size of this library buffer is by default:
            <p> 
            <p> <b>OS Size in Bytes</b>
            <p> Linux 8192
            <p> Solaris 8192
            <p> AIX 4096
            <p> SGI 4096
            <p> 
            <p> By default, all I/O requests to the system will be in the sizes 
              listed in the above table, no matter what the request size you make 
              from <b>fread</b>(3), <b>fwrite</b>(3), or <b>fprint</b>(3) call. 
              This scheme is often called buffered I/O. The library functions 
              read and write their requests to the library buffer, and data is 
              read or written to the system in blocks corresponding to the library 
              buffer size.
            <p> You can change the library buffer size with a call to the function 
              <b>setvbuf</b>(3). Making the library buffer bigger improves performance 
              for sequential I/O (although making the buffer size over 128 KB 
              does not always work because some operating systems and configurations 
              do not support larger I/O requests).
            <p> One of the important things that using the C library buffer provides 
              is that all I/O is completed in 512 byte boundaries. 512 bytes is 
              the base block size for most UNIX systems and is generally the smallest 
              physical amount of data that can be written to a disk, as it is 
              the hardware sector size.
            <p> As you will see in future columns, the amount of data moved does 
              not depend on the request size from the application or the library 
              buffer, but depends on the file system.
            <p> <b>Why This Matters</b>
            <p> All of these issues are important because they affect the system 
              CPU overhead and the performance of the underlying I/O system. During 
              the past 22 years that I have been in the business, the system CPU 
              system performance has improved over 10000000 times, but I/O performance 
              is severely lacking. Over that same time period, the fastest disk 
              seek and latency time has changed from 24 milliseconds to 5.6 milliseconds 
              (the fastest disks then and today), transfer rate per device has 
              only increased at most 133 times (3 Mbytes/sec 1/2 duplex in 1981 
              to 200 Mbytes/sec full duplex in 2002). Without question, storage 
              is the bottleneck for the foreseeable future as the performance 
              increases in system CPU performance will far exceed the performance 
              of disk and/or RAID storage.
            <p> Figure 2 shows device efficiency of I/O read and write requests 
              if each I/O request is followed by an average seek and average latency. 
              This is shown for both 10K and 15K RPM disk drives. In the case 
              of C library-based I/O, the size of the "request" corresponds 
              to the size of the library buffer. As is clearly shown the larger 
              the request the more efficient use of the disks, but even at 512 
              KB request sizes the 15K RPM disk is still less than 50% efficient. 
              Keep in mind that this is absolute worst-case scenario, but it is 
              indicative of the problem.
            <p> Today disks have fairly large caches on each drive (4 MB is common 
              and 16 MB is available, which is larger than the memory of the Cray-1A 
              -- the first machine I worked on). Additionally, both RAID devices 
              and disks use sort algorithms to reduce the number of head seeks. 
              All of this increases the performance over this worst-case chart, 
              but does not eliminate the real issues with the hardware. You still 
              must seek, and you will miss disk revolutions.
            <p> The bottom line is, you must either make large I/O requests to 
              efficiently use the hardware and/or use many disk drives to allow 
              for many average seeks and average latencies.
            <p> <b>Making Larger Requests</b>
            <p> As mentioned previously, if you are using the C library calls 
              you can change the size of your library buffer by using the <b>setvbuf</b>(3) 
              function after the file is opened using the <b>fopen</b>(2) call, 
              but before writing to the file.
            <p> The library buffer size should be exactly 8 bytes more than a 
              multiple of 512 bytes. The additional 8 bytes are required for the 
              hash table for each library buffer and, if not used, will significantly 
              reduce your I/O performance and increase your system overhead (see 
              Figure 1). For example, if you want to set your library buffer size 
              to 64 KB, you would set it to:
            <p> 
            <pre>
	buffer_size=(64*1024)+8;
</pre>
            The following example shows how to use <b>setvbuf</b>(3):
            <p> 
            <pre>
#include &lt;stdio.h&gt;

main ()
{
   char    *buf;
   FILE    *fp;
   fp = fopen ("data.fil", "r");
   buf = malloc (262144);
   setvbuf (fp, buf, _IOFBF, 262144+8);
   fclose(fp);
}
</pre>
            As mentioned, if the 8 bytes are not added, it will cause poor performance. 
            For each write request the system will have to read-modify-write. 
            Read-modify-write happens when the requests are not on 512-byte boundaries. 
            Each time data is written, the system will have to read the data into 
            the system buffers, and then the system will write the data from the 
            user space to the system buffers such that it is written on 512-byte 
            boundaries, and then data will be written from the system buffers 
            to the device. This will happen for each write. This process increases 
            system overhead significantly and reduces I/O performance dramatically.
            <p> So, for programs using <b>fopen</b>/<b>fread</b>/<b>fwrite</b>, 
              making larger I/O requests is easy if you have access to the source 
              code. The real question is how to determine the library buffer size. 
              My rule of thumb for sequential I/O is to make the library buffer 
              size at least 4 times the size of the <b>fread</b>(3) or <b>fwrite</b>(3) 
              request size. If you can afford the memory usage, make the library 
              buffer size a much larger multiple in the range of 512 KB to 16 
              MB of the request size. Determining the correct size has a great 
              deal to do with the rest of the I/O path including the operating 
              system, file system, volume manager, and storage hardware. Determining 
              the exact optimal value will have to wait a few months as we progress 
              with this discussion, but making it large will immediately improve 
              performance over using the default. Of course, this only works for 
              files for which you are doing a great deal of I/O.
            <p> If you have an application that does random I/O, making the buffer 
              larger than the I/O request will hurt performance because you will 
              read data into the buffer that you will not use. The only time making 
              the buffer larger than the request helps is when you can fit the 
              whole file in the library buffer. Sometimes for older application 
              where memory was at a premium files where used where now the data 
              could be placed into memory.
            <p> <b>Conclusion</b>
            <p> The real issue is that you will need to match the application 
              I/O efficiency with the amount of storage that you will need. If 
              you have an application requirement of 90 MB/sec of reading and 
              writing and the application makes 1K random I/O requests, the amount 
              of hardware needed to support that requirement is much greater (likely 
              10x greater) than if the application was making 512-KB sequential 
              I/O requests.
            <p> In the next few columns, I plan to address the whole I/O path 
              and the issues with performance and tuning for the server hardware, 
              operating system, file system, HBA, FC devices (such as tape and 
              disk), and applications (such as databases).
            <p> <i>Henry Newman has worked in the IT industry for more than 20 
              years. Originally at Cray Research and now with a consulting organization, 
              he has provided expertise in systems architecture and performance 
              analysis to customers in government, scientific research, and industry 
              around the world. His focus is high-performance computing, storage 
              and networking for UNIX systems, and he previously authored a monthly 
              column about storage for </i>Server/Workstation Expert<i> magazine. 
              He may be reached at: <b>hsn@hsnewman.com</b>.</i>
          </table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
