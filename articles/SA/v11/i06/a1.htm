<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v11, i06: Multi-Platform Performance Monitoring on the Cheap</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><a href="../../../../source/SA/2002/jun2002.tar"><b class=codeListing>jun2002.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <h1><b><img src="a1.gif" width="200" height="174" align="right">Multi-Platform 
              Performance Monitoring on the Cheap</b></h1>
            <p> <i>Dale Southard</i> 
            <p> The monitoring system presented in this article grew out of a 
              simple need for a portable system with which I could remotely monitor 
              performance metrics on UNIX hosts. When I began looking for a solution, 
              I considered many of the available commercial products. The standard 
              <b>xload</b> is easy to understand, but only provides a single metric 
              (load average) and doesn't retain information across invocations. 
              The <b>top</b> command is better, but again provides only a snapshot, 
              not a history trail. Sun's <b>perfmeter</b> and <b>rstatd</b> 
              provide more metrics and the ability to save trails, but are only 
              available under a few architectures. SGI's Performance Co-Pilot 
              can monitor and save an incredible number of metrics, but at the 
              time was only available under IRIX (it has since been ported to 
              Linux as well). Finally, SNMP looked like a future contender, but 
              still suffered from a lack of affordable monitoring packages and 
              security issues on some platforms. 
            <p> What I really wanted was the ability to collect and save a group 
              of performance metrics and then reduce them to a form that is easy 
              to understand. Ideally, the tools should be portable to a wide range 
              of UNIX flavors. Upon further consideration, I found my needs were 
              simple enough to be met with <b>syslog</b> and some common UNIX 
              utilities. 
            <p> The original inspiration for the design came from one of <b>syslogd</b>'s 
              built-in features, the mark timestamp. Most modern <b>syslog</b> 
              daemons provide a mark function that places a timestamp in the logfile 
              at regular intervals. This is often used to help fix the time of 
              catastrophic system events (such as sudden power loss) that would 
              otherwise provide no log evidence. What limits the usefulness of 
              the standard <b>syslogd</b> mark function is that it provides only 
              a mark indicating that the machine is powered on and running <b>syslogd</b>. 
              In most cases, users and sys admins are interested in monitoring 
              more than the state of machine power and correct <b>syslogd</b> 
              function. 
            <p> <b>Collecting the Metrics</b> 
            <p> For this article, I assume that we have a network of six machines 
              at foo.com. One is the central "monhost" where we will 
              be doing the monitoring. The other five are the client hosts that 
              we will monitor. Each client host is running a different operating 
              system (Solaris, IRIX, IRIX64, Linux, and MacOS X). For this example, 
              we will monitor the following metrics on each client host: 
            <p> 
            <p> 
            <ul>
              <li> Load average of the machine 
              <li> Amount of free memory in MB 
              <li> Amount of free swap space in MB 
            </ul>
            <p> The first step in the process was determining what metrics to 
              monitor and how to obtain them. Getting the load average is trivial. 
              Most variants of SVR4 UNIX (Solaris 2.x, IRIX, etc.) and BSD UNIX 
              (SunOS 4.x, BSD, MacOS X, etc.) include the "uptime" command 
              that includes the system load averages for the past 1, 5, and 15 
              minutes. Since I was interested in the five-minute load average, 
              a simple awk command is enough to select the appropriate field: 
            <p> 
            <pre>
uptime | awk '{gsub(",",""); print $(NF-1)}'
</pre>
            Free memory and swap are more difficult since the commands used to 
            monitor them differ wildly between UNIX flavors. Because the "freemem" 
            and free swap values are related and share the same units, I chose 
            to extract them to a single line of output -- first memory, then 
            swap, separated by white space. 
            <p> Solaris and IRIX are standard SVR4 variants and provide the <b>sar</b> 
              command for monitoring a variety of performance metrics. For memory 
              and swap, the <b>-r</b> flag will narrow <b>sar</b>'s output 
              to the metrics we are interested in. <b>sar</b> lists freemem in 
              pages and free swap in blocks. The default output is the freemem 
              and free swap metrics are pages and disk blocks. <b>sar</b>'s 
              notion of a basic block is 512 bytes on all platforms presented 
              here, so dividing by 2048 will convert blocks to MB. 
            <p> Conversion of freemem pages to MB is OS-dependent. For Solaris, 
              "pagesize" is 8192 bytes, so dividing by 128 gives MB. 
              IRIX comes in two "widths" -- the smaller desktop 
              machines use 32-bit kernels with a pagesize of 4096 bytes; the larger 
              servers use 64-bit kernels with a pagesize of 16384 bytes -- 
              so we will need to divide by 254 or 64, respectively. We will again 
              use awk to filter for our desired metrics and perform the necessary 
              conversions. 
            <p> 
            <p> Solaris free memory and free swap in MB: 
            <p> 
            <pre>
sar -r 1 | awk '{m=int($2/128);s=int($3/2048)} END {print m,s}'
</pre>
            IRIX free memory and free swap in MB: 
            <p> 
            <pre>
sar -r 1 | awk '{m=int($2/256);s=int($3/2048)} END {print m,s}'
</pre>
            IRIX64 free memory and free swap in MB: 
            <p> 
            <pre>
sar -r 1 | awk '{m=int($2/64);s=int($3/2048)} END {print m,s}'
</pre>
            Linux and MacOS both lack <b>sar</b>, and obtaining the memory and 
            swap information is more difficult. In the case of Linux, the memory 
            information can be accessed through the <b>/proc/meminfo</b> pseudo-file. 
            Since that file presents the output as a series of lines, we will 
            need to use awk's pattern-matching abilities to select the correct 
            line for each of our metrics: 
            <p> 
            <pre>
awk '/MemFree/ {m=int($2/1024)} \
/SwapFree/{f=int($2/1024)}\
END {print m,f}' /proc/meminfo
</pre>
            For MacOS X, things are more difficult. OS X uses a dynamic paging 
            system that can create swap files as needed (assuming that the disk 
            has enough free space to accommodate such files). This makes our notion 
            of "free swap space" somewhat bogus since the OS will simply 
            create additional swap files as space is required. Rather than present 
            unrealistic numbers for free swap, we'll just punt and report 
            only the free memory information under Mac OS X. This can be obtained 
            from the output of <b>vm_stat</b>: 
            <p> 
            <pre>
vm_stat | awk '/free:/ {gsub("\\.","");print int($3/256)}'
</pre>
            <b>Using syslogd for Transporting Information</b> 
            <p> Now that we have determined how to extract the metrics we want, 
              the next step is to provide a way to remotely monitor them. Luckily, 
              most UNIX variants provide this capability in the form of <b>syslog</b>. 
              The <b>syslog</b> daemon handles messages according to their priority. 
              In the case of <b>syslog</b>, it means a "facility.level" 
              pair. "Facility" refers to the class of information contained 
              in the message -- common facilities are "kern" for 
              kernel messages, "daemon" for system daemon messages, 
              and "auth" for security messages. Because our performance 
              metrics are a local addition, we will use one of the local facilities. 
              (For these examples, I chose the "local3" facility.) 
            <p> The level part of the priority refers to how serious the message 
              is, ranging from <b>emerg</b> (meaning the system is unstable), 
              to <b>debug</b> (meaning normal debug-level messages). Since our 
              service will be informational, we will log at the <b>info</b> level. 
            <p> The first step is to configure our central loghost to save the 
              messages in a file separate from the usual <b>syslog</b> information 
              (in this example, <b>/var/log/perflog</b>). This can be done by 
              adding the following line to the <b>syslog.conf</b> file and then 
              sending a SIGHUP to <b>syslogd</b>: 
            <p> 
            <pre>
local3.info     /var/log/perflog
</pre>
            We also need to configure <b>syslog</b> on each client host to send 
            the performance data to the monitoring host. Again, this is done by 
            adding a single line to <b>syslog.conf</b> and sending a SIGHUP to 
            the <b>syslog</b> daemon: 
            <p> 
            <pre>
local3.info    @monhost.foo.com
</pre>
            Finally, we should test that the above changes are working. Running 
            the command <b>logger -p local3.info -t TEST hello world</b> on one 
            of the clients should generate a line like the following in the <b>/var/log/perflog</b> 
            file on the monhost: 
            <p> 
            <pre>
Jan 25 23:26:51 irixhost TEST: hello world
</pre>
            Note that the <b>syslog</b> entry includes both "timestamp" 
            and "hostname" information, which will be useful later when 
            we parse the <b>perflog</b> file for the metrics we want. 
            <p> <b>Gathering the Data</b> 
            <p> With <b>syslogd</b> configured on the hosts, we can now use the 
              command pipelines determined in the first step to extract and send 
              data to the monitoring host. Because we want to monitor the metrics 
              over time, we will execute the collection command from cron on the 
              client hosts. As an example, the following crontab entries could 
              be used to send the load, memory, and swap information from the 
              irixhost every 15 minutes (linebreaks have been added for clarity): 
            <p> 
            <pre>
  0,15,30,45 * * * * uptime | awk '{gsub(",",""); print $(NF-1)}' |\
                 logger -p local3.info -t load

  0,15,30,45 * * * * sar -r 1 | awk '{m=int($2/256);s=int($3/2048)}\
                 END {print m,s}' | logger -p local3.info -t memswp
</pre>
            Similar entries should be made in the other client hosts, adjusting 
            the <b>memswp</b> line to incorporate the OS-specific metric collection 
            method previously determined. At this point, we should now be accumulating 
            performance data in the <b>perflog</b> file we configured on the monhost. 
            <p> <b>Data Reduction</b> 
            <p> The final step is to take the gathered data and turn it into something 
              understandable by non-technical users. We will use the open source 
              GNUplot program for this task. The first step is to split the data 
              into separate files for each machine and metric. Each file should 
              contain XY data for plotting (where the X data will be the time 
              and date information, and the Y data will be one or more related 
              metrics). Again, we can do this with awk or sed. For example, we 
              can extract the load average data for the irixhost client using 
              a command like the following: 
            <p> 
            <pre>
awk '/irixhost load:/ {print $1,$2,$3,$6}' /var/log/perflog &gt;irixhost.load
</pre>
            Or, we can use a simpler sed command to filter out the values we are 
            not interested in: 
            <p> 
            <pre>
sed 's/irixhost load://;t;d' /var/log/perflog &gt;irixhost.load
</pre>
            Once we have created one or more XY datafiles, we can plot them using 
            the GNUplot program. I find it useful to do the initial plots using 
            GNUplot's interactive mode. Again using the load average data 
            from the irixhost client as an example, the following GNUplot settings 
            will produce a basic plot that is a good starting point for further 
            customization: 
            <p> 
            <pre>
set title "Load Average"
set data style lines
set yrange [0:]
set xdata time
set timefmt "%b %d %H:%M:%S"
set format x "%m/%d %H:%M:%S"
plot "irixhost.load" using 1:4 title "IRIX Workstation"
</pre>
            GNUplot is a very capable program, and entire articles could be devoted 
            to exploring various options. Here are a few suggestions: 
            <p> 
            <p> 
            <ul>
              <li> GNUplot has extensive online help. When in doubt, try the <b>help</b> 
                command. 
              <li> The <b>set yrange</b> command can be used to select a range 
                of dates to plot. 
            </ul>
            <p> For example: 
            <p> 
            <pre>
set xrange ["Jan 1 00:00:00":"Feb 1 00:00:00"]
</pre>
            <ul>
              <li> Multiple data sets can be plotted using a single plot command 
                (e.g., plotting a total of four metrics selected from two different 
                data files): 
            </ul>
            <pre>
  plot \
    "irixhost.memswp" using 1:4 title "irixhost.foo.com free mem",\
    "irixhost.memswp" using 1:5 title "irixhost.foo.com  free swap",\
    "solarishost.memswp" using 1:4 title "solarishost.foo.com free mem",\
    "solarishost.memswp" using 1:5 title "solarishost.foo.com free swap"
</pre>
            <ul>
              <li> GNUplot supports several output formats including .png and 
                .eps. Use the <b>set terminal</b> command to select a format, 
                and the <b>set output</b> command to choose an output file. 
              <li> The save and load commands can be used to store the variable 
                settings for later use once you've found a good layout. This 
                is especially useful if you tend to look at the same metrics frequently, 
                because the plot can be updated by simply re-parsing the <b>perflog</b> 
                file and re-running gnuplot. 
            </ul>
            <b>Where to Go from Here</b> 
            <p> It's easy to extend this system to metrics beyond those presented 
              here. Almost anything that can be run from cron can be timed or 
              filtered through awk, sed, or Perl to produce a metric that can 
              be sent to the monitoring host. It's also fairly easy to use 
              GNUplot from within a cron script to generate near-real-time performance 
              graphs. Such graphs can even be generated or copied into a directory 
              that is exported by a Web server to provide wider access to the 
              performance data. At one site I even extended this concept to send 
              alpha pages to the sys admin by parsing the <b>perflog</b> file 
              every few minutes and testing metrics against some established minimum/maximum 
              values. 
            <p> Although the system presented here lacks many of the high-end 
              features found in packages like PCP, I have found it useful on many 
              occasions over the years. Because this method relies on components 
              found on almost all UNIX-like operating systems, I often find it 
              easier to make a couple of <b>syslog.conf</b> and crontab entries 
              than to install a more complex package just to monitor a single 
              metric. 
            <p> Links 
            <p> <b>http://www.gnuplot.info/</b> 
            <p> <b>http://www.sgi.com/software/co-pilot/</b> 
            <p> <b>http://oss.sgi.com/projects/pcp/</b> 
            <p> <i>Dale Southard is currently a systems administrator with the 
              Accelerated Strategic Computing Initiative at Lawrence Livermore 
              National Laboratory in Livermore, California. He can be contacted 
              at: <b>dsouth@llnl.gov.</b></i> 
          </table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
