<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v11, i06: Tuning Large Solaris&#153; Servers</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#000000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center>
<table width=98% cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">
  <tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class="navbarLink">Article</span></td>
		<td><a href="../../../../source/SA/2002/jun2002.tar"><b class=codeListing>jun2002.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
    <td valign=top width=527 bgcolor="#ffffff"> 
      <table width=100% cellpadding=15 cellspacing=0 border=0>
<tr>
          <td valign=top> 
            <! -- Insert Content ------ >
            <h1><b><img src="a2.gif" width="200" height="174" align="right">Tuning 
              Large Solaris<sup>TM</sup> Servers</b></h1>
            <p> <i>Bob Larson</i>
            <p> Solaris will always attempt to choose good default configuration 
              values, but sometimes a simple heuristic cannot find the best value. 
              The optimum configuration may depend upon an important trade-off, 
              such as speed vs. space efficiency. In this article, I'll guide 
              you through a medley of performance-related considerations for large 
              Solaris servers and also cover some of the algorithms and heuristics 
              surrounding the most important tunables.
            <p> <b>1. Sizing Swap</b>
            <p> Sizing swap properly can be a trade-off since having either too 
              much or too little can cause problems. Neither situation prints 
              a simple message like "Swap too small" to the console. 
              You may see "not enough memory" messages, but you never 
              see "too much memory", even though this can happen.
            <p> Having swap too large will at best waste disks, and can easily 
              lead to hard-pageout. Adjustment depends on the application and 
              particular installation. Having too little swap can make the system 
              run out of virtual memory before it runs out of physical memory. 
              One problem is that as memory density grows, we often won't 
              care about larger and larger amounts of memory. Looking at "wasted" 
              memory should be done with a threshold set in cost, rather than 
              megabytes.
            <p> What looks like a wasted 100 MB of memory on one hand, may also 
              be considered a cheap insurance policy on avoiding performance problems. 
              Because of the implementation of swap in Solaris, the usual rule 
              of sizing a swap device at two to three times the size of real memory 
              is no longer accurate, and these values should each be reduced by 
              one. Thus, sizing swap at one to two times real memory is a good 
              starting point.
            <p> <b>2. iowait</b>
            <p> The "iowait" statistic can be misleading on large Solaris 
              machines. In the past, certain systems required the CPU that initiated 
              an I/O to service that I/O interrupt, and this CPU was not available 
              for other processes on some systems until this I/O request was satisfied. 
              Thus, I/O-wait time came to be considered one of the states of a 
              processor -- a processor was either busy in Userland, busy in 
              kernel code, sitting in I/O-wait, or idle and therefore free to 
              run other processes.
            <p> Under Solaris there is no need for a particular CPU to service 
              the I/Os that it initiated. Any processor can service the I/O interrupt 
              and, more importantly, as soon as the I/O is initiated, the CPU 
              is free to go back to the run queue and pick up another process 
              (i.e., the CPU is idle). Any other CPU can then process the I/O 
              interrupt whenever it arrives. Therefore, my argument is that we 
              should have only three states for a processor: user, kernel, and 
              idle.
            <p> Adrian Cockcroft points out that the only reason we still have 
              <b>%wio</b> time reported is as some virtually meaningless artifact 
              that's hard to get rid off. Some people still wonder, "After 
              my I/O was initiated, how long until the interrupt was serviced?" 
              At best it's an attempt to measure the behavior of the I/O 
              hardware, but it certainly does not reflect a CPU state. Having 
              just a few outstanding I/O operations means that any idle CPUs are 
              potentially able to service the interrupts when they come. However, 
              only one CPU will handle them when they come, and there is no real 
              way to correlate the number of processes that are in the queue waiting 
              for the I/Os.
            <p> I can take an SMP with 20 processors, run a single threaded I/O-bound 
              process and <b>sar</b> will show about 95% I/O-wait time for the 
              entire system. It gives the incorrect impression that one thread 
              issuing I/O requests in a serial manner can tie up 20 processors. 
              Thus, it's best to ignore the <b>%wio</b> statistic. (There 
              is a proposed fix for the statistic, but it will still be a while 
              before that becomes standard.) I don't have enough experience 
              with non-SPARC MP systems to say whether this statistic is broken 
              on all platforms, but Adrian said, "I spoke to an HP performance 
              engineer, and he agreed that it was broken on all MP systems, including 
              HP-UX."
            <p> <b>3. Using Logging mount Options</b>
            <p> Using logging mount options is especially important on any filesystem 
              where fsck is impractical. Running fsck has limitations in both 
              the time spent and the impact it has in the case of a failure. Failures 
              can be costly because they mean recent changes to files may be lost 
              or even whole files may not be saved in "lost+found".
            <p> Logging filesystems treat metadata updates as atomic operations 
              with an intent, operation, and confirm sequence. This makes the 
              consistency check trivial; thus it doesn't take much time.
            <p> <b>4. Boosting Blocks Per Inode in Big Filesystems</b>
            <p> UNIX filesystems are general purpose, so there are many trade-offs 
              made upon initializing a filesystem. For large systems running databases, 
              some benefits can be realized by tuning the <b>newfs</b> parameters. 
              The defaults are set statically for older versions of Solaris (i.e., 
              2.8 chooses a value between 2048 and 8192). This is fine for general-purpose 
              use (e.g., average file size of around 2K to 8K, somewhat dynamic 
              in size, and capacity demands that grow and shrink regularly). When 
              creating a filesystem for a modern database computer or fileserver, 
              change the default to 500K or higher. This greatly reduces the time 
              to complete the <b>newfs</b>.
            <p> There are several options to <b>mkfs</b> and <b>newfs</b> that 
              are no longer used, are still documented, but have no effect on 
              modern disks. (One parameter that is ignored is <b>rotdelay</b> 
              or <b>gap</b>.)
            <p> <b>5. Tuning fsflush</b>
            <p> Depending on the amount of memory devoted to filesystem buffering, 
              the system process, <b>fsflush</b>, may become overloaded. Files 
              not opened with <b>O_DSYNC</b> have a policy of updating via <b>fsflush</b>. 
              The job of <b>fsflush</b> is to periodically check all buffers according 
              to two variables that can be set in the /etc/system file -- 
              <b>autoup</b> and <b>tune_t_fsflushr</b>. See Listing 1.
            <p> In kernel versions before 2.8, this scan is the only way for the 
              system to recover used memory. Creating free memory is especially 
              important when many processes are consuming memory via <b>malloc</b>, 
              <b>read(2)</b>, or <b>mmap(2)</b>-type reads. The default of 30 
              seconds for <b>autoup</b> makes the <b>fsflush</b> process too costly 
              on a big memory system.
            <p> To cool off <b>fsflush</b>, you can increase the cycle time by 
              increasing the <b>autoup</b> value, which will hit "diminishing 
              returns" above a few minutes. The impact of <b>fsflush</b> 
              is system-wide, so it is best to make it run for shorter periods 
              as well. By setting <b>tune_t_fsflushr</b> to a smaller number of 
              seconds, we make it do one fifth as much scanning (five times more 
              often than the default).
            <p> <b>6. Setting noatime on mount</b>
            <p> Sometimes we run applications on filesystems that don't need 
              the OS-level metadata part of the filesystem. The last accessed 
              time is simply not going to be used on these systems, but the default 
              <b>mount</b> option has access time update enabled, which means 
              every access to the file also has to update this value. This can 
              make a big hit on performance. Since Solaris 7, the <b>mount</b> 
              command has an option to relax the update policy on access time.
            <p> <b>7. Tuning maxcontig</b>
            <p> UFS under the Solaris operating environment uses an extent-like 
              feature called "clustering". A default setting is not 
              optimal for all filesystems since it is application-dependent. Many 
              small files accessed in a random pattern do not need extents, and 
              performance can suffer for both reads and writes when using extents. 
              Larger files can benefit from <b>prefetch</b> on reads and improved 
              allocation units when using extents in writes.
            <p> For reads, the extent-like feature is essentially read-ahead. 
              Tuning the <b>fetchahead</b> algorithm is done simply and dynamically 
              with the <b>tunefs(1m)</b> command. The value changed is <b>maxcontig</b>, 
              which sets the number of filesystem blocks read in read-ahead. When 
              a process reads more than one filesystem block, the kernel will 
              schedule the reads to fill the rest of <b>maxcontig * filesystem</b> 
              block-size bytes. One 8K (or smaller) random read on a file will 
              not trigger <b>fetchahead</b>. <b>fetchahead</b> also will not occur 
              on files being read with <b>mmap</b>.
            <p> The kernel will attempt to automatically detect whether an application 
              is doing small random or large sequential I/O. This often works 
              fine, but the definition of "small" or "large" 
              depends more on system application criteria than device characteristics. 
              Tuning <b>maxcontig</b> can obtain optimal performance.
            <p> With Solaris 2.5.1, the default <b>maxcontig</b> was always 7. 
              After that, the default changed to a device-specific value, so it 
              depends on the device and other related values:
            <p> 
            <p> Device default maxcontig<br>
              SD maxphys<br>
              SSD 1MB<br>
              vxvm stripe_width/512k<br>
              DiskSuite maxphys
            <p> Since hardware RAID (A3500 and T3) devices do <b>prefetch</b>, 
              the benefit of doing it at the filesystem level is diminished, so 
              it is best to disable or reduce <b>maxcontig</b> for these devices.
            <p> When running a transaction-processing database, usually the database 
              will not use any fetch-ahead blocks, so all blocks pulled in for 
              the read-ahead will just be adding wasted activity on the disks. 
              In this case, set <b>maxcontig</b> to 1. Conversely, for large files 
              accessed in a roughly sequential manner, a boosted <b>maxcontig</b> 
              with boosted <b>maxphys</b> can improve the transfer efficiency 
              by increasing the amount of data read once the disk heads are positioned 
              within a cylinder. The best value will be to have it as large as 
              a cylinder, around 1 MB, which translates to a <b>maxcontig</b> 
              of 128.
            <p> Values for <b>maxcontig</b> should be chosen based on the likelihood 
              of getting good data out of blocks near each other on the disk (or 
              volume) vs. the wasted I/O and (especially) memory to hold those 
              blocks if they are never used.
            <p> <b>8. Routing in the Kernel</b>
            <p> Using more than one network interface card (NIC) will, by default, 
              turn on routing in the kernel between those two subnets. By simply 
              having the file /etc/notrouter, the initialization scripts don't 
              let this happen:
            <p> 
            <pre>
touch /etc/notrouter</pre>
            <p> <b>9. Dispatch Table Tuning</b>
            <p> The dispatch table is used by the kernel scheduler to make choices 
              about which process runs when the CPUs are too busy for every process 
              to have exclusive use of a CPU. From the classic point of view, 
              a CPU-intensive process is automatically pushed to lower priority, 
              which is achieved by noting that it is still running when its time 
              quanta expires. Interactive performance is enhanced by floating 
              processes to a higher priority when they sleep (waiting for I/O) 
              before their quanta expires. This classic view is how the default 
              Sun "Time-share" dispatch table works.
            <p> On the other hand, for big database servers, this can result in 
              a problem where a CPU-intensive portion of the database holds a 
              data-structure lock as it is running at low priority. To avoid this, 
              we use higher values for the time quanta, which may be known as 
              the "Starfire" dispatch table or the "Cray" 
              dispatch table. It is automatically used in certain cases on E10000 
              and may be useful on other machines as well. This feature has been 
              implemented slightly differently on different versions of the operating 
              environment. In 2.5.1 and 2.6, the E10K has the big quanta table 
              loaded by default. In 2.7 599, the E10K has a special initialization 
              script called "S99bigquanta", which loads on boot. In 
              the 2.8 kernels, a platform-specific module is loaded, but a script 
              can also be used.
            <p> Any user can execute the command <b>dispadmin -c TS -g</b> to 
              see what the quanta are. The big quanta tables run from 400 Ms to 
              340, while the normal quanta run from 200 to 20. I often make a 
              new rc3.d script to switch between a fast quanta and a big quanta 
              via the stop and start argument. This could be useful for a system 
              using batch at night and interactive during the day.
            <p> <b>10. Fast-Write Enabled Disks on Database Logfiles</b>
            <p> As databases scale up, the pressure on the log devices can become 
              an important issue. The database has to hold up a transaction while 
              the I/O is in progress. If the log is going to plain-mirrored devices, 
              the maximum throughput will include considerable wait time as the 
              disk rotates to the heads. Use fast-write enabled disks on database 
              logfiles.
            <p> <b>11. Console Messages</b>
            <p> Don't try to troubleshoot a problem without looking at the 
              console messages. Many drivers post error information through the 
              <b>syslogd</b>.
            <p> <b>12. The Fastest Filesystem</b>
            <p> Tempfs is not always the fastest filesystem. The tempfs filesystem 
              type has been optimized for small file and metadata-intensive access. 
              Make a real <b>ufs</b> mount if you want fast sequential reads. 
              12-MB/s is typical for <b>/tmp</b>, while 150-MB/s is easily obtained 
              on re-reads of cached <b>ufs</b> data.
            <p> <b>13. Reboots</b>
            <p> Reboots due to panic can cause performance problems. Because of 
              the activity early in the boot to do the <b>savecore</b>, the memory 
              freelist may become fragmented. This can cause the shared memory, 
              ISM, not to get full allocation in 4-MB pages. One solution is to 
              start the database before the <b>savecore</b> or to <b>umount</b> 
              or direct mount the filesystem where the crash is kept.
            <p> <b>14. Disk and Volume Management History</b>
            <p> Several methods are plausible of saving disk and volume management 
              history. At a minimum, save the current configuration often:
            <p> 
            <pre>
alias newv='mv /etc/vxprint.out /etc/vxprint.out.old;
date &gt; /etc/vxprint.out ; vxprint -thr &gt;&gt; /etc/vxprint.out'
format &lt;&lt; EOT &gt; /etc/disks.format.out
EOT.
</pre>
           <p> <b>15. Copies of Crucial Files</b>
            <p> Consider the size of some files versus the critical nature of 
              customizations when saving crucial files. I keep spare or "before" 
              versions of /etc/system, partition maps of disks, <b>vfstab</b>, 
              etc. right on the server in the etc directory:
            <p> 
            <pre>
prtvtoc /dev/rdsk/c1t0d0s2 &gt; /etc/vtoc.ddisk
</pre>
            <p> <b>16. Full Install on Big Server</b>
            <p> There's no point in trying to save a few hundred megabytes 
              only to discover that you need some package that's only installed 
              on the "server" option. Do the full install on the big 
              server.
            <p> <b>17. coreadm Policy</b>
            <p> You may spend a lot of time finding core files if you haven't 
              set up a policy with <b>coreadm</b>. The recommendation is to reverse 
              the default action on saving core files by doing <b>limit coredumpsize 
              0 in /etc/.login and ulimit -c 0</b> in /etc/profile. Under Solaris 
              8, the system policy is more easily managed with the <b>coreadm</b> 
              utility.
            <p> <b>18. Tail /var/adm/messages</b>
            <p> Tail /var/adm/messages when looking for a problem. The error log 
              delivery subsystem can be managed via the <b>syslogd</b> process. 
              Messages can be routed based on severity in addition to the default 
              of going to the logfile and console.
            <p> <b>19. Pesky mount Issues</b>
            <p> If a mount fails after doing a <b>umount -f</b>, you can reinitialize 
              the mount point, making a new one: <b>rmdir mntpoint</b>; <b>mkdir 
              mntpoint</b>.
            <p> <b>20. /var/sadm/install/contents</b>
            <p> Use the /var/sadm/install/contents file to be comfortable with 
              package name and utility name relationships.
            <p> <b>21. mkfile Speed</b>
            <p> Don't expect <b>mkfile</b> to be fast if you use the small 
              <b>maxcontig</b>. The blocksize that <b>mkfile</b> uses is 8192. 
              This is too small for working on very large files without help from 
              <b>maxcontig</b>. With no option to increase the blocksize, you 
              must switch to a different utility for large file initialization. 
              <b>dd</b> works fine like this:
            <p> 
            <pre>
dd if=/dev/zero of=/misc/junk.3 bs=1024k count=100
timex mkfile 100m /misc/junk/junk.2
</pre>
            Another solution is to temporarily change <b>maxcontig</b> to 32 or 
            more when using programs that write sequential data in small blocks.
            <p> <b>22. Print man Pages</b>
            <p> Make pretty print man pages by setting the environment variable 
              TCAT to <b>/usr/lib/lp/postscript/dpost</b>, then:
            <p> 
            <pre>
man -t whatever | lp
</pre>
           <p> <b> 23. Keeping Records</b>
            <p> Install and use a permanent recorder of overall system performance 
              by extending <b>iopass</b> to make permanent records of other performance 
              and accounting information. Simply make a cron job like this:
            <p> 
            <pre>
0 0 * * * ( IFS="";trap 'kill 0' 0; vmstat 2|&amp; while read -p \
            line; do X='date +%Y.%m.%d.%H:%M:%S'; echo $X $line; \
            done) &gt;&gt; /tmp/vmstat.'date +%m%d'
			</pre>
            <b>Acknowledgments</b>
            <p> Remember, as with any word-processing software, save early and 
              save often. I would like to thank Sleepy, Grumpy, Sneezy, and most 
              of all, Prince.
            <p> 
           <p> <b> References</b>
            <p> <b>http://sunsite.net.edu.cn/online/Books/Solaris/SysAdmGuide2.pdf</b>
            <p> <b>http://docs.sun.com/ab2/coll.707.1/SOLTUNEPARAMREF</b>
            <p> <i>Bob Larson is an Engineer in the Strategic Applications 
              Engineering group within Sun Microsystems which focuses on enterprise 
              server performance and architecture. He has over 12 years experience 
              in performance tuning, application and kernel development, and benchmarks.</i>
          </table></table>&nbsp;

<! -- End Content ------ >

<!/center>
<! -- End MASTER TABLE -- >

</body>



<! -- Begin Content ------ >
</html>
