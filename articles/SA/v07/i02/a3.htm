<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 4.0//EN">


<HTML>
<HEAD>
<TITLE>v07, i02: File Replication</TITLE>
<LINK REL=StyleSheet HREF="../../resource/css/sacdrom.css" TYPE="text/css" TITLE="CSS1">
</HEAD>

<body bgcolor="#ffffff" text="#000000" link="#990000" alink="#669999" vlink="#333366" topmargin=0 leftmargin=0>

<! -- Begin MASTER TABLE -- >
<!center><table width=600 cellpadding=0 cellspacing=0 border=0 bgcolor="#ffffff">

<tr>

<table cellpadding=5 cellspacing=0 border=0>
	<tr>

		<td><span class=navbarLink>Article</span></td>
		<td><span class=navbarLink><a href="a3_l1.htm">Listing 1</a></span></td>
		<td><span class=navbarLink><a href="a3_l2.htm">Listing 2</a></span></td>
		<td><a href="../../../../source/SA/1998/feb98.tar"><b class=codeListing>feb98.tar</b></a></td>

	</tr>
</table>


</tr>
<tr>
<! -- Begin Content ------ >
<td valign=top width=440 bgcolor="#ffffff">
<table width=100% cellpadding=15 cellspacing=0 border=0>
<tr><td valign=top>



<H1>File Replication<p>
</H1><p>Jim McKinstry
<p>
Today, it's a given that UNIX systems need to share data across a network. A less common requirement is to have copies of data on multiple servers. The most common solution for sharing data is NFS. However, there is no widely accepted way to provide copies of data across the network. One possible solution is to replicate the data periodically to meet your needs.<p>
While NFS provides current, real-time access to data on another server, replicated data will lag behind the original by minutes or hours. This is not a problem since replicated data is usually not used for real-time processing. Combined with Replication Server from Sybase, I have seen file replication provide an effective way to off-load defect correction tasks and report generation to a non-production server allowing the production server to run more efficiently. File replication is also ideal for off-loading production data that has been archived. Most importantly, critical system configuration files (<B>vfstab</B>, <B>crontab</B>, volume configuration, etc.) can be replicated to other servers so that the information is recoverable in case of a failure.<p>
Originally, this file replication solution was written in C and used pipes. An associate hacked the solution out of a book and tweaked it to work on our systems. When it stopped working, I was asked to fix the problem. After a couple of days of debugging, I decided to rewrite the solution so that people with minimal coding experience would be able to maintain it. This version of the software has been running successfully in a production shop for more than 6 months and should be very easy to maintain. It is much less complex, much more reliable, and actually faster than the original C implementation. This article describes my solution.<p>
<B>The Code</b><p>The server does the majority of the work and controls the flow of the replication process. The clients' actions are based on which files the server sends. The process is invoked from the server machine by roots' <B>cron</B> but may be run from the command line or other shells. Here's a sample <B>cron</B> entry on the server that replicates the data in our archive directories to the client:<p>
<pre>
0 22 * * * /replication/replication_server.sh \
/replication/data/archive_dirs client_1 \
&gt;/dev/null 2&gt;&amp;1
</pre><p>
<p>$1</B> is the file that contains the list of directories to be replicated (<B>/replication/data/archive_dirs</B>). <B>$2</B> is the name of the machine to replicate to (<B>client_1</B>).<p>
<B>$1</B> must list each directory to be replicated. Subdirectories are not replicated unless they are listed. <B>/replication/data/archive_dirs</B> follows:<p>
<pre>
/archive
/archive/199701
/archive/199702
/archive/199703
/archive/199704
/archive/199705
/archive/199706
/archive/199707
</pre><p>
</b><p>The easiest way to create a file containing a list of directories to be replicated is to do a <B>find</B> with a <B>-type d</B> option on the topmost node of the directory structure that you want replicated. The output can then be edited to remove any directories that you don't want replicated. Here's how I created <B>/replication/data/archive_dirs</B>:<p>
<pre>
find /archive -type d -print &gt; /replication/data/archive_dirs
</pre><p>Replication starts with the server generating the list of files on itself and on the client for the first directory listed in <B>$1</B>. These two lists are compared to see which files are only on the server and which files are only on the client. The files only on the server need to be sent from the server to the client. Files and directories only on the client need to be removed from the client. The files that need to be sent to the client are <B>cpio</B>'d into a compressed file and sent to the client. The list of files that need to be removed on the client is also sent to the client. The client uncompresses and un-cpio's the file sent from the server and also removes each file listed in the list of files to be removed that was sent from the server. Each directory listed in the list of directories to be replicated is processed in this manner until all have been replicated. Note that there are some C files included in this solution. I do not review them in great detail, but they are discussed in the Security section.<p>
<B>replication_server.sh<p>
replication_server.sh</B> is a pretty simple Bourne script, and could easily be rewritten as Perl, C, etc. I chose Bourne because it's extremely portable and easy to maintain. (All listings for this article can be found at: <B>ftp.mfi.com</B> in <B>/pub/sysadmin</B>.) After setting up some variables, the inputs are checked for validity. The file passed in must exist, and the client machine must be known to the server (i.e., must be in <B>/etc/hosts</B>). If either input is bad, then an error is logged and the script exits. If the inputs look good, then it loops through the list of directories that are in the file passed as <B>$1</B>. For each valid directory listed, an <B>ls -an</B> is executed on the server and the files are sorted. I ignore <B>.</B>, <B>..</B>, <B>lost+found</B>, the "total" line, and <B>core</B> files. The same is done on the client by invoking a C program, which runs as <B>replid</B> and does a <B>remsh</B> of a C program that executes an <B>ls -an</B> as root on the client machine. By using <B>ls -an</B>, I can check mode, number of links, owner, group, size in bytes, and time of last modification for each file with one command. If your systems use a lot of hard links, then you may want to cut out the number of links field. Once I have the sorted lists of files from the client and server, I do a <B>comm</B> on them:<p>
<pre>
comm -23 ${SERVER_FILE_LIST}.sorted \
${CLIENT_FILE_LIST}.sorted \
&gt; ${CPIO_LIST_FILENAME}.long
</pre><p>This <B>comm</B> command generates a list of files that are on the server and are either not on the client or are different (size, date, etc.) on the client. This list is still in <B>ls -an</B> format. All I need is the file name, so I create the list of filenames with this <B>awk</B> routine:<p>
<pre>
awk '{print $9}' ${CPIO_LIST_FILENAME}.long &gt; ${CPIO_LIST_FILENAME}
</pre><p>
</b><p>Next, I figure out which files need to be removed on the client side. I could have repeated the <B>comm</B> from above using <B>-13</B> instead of <B>-23</B>. This would have worked for everything except directories. It's been my experience that two directories can have identical contents, but their sizes can legitimately show up as being different. I don't want to remove a whole directory structure because of this idiosyncrasy. This can be avoided by comparing the file names when determining which files need to be deleted. Instead of repeating an <B>ls</B> on the server and across the network on the client, I use the <B>awk</B> routine above to create lists of files on the client and server from the original <B>ls -an</B> outputs. I've found that the <B>awk</B> routine is much faster than running the <B>ls</B> over the network and at least as fast as running the ls on the local machine. In fact, when dealing with hundreds or thousands of files in a directory, the <B>awk</B> is significantly faster than <B>ls</B> in all cases. These lists of files are then sorted, and <B>comm</B> is run against them to get the list of files to be deleted on the client:<p>
<pre>
comm -13 ${SERVER_SHORT_LIST}.sorted ${CLIENT_SHORT_LIST}.sorted &gt;
${REMOVE_LIST_FILENAME}
</pre><p>At this point, I have the list of files that need to be copied from the server to the client and the list of files that need to be deleted on the client. I now need to get the files from the server to the client while preserving the files' attributes and not overwhelming the network. I found that using <B>cpio</B> combined with <B>gzip</B> works well to package the files to be sent to the client. The <B>if</B> keeps me from trying to send no files to the client. (<B>gzipping</B> an empty file produces a 3-byte file, which would then be sent to the client, un-<B>gzipped</B>, and un-<B>cpio</B>'d.)<p>
<pre>
if [ -s ${CPIO_LIST_FILENAME} ]
then
cat ${CPIO_LIST_FILENAME} | \
cpio -ovc | /usr/local/bin/gzip \
&gt; ${CPIO_FILENAME}
fi
</pre><p>If you are low on disk storage or network bandwidth, you may want to use <B>gzip</B> with a <B>-9</B> flag for the best compression. If you have plenty of disk space and network but are low on CPU power, you may want to remove the <B>gzip</B>. This version of <B>replication_server.sh</B> does not compress the "list-of-files-to-be-deleted" file before sending it to the client. If you find that this file is frequently large, you may want to compress it before sending it to the client.<p>
I use <B>ftp</B> to transfer the compressed <B>cpio</B> file and the "list-of-files-to-be-deleted" file to the client. To do this I was forced to keep the password for the replication ID (<B>replid</B>) in a file. I didn't see any way around this, so the password is stored in the <B>TO_PASSWD</B> variable at the top of this script. This script is owned by root, and its permissions are 100 so that no one will have access to the password.<p>
I only <B>ftp</B> the files if they have data in them. There is no need to open an <B>ftp</B> session if there is no data to send.<p>
<pre>
if [ -s ${CPIO_FILENAME} ]  # If there are files to copy
then                      # then send them to the
# remote machine
ftp -n -v ${TO_NODE}  &gt;&gt; ${LOG_FILE} &lt;&lt;  EOL
user ${TO_ID} ${TO_PASSWD}
bin
put ${CPIO_FILENAME}
quit
EOL

</pre><p>After the <B>ftp</B>, the script (<B>check_ftp.sh</B>) checks whether the files were successfully transmitted then continues appropriately. If there were files to be copied or removed, then it calls the client process on the client machine to process the files as needed. This is executed in the background so that processing can continue on the next directory to be replicated. The server should not sit idle while the client works.<p>
<pre>
if [ ${COPY_FILE_COUNT} -ne 0 -o \
${REMOVE_FILE_COUNT} -ne 0 ]
then             # If files were sent to be copied/removed
# then process them
/admin/shell/replication/process_cpio_file \
${CPIO_FILENAME} ${DIRECTORY} \
${REMOVE_LIST_FILENAME} \
${TO_NODE} \
${LIST_OF_DIRECTORIES} \
&gt; /dev/null 2&gt;&amp;1 &amp;
fi
</pre><p>If your client machine is being overwhelmed by the server, you can call the client process in the foreground so that it only works on one directory at a time.<p>
<B>check_ftp.sh<p>
check_ftp.sh</B> is used to check whether a file was successfully transferred between the client and the server. It does this by comparing the results of a <B>sum</B> of the file on the client and server. Running a <B>sum</B> on a large file twice (once on the client and once on the server) can be very time consuming. If you are confident in your network's reliability or you have a third party replacement for <B>ftp</B> that guarantees the data, you may want to remove the calls to this shell. The guts of it are shown in Listing 1.<p>
<B>replication_client.sh<p>
replication_client.sh</B> is the client part of the replication process. All of the complexity and logic needed to decide what should be replicated and deleted takes place on the server. All the client does is copy or delete what it has been sent. The inputs are:<p>
<p>
<B>$1</B> - Name of the <B>cpio</B>/<B>gzipped</B> file<br>
<B>$2</B> - Name of the directory being replicated<br>
<B>$3</B> - List of files to be deleted<br>
<B>$4</B> - Name of file that contains the list of directories to be replicated<p>
<p>
This script is straightforward, and the interesting parts are shown in Listing 2.<p>
<B>Security</b><p>The cost of a simple, automated, portable replication solution is security. I am not too concerned about the security "holes" that this solution may introduce to a system. It has been my experience that with the proper measures in place, there are no security problems. Each user of this code must decide if the means justify the gains. I will address the following three security concerns:<p>
<ul>
<li>The use of the "r" services<br>
<li>A password stored in a file<br>
<li><B>setgid</B>/<B>setuid</B> C routines<br>
</ul><p>
It is often recommended that the "r" services (<B>remsh</B>, <B>rlogin</B>, etc.) be turned off as a security precaution. While nothing is more secure than turning the services off, I have found that using TCP Wrappers is an acceptable alternative. Turning on <B>remsh</B> (<B>rsh</B>, <B>rexec</B>, etc.) and configuring TCP Wrappers to allow only your trusted servers access is pretty safe. In conjunction with that, I also have defined no shell for the replication ID. This essentially keeps anyone from logging in with that ID. Here's my entry from <B>/etc/passwd</B> for <B>replid</B> (<B>replid</B> is also in its own group):<p>
<pre>
replid:x:1234:1234: FILE REPLICATION ID:/replication:
</pre><p>Note that the home directory is the directory in which all of the replication software is stored. This directory is owned by root with permissions 111. As an extra precaution, I also run a nightly <B>cron</B> that searches for any <B>.rhosts</B> that are not in the replication ID's home directory and alerts me through mail so that I can investigate.<p>
Storing a password in a file is painful at best. Unfortunately, it is a necessary evil when automating <B>ftp</B>. To protect this password, the permissions on the file are 100, and its directory has permissions of 111. The directory, and all of the files in it, are owned by root, and the group is the replication group. With permissions of 111 on the home directory and no login shell, there's not much someone could do even if they were to learn the password for the replication ID.<p>
The C routines were written so that I wouldn't need a <B>.rhosts</B> file for root. Essentially, the C routines allow <B>replication_server.sh</B> to issue <B>remsh</B> commands as <B>replid</B> to the client and have those commands run as root on the client. While gathering the <B>ls -an</B> information from the client, for example, <B>replication_server.sh</B> calls <B>client_ls-n</B>. <B>client_ls-n</B> sets its UID to <B>replid</B> and calls <B>client_ls-n.sh</B>. <B>client_ls-n.sh</B> makes the <B>remsh</B> call to the client. The <B>remsh</B> executes <B>ls-an_client</B> on the client, which runs as <B>replid</B>. <B>ls-an_client</B> sets its UID to root and runs <B>ls-an_client.sh</B> to gather the <B>ls -an</B> information. Running as root avoids permission problems when gathering the list of files. A similar set of calls is performed to call <B>replication_client.sh</B>. The C source modules have permissions of 000 and are owned by root. The group ownership is the replication group, and they are held in a secure directory. The executables have permissions of 4110. SETUID is turned on so that the replication ID can execute it as root. If you are not concerned about having a <B>.rhosts</B> file for root, then you can get rid of the C code and issue the <B>remsh</B> commands directly from the main scripts.<p>
<B>Reliability</b><p>Large replications (hundreds of files, hundreds of megs of data) occur nightly across a LAN and a WAN without incident. If you have problems with the <B>ftp</B>s failing, you may want to put each <B>ftp</B> in a loop that will either loop until each <B>ftp</B> is successful or fail after a certain number of retries. Currently only one <B>ftp</B> attempt is made. If this attempt fails, root is notified by email. You can then re-run it manually, or let the next replication catch it.<p>
<B>Setup</b><p>Some setup is required to use this software. A dedicated replication ID with no login shell in its own dedicated replication group should be added to each system. The code, as well as a <B>.rhosts</B> file containing the appropriate information, should be put on each machine in the replication ID's home directory. Lock down the home directory as described above. <B>PGNAME</B> and <B>USERID</B> should be set up appropriately in the C modules and should be recompiled (use <B>compall.sh</B> to compile and set up the recommended permissions).<p>
Begin with small amounts of data when implementing this software. Also, set up some test directory structures to experiment with until you get a feel for how it is going to work on your systems. Be very careful, this solution will remove entire directory structures on the client. You should always have a complete system backup before testing.<p>
<B>Conclusion</b><p>File replication has proven to be invaluable to our department. Our Disaster Recovery solution depends heavily on replication. It has cut down recovery time from system outages by allowing us to restore critical information (volume configuration, disk configuration, etc) from another server as opposed to restoring from tape or starting from scratch. We have also been able to off-load all of our development activities and batch reporting to another server by providing hourly replication of production files. Production customers' response time has been greatly improved. I hope you will be able to make use of this software. n<p>
<p>





<h1>About the Author</h1><p>










Jim is a Technical Analyst specializing in UNIX. He has worked for IBM, Rite Aid and EDS and is currently working for Sprint Paranet. He can be reached at <B>jrmckins@yahoo.com</B>.





















<br>&nbsp;<br>
</table></center></td>
<! -- End Content ------ >

</table><!/center>
<! -- End MASTER TABLE -- >

</body></html>