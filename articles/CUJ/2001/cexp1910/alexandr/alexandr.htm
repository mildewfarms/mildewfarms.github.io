<HTML>   
     <HEAD>
<TITLE>October 2001 C++ Experts Forum/Generic&lt;Programming&gt;</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../../19.10/tococt.htm"></A><FONT COLOR="#FF0000">   C++ Experts Forum</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">Generic&lt;Programming&gt;: Typed Buffers (II)</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Andrei Alexandrescu</FONT></H3>

<HR>
<BLOCKQUOTE>

<p>In a classic soap-opera style, let's start with highlights from the previous episode. We sketched a class template <b>buffer</b> that is much like <b>std::vector</b>, except that <b>buffer</b> does not have the notion of capacity and that it adds a couple of primitive functions, such as <b>grow_noinit</b> and <b>shrink_nodestroy</b>. In addition, the previous installment talked about type traits as an enabling technology for optimization. Finally, the villain threatened to talk about copying objects and memory allocation. </p>
<p>This article does not treat buffers directly, but rather two operations that you commonly perform with buffers: filling a buffer with a value and copying between buffers and various containers. We will compare with numbers several filling and copying methods.</p>

<H3><FONT COLOR="#000080">Filling</FONT></H3>

<p>You know &#151; copy the same value to all objects in a range. The C++ Standard library provides two filling functions: <b>std::fill</b> and <b>std::uninitialized_fill</b>. The second one assumes the values to fill have not been initialized at all. </p>
<p>The simplest implementation of a generic memory filling function might look like this:</p>

<pre>
// Example 1: A simple generic filling routine
template &lt;typename T&gt;
void Fill(T* beg, T* end, const T&amp; value)
{
  for (; beg != end; ++beg) *beg = value;
}
</pre>

<p>The question is, is this the best implementation money can buy? The stock answer is &quot;it's the compiler's responsibility to generate optimal code&quot; &#151; so I set out to test this. </p>
<p>First, I looked at the code for <b>Fill</b> generated by Microsoft Visual C++ 6.0 (MSVC) and Metrowerks CodeWarrior 6.9 (MWCW). They both generate a straight loop in assembler. However, x86, like many modern processors, offers special assembler instructions for quickly filling chunks of memory. The C library function <b>memset</b> is likely to make use of them. Unfortunately, <b>memset</b> is able to set memory only to the same byte; as long as you have to set the memory to any pattern that's longer than one byte, you can't use <b>memset</b> anymore, so <b>memset</b> doesn't &quot;scale&quot; to generic code.</p> 
<p>(Let me make an aside here. There is something sacrosanct about C's memory manipulation functions <b>memset</b>, <b>memcpy</b>, and <b>memcmp</b>. They are likely to be highly optimized by the compiler vendor, to the extent that the compiler might detect calls to these functions and replace them with inline assembler instructions &mdash; this is the case with MSVC. Consequently, using <b>mem*</b> functions is considered cool style by speed-hunting programmers.)</p> 
<p>One idea that comes to mind is to implement filling in terms of copying. That is, you can exploit the fact that, once you partly filled the target range, you can copy the already filled part to the unfilled part using a fast copy routine &#151; and voil&agrave! The cool part is that you can double the size of the chunk that's copied every pass. For example, say you want to fill a range of 1,000 numbers of type <b>double</b> with <b>1.0</b>. First step, you assign <b>1.0</b> to the first slot. Second step, you copy the just assigned slot to the slot next to it. Third step, you copy the two newly assigned slots to the next two adjacent slots. Fourth step, you copy four values and you get eight &#151; and so on. In 10 steps, you filled the entire range of 1,000 <b>double</b>s. The bulk of the job is actually done in the last step, when 512 slots have already been filled, and 488 of them are copied into the 488 remaining slots. Assuming you benefit from a fast copying routine with the signature:</p> 

<pre>
template &lt;typename T&gt;
void QuickCopy(const T* sourceBegin, 
  const T* sourceEnd, T* destination);
</pre>

<p>then the <b>FillByCopy</b> algorithm looks as like:</p>

<pre>
template &lt;typename T&gt;
void FillByCopy(T* beg, T* end, const T&amp; value)
{
  if (beg == end) return;
  *beg = value;
  T* dest = beg + 1;
  for (;;)
  {
    const size_t alreadyFilled = dest &#151; beg;
    if (alreadyFilled &gt;= end &#151; dest)
    {
      QuickCopy(beg, beg + (end &#151; dest), dest);
      break;
    }
    else
    {
      QuickCopy(beg, dest, dest);
      dest += alreadyFilled;
    }
  }
}
</pre>

<p><b>FillByCopy</b> is a cool algorithm if <b>QuickCopy</b> is indeed a fast copy routine. <b>FillByCopy</b> is somewhat similar to the Russian Peasant algorithm for computing the integral power of a number with minimal multiplication <a href="#1">[1]</a>. Many people invented fill-by-copy in a variety of situations &#151; one that comes to mind is filling a whole hard disk starting with a one-byte file.  Were it an original idea, I would have hastened to call filling by copy &quot;The Romanian Peasant&quot; algorithm.</p> 
<p>So I excitedly put in place a test, and I got interesting results. But first, let me introduce another contender to you.</p>

<H3><FONT COLOR="#000080">Duff's Device</FONT></H3>

<p>Duff's Device <a href="#2">[2]</a> is a C coding trick for speeding up loops. The basic idea is that if, in a <b>for</b> loop, the operation performed is cheap enough (such as, um, an assignment) &#151; then testing the loop condition (<b>beg != end</b> in Example 1 above) occupies a large percentage of the time spent in the loop. Then, the loop ought to be partially unrolled so that several operations are done in a shot, and the test is done less frequently. For example, if you fill a range of objects, you may want to assign to two or more consecutive objects in a shot inside the loop.</p> 
<p>You must pay attention to the details of limit conditions, etc. Here's where Duff's Device is an original, creative solution. Let's take a quick look at a Duff's Device-based implementation of a generic filling algorithm:</p> 

<pre>
template &lt;class T&gt; inline void FillDuff
  (T* begin, T* end, const T&amp; obj)
{
  switch ((end - begin) &amp; 7)
  {
  case 0: 
    while (begin != end)
    {
      *begin = obj; ++begin;
  case 7: *begin = obj; ++begin;
  case 6: *begin = obj; ++begin;
  case 5: *begin = obj; ++begin;
  case 4: *begin = obj; ++begin;
  case 3: *begin = obj; ++begin;
  case 2: *begin = obj; ++begin;
  case 1: *begin = obj; ++begin;
    }
  }
}</pre>

<p>Now, if you haven't seen it before, go back and look <i>again</i> because it's possible you missed something. The function consists of a <b>switch</b> statement, whose labels land the execution path in a precise spot inside of a <b>while</b> loop (or, in one case, outside). The expression inside the <b>switch</b> computes the remainder of dividing the number of objects by eight. Depending on the remainder, execution starts upper or lower in the <b>while</b> loop and falls through from there. (There's no <b>break</b>.) This way Duff's Device solves the margin condition neatly and concisely. By the way, why is the &quot;<b>case 0:</b>&quot; label outside the loop, breaking a nice symmetry? The only reason is handling empty sequences. When the remainder is zero, an extra test is performed to weed out empty sequences. It all falls in place quite nicely.</p> 
<p>The net result is that the test <b>begin != end</b> is performed eight times less frequently. Therefore, the contribution of the test itself to the duration of the loop was slashed by a factor of eight. Of course, you can experiment with other factors as well.</p>
<p>The possible inefficiencies of Duff's Device stem from code bloating (some processors tend to execute tight loops better than larger loops) and its unusual structure. Optimizers might be built to say &quot;aha!&quot; when the familiar straight loop structure comes in sight, but they might be bewildered and generate conservative code when they see something trickier. </p> 

<H3><FONT COLOR="#000080">Numbers</FONT></H3>

<p>If there is one thing to know about optimization (after you're past the &quot;don't do it&quot; and the &quot;don't do it yet&quot; phases), that thing must be: optimizations must be validated by measurements. The filling algorithms described above might sound ingenious, but only testing can validate them.</p> 
<p>I wrote a simple test program that fills an array of <b>COUNT</b> variables of type <b>double</b> using one of the three algorithms described above: the <b>for</b> loop, fill-by-copy, and Duff's Device. The test is repeated <b>REPEAT</b> times. I tested each algorithm several times on a P III/750 MHz with three compilers: MSVC, MWCW, and GNU's g++ 2.95.2. By varying <b>COUNT</b> and <b>REPEAT</b>, I got the following results:</p>

<UL>
<LI>When filling a large buffer (<b>COUNT</b> 100,000 and beyond), the straight <b>for</b> loop and Duff's Device behave practically the same. The fill-by-copy algorithm actually performs <i>worse</i> by 1-5% <a href="#3">[3]</a>. </LI>
<LI>When filling ranges of 10,000 doubles, <b>FillByCopy</b> on MSVC and MWCW was faster by 23%, but g++ was most comfortable with the <b>for</b> loop, yielding the best results by far (20%-80%) across all compilers and methods.</LI> 
<LI>When filling ranges of 1,000 <b>double</b>s, MSVC and MWCW generated similar results: Duff's Device is 20% faster, and fill-by-copy is 75% faster, compared to the straight <b>for</b> loop. Again, g++ had a separate opinion and yielded amazingly faster results for all methods (100% faster than the competition's best).</LI>
<LI>For 100 <b>double</b>s, the results stayed about the same for MSVC and MWCW. Again, g++ jovially performed the same task in roughly half the time (Duff's device and fill-by-copy being 12% faster than the <b>for</b> loop).</LI>
</UL>

<p>We find the explanation of this behavior by looking at the architecture of current PCs. The processor is 5-10 times faster than the main memory bus. There are two levels of cache to enhance speed of memory access: one is right inside the processor (Level I), and the other is next to the processor (and comes in the same package in Pentium III). </p>
<p>In the best case, all memory manipulated in one operation fits in the Level I cache. In the worst case, you make random, scattered accesses, so you keep missing the cache and end up hitting on the main memory.</p>
<p><b>FillByCopy</b> is cache-unfriendly, because in each pass it hits two memory areas at a time &#151; the source and the destination area. If you, for example, fill 1 MB worth of data, the last step will copy 512 KB from one location to another in memory. This will make the cache manager unhappy, or at least less happy than in the case of a straight filling loop. That's why <b>FillByCopy</b> is slightly slower than a <b>for</b> loop for filling large memory chunks.</p> 
<p>(Exercise: There is a simple change you can make to one line of code of <b>FillByCopy</b> to improve its cache friendliness. Hint: think of locality of access.)</p>
<p>When filling <i>large</i> amounts of data, you can't benefit too much from caching, so the filling speed will saturate at the main memory bandwidth. The optimization you bring to the loop itself does not bring much improvement because the bottleneck is dictated by the memory, not by processor activity. No matter how smart you write the loop, use registers, or unroll, the processor will wait for the main memory anyway. That's why Duff's Device and the <b>for</b> loop perform the same for large memory chunks.</p> 
<p>When filling <i>smaller</i> amounts of data, the landscape changes. The data is much more likely to fit in the cache, which is as fast as the processor. Now, the code executed by the processor dictates the speed of the loop. </p>
<p><b>memcpy</b> (the routine ultimately used by <b>FillByCopy</b> in my test case) uses a special looping assembler statement (<b>rep stos</b> in x86 jargon). For cache-to-cache copying, the assembler statement is faster than a loop built out of jumps, assignments, and comparisons. That's why <b>FillByCopy</b> is fastest for medium amounts of data. Likewise, Duff's Device also has an edge over the <b>for</b> loop because it performs less comparisons.</p> 

<H3><FONT COLOR="#000080">Quick Copying</FONT></H3>

<p>Another common buffer operation is to copy data to and from buffers. In a quest for a fast copy routine, I tested three variants with data of type <b>double</b>: a straight <b>for</b> loop, a Duff's Device-based implementation, and <b>memcpy</b>. (Oddly enough, although there is a fill-by-copy algorithm, a copy-by-fill algorithm does not exist.) </p> 
<p>There's not a lot of competition to expect &#151; <b>memcpy</b> ought to leave everything else in the dust. After all, <b>memcpy</b> is <i>the</i> highly optimized, finely tuned copy method provided by your standard library implementer. Too bad it doesn't apply to all types! Then Duff's Device should compare to the <b>for</b> loop pretty much the same as for the filling case. Of course, as it often happens, reality turns out to be a bit different. The results were as follows:</p>

<UL>
<LI>When copying large amounts of data (megabytes), the speed is about the same for all methods (and all compilers). This is, again, because for large amounts of data the main memory bandwidth dictates the operating speed.</LI>
<LI>When copying a smaller quantity of data, the compilers start disagreeing. For example, when copying 100,000 objects of type <b>double</b>:</LI>
<UL><LI>MWCW's <b>for</b> loop was pretty slow; Duff's Device and <b>memcpy</b> both bypassed it by 20%. </LI>
<LI>MSVC and g++ were equal-opportunity code generators: all methods performed about the same &#151; and about as good as MWCW's best. </LI></UL>
<LI>When decreasing the quantity of data copied, divergence becomes deeper. Below are the results for copying 10,000 <b>double</b>s:</LI>
<UL><LI>MSVC generated 25% faster code for Duff's Device and 67% faster code for <b>memcpy</b>.</LI>
<LI>MWCW (be prepared) generated 9% faster code for Duff's Device and 20% <i>slower</i> code for <b>memcpy</b>, compared to a <b>for</b> loop. The <b>for</b> loop speed was about the same as MSVC's.</LI>
<LI>g++ was really cool. For starters, g++'s <b>for</b> loop was 42% faster than MSVC's and MWCW's. Then, <b>memcpy</b> performed as good as MSVC's <b>memcpy</b>, and 10% faster than g++'s <b>for</b> loop. But g++ implementers must love Duff's Device because the Duff-based copy blew away all competition, performing 11% better than <b>memcpy</b>, which is supposed to be <i>la cr&egrave;me de la cr&egrave;me</i>!</LI></UL>
<LI>When decreasing the amount of data to 1,000 <b>double</b>s:</LI> 
<UL><LI>MSVC's <b>for</b> loop was really slow, Duff was 90% faster, and <b>memcpy</b> was 200% faster.</LI>
<LI>MWCW generated a better <b>for</b> loop, and then Duff performed 5% faster and <b>memcpy</b> performed 130% faster.</LI>
<LI>g++ generated a <b>for</b> loop as good as MWCW's. Duff was 75% better, and  <b>memcpy</b> was 130% better.</LI>
<LI>All compilers yielded similar, and best, results for <b>memcpy</b>. </LI></UL>
<LI>And, finally, when copying 100 <b>double</b>s, I got similar results as with 1,000 <b>double</b>s, except that g++ preferred Duff's Device again, which was speedier by 75% than the <b>for</b> loop, and 40% than <b>memcpy</b>.</LI>
</UL>

<p>If you are confused, so am I. It seems hard to decide which method is best across all compilers and all data amounts. It also came as a surprise to me that a freely available, open-source compiler not only bypassed, but crushed, two respected commercial compilers.</p>
<p>All these measurements make me envious of compiler-targeted library writers &#151; they have the luxury of optimizing their code for <i>one</i> compiler and <i>one</i> computer architecture. However, this column's title is &quot;Generic &lt;Programming&gt;,&quot; not &quot;Specific&lt;Programming&gt;&quot; &#151; which makes it all the more challenging and interesting. By the way, let's now talk about generic in &quot;generic programming&quot; &#151; how generic are the routines we just talked about?</p> 

<H3><FONT COLOR="#000080">Considerations on Genericity</FONT></H3>

<p>So far, we have assumed that the type being tested can be copied with <b>memcpy</b>. In reality, only a subset of C++ types has this property. That subset is called POD (&quot;Plain Old Data&quot;). The POD set includes all primitive types and C-like structures with only public member variables, no constructors and destructors, and no virtual functions. For all other types, the effect of copying them with <b>memcpy</b> is undefined. </p> 
<p>This gives Duff's Device an important advantage over <b>FillByCopy</b> and <b>memcpy</b>: Duff's Device is 100% &quot;portable&quot; across types.  Better yet, Duff's Device works not only with in-memory ranges iterated with pointers, it works with any random iterator type.</p>
<p>It is important, however, to figure out if you can copy a type with <b>memcpy</b> This is very interesting not only for speed purposes, but also, paradoxically, for allocation purposes, as you'll see in the next installment. Defining a <b>MemCopyable</b> flag is a matter of opening the <b>TypeTraits</b> namespace (defined in the previous installment) and planting a definition like this:</p> 

<pre>
namespace TypeTraits
{
  template &lt;typename T&gt; struct IsMemCopyable
  {
    enum { value = IsPrimitive&lt;T&gt;::value != 0 };
  };
}
</pre>

<p>Then you can easily write a <b>NativeCopy&lt;T&gt;</b> generic routine that dispatches at compile time to <b>memcpy</b> or to a more conservative method, depending on <b>TypeTraits::IsMemCopyable&lt;T&gt;::value</b>. This task is left as an exercise to the reader.</p>
<p>Duff's Device works well only when the type to copy/fill is cheap enough to copy/fill. Otherwise, the comparison's cost becomes negligible in (here's a pun for you) comparison to the assignment itself. So we need to define a type trait <b>CheapToCopy</b> as shown below:</p>

<pre>
namespace TypeTraits
{
  template &lt;typename T&gt; struct CheapToCopy
  {
    enum { value =
      CopyThrows&lt;T&gt;::value != 0 &amp;&amp; sizeof(T) &lt;=
      4 * sizeof(T*) };
  };
}</pre>

<p>Interestingly, <b>CheapToCopy</b> defaults to a little Sherlock Holmesism: a type is considered cheap to copy if its copy operation does not throw (which often implies that no costly resources are allocated) and its size is below a threshold. If a type is more than four times bigger than a pointer, that means that the loop is kind of unrolled already.</p>
<p>Using <b>TypeTraits::CheapToCopy&lt;T&gt;::value</b>, you can decide at compile time between using Duff's Device or something else. </p>

<H3><FONT COLOR="#000080">Conclusion</FONT></H3>

<p>Things are clearly hazy, aren't they? First off, maybe it came as a surprise to you that there's more than one way to fill and copy objects. Then, there's no single variant of fill and copy that works best on all compilers, data sets, and machines. (I guess if I tested the same code on a Celeron, which has less cache, I would have gotten very different results. To say nothing about other architectures.)</p>
<p>As a rule of thumb, it's generally good to use <b>memcpy</b> (and consequently fill-by-copy) if you can &#151; for large data sets, <b>memcpy</b> doesn't make much difference, and for smaller data sets, it might be much faster. For cheap-to-copy objects, Duff's Device might perform faster than a simple <b>for</b> loop. Ultimately, all this is subject to your compiler's and machine's whims and quirks.</p>
<p>There is a very deep, and sad, realization underlying all this. We are in 2001, the year of the Spatial Odyssey. We've done electronic computing for more than 50 years now, and we strive to design more and more complex systems, with unsatisfactory results. Software development is messy. Could it be because the fundamental tools and means we use are low-level, inefficient, and not standardized? Just step out of the box and look at us &#151 after 50 years, we're still not terribly good at filling and copying memory.</p>
<p>I'm approaching the word limit for this article without exhausting these buggers, sorry, buffers. Moving objects is another important topic, and we haven't even started talking about memory allocation (the Caudine Forks, remember?). But, as a movie script says, the mind can only enjoy what the butt can endure, so I'll have to stop here for now. See you soon.</p>

<H3><FONT COLOR="#000080">Bibliography and Notes</FONT></H3>

<p><a name="1"></a>[1] Matt Austern. &quot;The Russian Peasant Algorithm,&quot; <i>C++ Report</i>, June 2000. Matt didn't invent the algorithm, but the article gives a very good description of it.</p>
<p><a name="2"></a>[2] See &lt;www.lysator.liu.se/c/duffs-device.html&gt;.</p>
<p><a name="3"></a>[3] It must be said here that g++ must be using some quantum effect for filling data, an effect that works only sometimes. In repeated tests with filling 10 million <b>double</b>s, Duff's Device compiled by g++ <i>sometimes</i> unexplainably yielded 40%-50% faster results than anything else on Earth. Frailty, thy name is speed measuring...</p>


<p><i><b>Andrei Alexandrescu</b> is a Development Manager at RealNetworks Inc. (&lt;www.realnetworks.com&gt;), based in Seattle, WA, and author of the acclaimed book </i>Modern C++ Design<i>. He may be contacted at <b>andrei@metalanguage.com</b>. Andrei is also one of the featured instructors of The C++ Seminar (&lt;www.gotw.ca/cpp_seminar</a>).</p>

</BLOCKQUOTE></BODY></HTML>
