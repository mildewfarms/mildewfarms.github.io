<HTML>   
     <HEAD>
<TITLE>November 2001/MAPM, A Portable Arbitrary Precision Math Library in C</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../tocnov.htm"></A><FONT COLOR="#FF0000">   Engineering and Numerical Programming</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">MAPM, A Portable Arbitrary Precision Math Library in C</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Michael C. Ring</FONT></H3>

<BLOCKQUOTE>
<p>Frustrated by the finiteness of fixed-size arithmetic? This math library gives you the precision you need.</p>
</BLOCKQUOTE>

<HR>
<BLOCKQUOTE>
<p>Sometime ago I had to solve what is a fairly common problem in numerical and scientific programming. I needed to curve-fit a set of x,y data samples to a polynomial equation. This problem eventually led me to write my own arbitrary precision math library. Of course I knew at the time that other libraries existed, but they were lacking some features I considered important. This article describes some of the added features of MAPM, and some of the implementation details that readers probably don&#146;t think about every day &#151; such as how to multiply two numbers together in less than 0(n2) times. I hope that readers will find MAPM interesting and useful for their own applications. In particular, thanks to a C++ wrapper class contributed by Orion Sky Lawlor, I believe MAPM is very easy to use.</p>

<H3><FONT COLOR="#000080">Why We Need Arbitrary Precision</FONT></H3>

<p>The project that inspired MAPM had specific requirements related to the maximum error allowed at each data point. In theory, by increasing the order of the polynomial used in the curve fit, it is possible to minimize the error of the data samples. After viewing a plot of the data samples, it became obvious that a high-order polynomial curve fit would be required. I used a least-squares curve-fitting algorithm. For a 10th-order polynomial, the algorithm required a summation of the x data samples raised to the 20th power. In general, for an Nth order polynomial, least-squares will require computing data samples to the 2N power.</p>
<p>At first, everything was working as expected. As I increased the order of the curve fit, the error in the fit compared to the raw data became smaller. This was true until I modeled an 18th-order polynomial. At this point, the solution did not improve. Higher order curve fits failed to yield improvement as well. I then tried the same input data on three different computers (with different operating systems and compilers) and generated three very different solutions. This is when I realized that a more fundamental problem was coming into play.</p>
<p>As you have probably already guessed, the accumulation of the round-off errors in the floating-point math was the culprit. I had reached the limits of Standard C&#146;s <B>double</B> data type, at least as implemented on the machines that were available to me. I realized I would need an arbitrary precision math package to complete my calculations. An arbitrary precision math package allows math to be performed to any desired level of precision.</p>
<p>For example, consider the two numbers <B>N1 = 98237307.398797975997</B> and <B>N2 = 87733164872.98273499749</B>. <B>N1</B> has 20 significant digits; <B>N2</B> has 22. If you assign these to C <B>double</B>s, each variable will maintain only 15-16 digits of precision <a href="#1">[1]</a>. If you then multiply these numbers, the result will be precise only to the 15-16 most significant digits.</p>
<p>The <I>true</I> multiplication result would contain 42 significant digits. An arbitrary precision math library will do the precise math and save the full precision of the multiplication. Addition and subtraction errors are also eliminated in an arbitrary precision math library. If you add <B>1.0E+100</B> to <B>1.0E-100</B> in normal C (or any other language), the small fractional number is lost. In arbitrary precision math, the full precision is maintained (~200 significant digits in this example).</p>
<p>After converting the curve-fitting algorithm to use arbitrary precision math, the three different computers all computed byte-for-byte identical results. This held true to well beyond 30th-order polynomials. In case you are curious, my accuracy requirements were satisfied with a 24th-order polynomial.</p>

<H3><FONT COLOR="#000080">MAPM Feature Set</FONT></H3>

<p>When I searched for an arbitrary precision math library to use, I noticed some common traits among the available libraries. First, most of them seemed to have a preference for integer-only math. And second, none of the arbitrary precision C libraries could perform the math functions typically found in <B>math.h</B>, such as <B>sqrt</B>, <B>cos</B>, <B>log</B>, etc.</p>
<p>It was at this point that I decided to write my own library. The basic requirements for my library were that it provide natural support for floating-point numbers and that it perform the most common functions found in <B>math.h</B>.</p>
<p>MAPM will perform the following functions to any desired precision level: <B>Sqrt</B>, <B>Cbrt</B> (cube root), <B>Sin</B>, <B>Cos</B>, <B>Tan</B>, <B>Arc-sin</B>, <B>Arc-cos</B>, <B>Arc-tan</B>, <B>Arc-tan2</B>, <B>Log</B>, <B>Log10</B>, <B>Exp</B>, <B>Pow</B>, <B>Sinh</B>, <B>Cosh</B>, <B>Tanh</B>, <B>Arc-sinh</B>, <B>Arc-cosh</B>, <B>Arc-tanh</B>, and also Factorial. The full <B>math.h</B> is not duplicated, though I think the functions listed above are most of the important ones. (My definition of what&#146;s important is what I&#146;ve actually used in a real application.) MAPM also has a random number generator with of period of <B>1.0E+15</B>.</p>
<p>MAPM has proven to be very portable. It has been compiled and tested under x86 Linux, HP-UX, and Sun Solaris using the GCC compiler. It has also been compiled and tested under DOS/Windows NT/Windows 9x using GCC for DOS as well as (old) 16- and 32-bit compilers from Borland and Microsoft. Makefiles are included in the online source archive (available at &lt;www.cuj.com/code&gt;) for the most common compilers under various operating systems.</p>

<H3><FONT COLOR="#000080">The MAPM C++ Wrapper Class</FONT></H3>

<p>Orion Sky Lawlor (<B>olawlor@acm.org</B>) has added a very nice C++ wrapper class to MAPM.</p>
<p>Using the C++ wrapper allows you to do things such as the following:</p>

<pre>
// Compute the factorial of the integer n

MAPM factorial(MAPM n)
{
   MAPM i;
   MAPM product = 1;

   for (i=2; i &lt;= n; i++)
      product *= i;

   return product;
}
</pre>

<p>The syntax is the same as if you were just writing normal code, but all the computations will be performed with the high precision math library, using the new data type <B>MAPM</B>.</p>
<p>See <a href="list1.htm">Listing 1</a> for another C++ sample. Note the use of literal character strings as constants. This allows the user to specify constants that cannot be represented by a standard C data type, such as a number with 200 digits or a number with a very large or small exponent (e.g., <B>6.21E-3714</B>).</p>

<H3><FONT COLOR="#000080">Algorithms for Implementing MAPM</FONT></H3>

<p>Since the MAPM contains over 30 functions, it is not practical to discuss all the algorithms used in the library. I will, however, discuss the more interesting ones.</p>

<H4><FONT COLOR="#000080">Multiplication</FONT></H4>

<p>In an arbitrary precision math library, the multiplication function is the most critical. Multiplication is normally an <B>O(n<SUP>2</SUP>)</B> operation. When you learned to multiply in grade school, you multiplied each digit of each number and did the final addition after all the multiplies were completed. To visualize this, multiply by hand a four-digit number by a four-digit number. The intermediate math requires 16 multiplies (4<SUP>2</SUP>). This method works and is very easy to implement; however it becomes prohibitively slow when the number of digits becomes large. So a 10,000-digit multiply will require 100 million individual multiplications!</p>

<H4><FONT COLOR="#000080">Faster Multiplication</FONT></H4>

<p>The next multiplication algorithm discussed is commonly referred to as the divide-and-conquer algorithm.</p>
<p>Assume we have two numbers (<B>a</B> and <B>b</B>) each containing <B>2N</B> digits:</p>

<pre>
let: a = (2<SUP>N</SUP>)*A<SUB>1</SUB> + A<SUB>0</SUB>, b = (2<SUP>N</SUP>)*B<SUB>1</SUB> + B<SUB>0</SUB>
</pre>

<p>where <B>A<SUB>1</SUB></B> is the &#147;most significant half&#148; of <B>a</B> and <B>A<SUB>0</SUB></B> is the &#147;least significant half&#148; of <B>a</B>. The same applies for <B>B<SUB>1</SUB></B> and <B>B<SUB>0</SUB></B>.</p>

<p>Now use the identity:</p>

<pre>
ab = (2<SUP>2N</SUP> + 2<SUP>N</SUP>)A<SUB>1</SUB>B<SUB>1</SUB> + 2<SUP>N</SUP>(A<SUB>1</SUB>-A<SUB>0</SUB>)(B<SUB>0</SUB>-B<SUB>1</SUB>) + (2<SUP>N</SUP> + 1)A<SUB>0</SUB>B<SUB>0</SUB>
</pre>

<p>The original problem of multiplying two <B>2N</B>-digit numbers has been reduced to three multiplications of <B>N</B>-digit numbers plus some additions, subtractions, and shifts.</p>
<p>This multiplication algorithm can be used in a recursive process. The divide-and-conquer method results in approximately <B>O(N<SUP>1.585</SUP>)</B> growth in computing time as <B>N</B> increases. So a 10,000-digit multiply will result in only approximately 2.188 million multiplies. This represents a considerable improvement over the generic <B>O(N<SUP>2</SUP>)</B> method.</p>

<H4><FONT COLOR="#000080">Really Fast Multiplication</FONT></H3>

<p>The final multiplication algorithm discussed utilizes the FFT (Fast Fourier Transform). An FFT multiplication algorithm grows only at <B>O(N*Log<SUB>2</SUB>(N))</B>. This growth is far less than the normal <B>N<SUP>2</SUP></B> or the divide-and-conquer&#146;s <B>N<SUP>1.585</SUP></B>. An FFT tutorial is beyond the scope of this article, but the basic methodology can be discussed.</p>
<p>First, perform a forward Fourier transform on both numbers, where the digits of each number are regarded as the samples being input to the FFT. This yields two sets of coefficients in the &#147;frequency&#148; domain. Each set of coefficients will contain as many samples as the number of digits in the original number. (Also, each coefficient will be a complex number.) Second, multiply the two sets of coefficients together. That is, if the numbers being multiplied are <B>a</B> and <B>b</B>, perform <B>c<SUB>a0</SUB>*c<SUB>b0</SUB></B>, <B>c<SUB>a1</SUB>*c<SUB>b1</SUB></B>, etc., where <B>c<SUB>an</SUB></B>, <B>c<SUB>bn</SUB></B> are the frequency-domain coefficients of <B>a</B> and <B>b</B>. This pair-wise multiplication in the frequency domain is equivalent to convolution in the &#147;time,&#148; or sample, domain. This yields the correct result, because when you multiply two <B>N</B>-digit numbers together, you are really performing a form of convolution across their digits. Third, compute the inverse transform on the product sequence. Lastly, convert the real part of the inverse FFT to integers and also release all your carries in the process.</p>
<p>The FFT used in MAPM is from Takuya Ooura. This FFT is fast, portable, and freely distributable.</p>
<p>To really see the differences in these three multiplication algorithms, compare these run times for multiplying two 1,000,000-digit numbers:</p>

<pre>
FFT Method          : 40 seconds
Divide-and-Conquer  : 1 hr, 50 min
Ordinary N<SUP>2</SUP>        : 23.9 days*

*projected!
</pre>

<p>The MAPM library uses all three multiplication algorithms. For small input numbers, the normal <B>O(n<SUP>2</SUP>)</B> algorithm is used. (After all, you don&#146;t need a special algorithm to multiply 43 x 87.) Next, the FFT algorithm is used. FFT-based algorithms do reach a point where the floating-point math will overflow, so at this point the divide-and-conquer algorithm is used. Once the divide-and-conquer algorithm has divided down to the point where the FFT can handle it, the FFT will finish up.</p>

<H4><FONT COLOR="#000080">Division</FONT></H4>

<p>Two division functions are used.</p>
<p>For dividing numbers less than 250 digits long, I used Knuth&#146;s division algorithm from <I>The Art of Computer Programming, Volume 2</I> <a href="#2">[2]</a> with a slight modification. I determine right in step D3 whether <B>q-hat</B> is too big, so step D6 is unnecessary. I use the first three (base 100) digits of the numerator and the first two digits of the denominator to determine the trial divisor, instead of Knuth&#146;s use of two and one digits, respectively.</p>
<p>For dividing numbers longer than 250 digits, I find <B>a/b</B> by first computing the reciprocal of <B>b</B> and then multiplying by <B>a</B>. The reciprocal <B>y = 1/x</B> is computed by using an iterative method such that:</p>

<pre>
y<SUB>n+1</SUB> = y<SUB>n</SUB>(2 - xy<SUB>n</SUB>)
</pre>

<p>where <B>y<SUB>n</SUB></B> is the current best estimate for <B>1/x</B>. This calculation is performed repeatedly until <B>y<SUB>n+1</SUB></B> stops changing to within a predetermined tolerance.</p>

<H4><FONT COLOR="#000080">Exponential, Sine, and Cosine</FONT></H4>

<p>These three functions are all computed by using a series expansion. Translations of the input are required to efficiently calculate these quantities. For more detail, see the accompanying sidebar, <a href="sidebar.htm">&#147;Practical Series Expansions for Sine, Cosine, and Exponentials.&#148;</a></p>

<H4><FONT COLOR="#000080">Random Number Generator</FONT></H4>

<p>The random number generator is also taken from Knuth <a href="#3">[3]</a>. Assuming the random number is <B>X</B>, compute (using all integer math):</p>

<pre>
X = (a*X + c) MOD m
</pre>

<p>where <B>x MOD y</B> represents <B>x modulo y</B>. From Knuth, <B>m</B> should be large, at least 2<SUP>30</SUP>. MAPM uses <B>1.0E+15</B>. The <B>a</B> coefficient should be between <B>0.01*m</B> and <B>0.99*m</B> and not have a simple pattern of digits. The <B>a</B> coefficient should not have any large factors in common with <B>m</B> and (since <B>m</B> is a power of 10 in this case) if <B>a MOD 200 = 21</B>, then all <B>m</B> different possible values will be generated before <B>X</B> starts to repeat. MAPM uses <B>a = 716805947629621</B>, so <B>a MOD 200</B> does equal 21, and <B>a</B> is also a prime number. There are few restrictions on <B>c</B>, except that <B>c</B> can have no factor in common with <B>m</B>, hence I have set <B>c = a</B>. On the first call, the system time is used to initialize <B>X</B>.</p>

<H4><FONT COLOR="#000080">Newton&#146;s Method (Also Known as Newton-Raphson)</FONT></H4>

<p>Unlike sine and cosine, which have convenient series expansions, not all math functions can be computed with a closed-form equation. Instead, they must be computed iteratively. These functions in MAPM (reciprocal, square root, cube root, logarithm, arc sine, and arc cosine) all use Newton&#146;s method to calculate the solution. (Although there are series expansions for logarithm, arc sine, and arc cosine, they all converge very slowly, so they are not practical for calculating these functions.) In general, the use of Newton&#146;s method in this application depends on the existence of an inverse for the function being computed <a href="#4">[4]</a>. An initial guess is made for <B>f(x)</B>, where <B>f</B> is the function being computed and <B>x</B> is the input value. Call this initial guess <B>y</B>. This initial guess is then plugged into the equation:</p>

<pre>
y<SUB>n+1</SUB> = y<SUB>n</SUB> - (g(y<SUB>n</SUB>) - x)/g'(y<SUB>n</SUB>)
</pre>

<p>where:</p>

<pre>
x is the input value
y<SUB>n</SUB> = current "guess" for the solution to
     f(x)
g(y<SUB>n</SUB>) = inverse function of f(x) 
        evaluated at y<SUB>n</SUB>
g'(y<SUB>n</SUB>) = first derivative of the inverse
         of f(x) evaluated at y<SUB>n</SUB>
y<SUB>n+1</SUB> = refined estimate of f(x)
</pre>

<p>This calculation is iterated until <B>y<SUB>n+1</SUB></B> stops changing to within a certain tolerance.</p>
<p>Newton&#146;s method is quadratically convergent. This results in doubling the number of significant digits that must be maintained during each iteration. MAPM uses the corresponding Standard C run-time library function to provide the initial guess.</p>
<p>As an example, I will demonstrate how to implement the square root function using Newton&#146;s method. Newton&#146;s method essentially asks: if the current guess for <B>sqrt(x)</B> is <B>y</B>, how close does squaring <B>y</B> come to producing <B>x</B>?</p>
<p>The equation for the inverse function <B>g(y)</B> is derived as follows:</p>

<pre>
f(x) = y = sqrt(x)
g(y) = y<SUP>2</SUP> = x, the inverse function of
            f(x)
g&#146;(y) = 2y, the derivative of the
        inverse function
</pre>

<p>Set up the iteration:</p>

<pre>
y<SUB>n+1</SUB> = y<SUB>n</SUB> - (g(y<SUB>n</SUB>) - x)/g'(y<SUB>n</SUB>)
y<SUB>n+1</SUB> = y<SUB>n</SUB> - (y<SUB>n<SUP>2</SUP></SUB> - x)/2y<SUB>n</SUB>
</pre>

<p>and after a little algebra:</p>

<pre>
y<SUB>n+1</SUB> = 0.5*[y<SUB>n</SUB> + x/y<SUB>n</SUB>]
</pre>

<p>To calculate the square root of 8.3, suppose I start with an initial guess of 1.0. (In actual use, I would generate a better initial guess.)</p>

<pre>
y<SUB>n+1</SUB> = 0.5*[y<SUB>n</SUB> + 8.3/y<SUB>n</SUB>]
</pre>

<p>The iterations of <B>y<SUB>n</SUB></B> are as follows:</p>

<pre>
iteration              y
&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
0          1.00000000000000000000000
0          1.00000000000000000000000
0          1.00000000000000000000000
1          4.65000000000000000000000
2          3.21747311827956989247311
3          2.89856862531072654536764
4          2.88102547060539111310782
5          2.88097205867270336382209
6          2.88097205817758669914416
7          2.88097205817758669910162
8          2.88097205817758669910162
</pre>

<p>As you can see, the iteration is converging to a constant value, the square root of 8.3. Similar iteration loops are used to compute the reciprocal, cube-root, logarithm, arc sine, and arc cosine functions. The library actually uses a better <B>sqrt</B> iteration:</p>

<pre>
y<SUB>n+1</SUB> = 0.5*y<SUB>n</SUB>*[3 - x*y<SUB>n<SUP>2</SUP></SUB>]
</pre>

<p>This iteration actually finds <B>1/sqrt(x)</B>. It is preferable since there is no division operation. Division is slower than multiplication and is best avoided when possible.</p>

<H3><FONT COLOR="#000080">Summary</FONT></H3>

<p>MAPM is a portable arbitrary math library that is easy to use, especially when used with the C++ wrapper class. It supports most of the math functions encountered in a typical application, so it can be used in many applications that require arbitrary precision. MAPM uses an FFT-based algorithm for multiplication, which is typically the performance bottleneck for arbitrary precision math libraries.</p>

<H3><FONT COLOR="#000080">Notes and References</FONT></H3>

<p><a name="1"></a>[1]  This assumes that <B>double</B>s are implemented as 64-bit words, which is the case for most general purpose computers available today.</p>
<p><a name="2"></a>[2]  Donald E. Knuth. <I>The Art of Computer Programming, Volume 2, Seminumerical Algorithms, Third Edition</I> (Addison-Wesley, 1998), pages 270-273.</p>
<p><a name="3"></a>[3]  ibid., pages 184-185.</p>
<p><a name="4"></a>[4]  The reciprocal function is its own inverse, which might seem to pose a problem in using Newton&#146;s method. Fortunately, it can be factored out so that it does not actually appear in the algorithm.</p>

<H3><FONT COLOR="#000080">Further Reading</FONT></H3>

<p>Bjarne Stroustrup. <I>The C++ Programming Language, Third Edition</I> (Addison-Wesley, 1997).</p>
<p>Takuya Ooura. &#147;A General Purpose FFT (Fast Fourier/Cosine/Sine Transform) Package,&#148; 1996-1999.</p>
<p>D. H. Bailey. &#147;Multiprecision Translation and Execution of Fortran Programs,&#148; <I>ACM Transactions on Mathematical Software</I>, September 1993, p. 288-319.</p>
<p>Press, Teukolsky, Vetterling, and Flannery. <I>Numerical Recipes in C: The Art of Scientific Computing, Second Edition</I> (Cambridge University Press, 1988-1992).</p>
<p>William H. Beyer. <I>CRC Standard Mathematical Tables, 25th Edition</I> (CRC Press, 1978).</p>
<p>Johnson and Riess. <I>Numerical Analysis</I> (Addison-Wesley, 1977).</p>
<p>Rule, Finkenaur, and Patrick. <I>FORTRAN IV Programming</I> (Prindle, Weber, and Schmidt, 1973). </p>


<p><i><B>Michael C. Ring</B> has a B.S. in Electrical Engineering from the University of Minnesota. He is currently a Principal Systems at Honeywell working system design and test on inertial navigation systems. He can be reached at <B>mike.ring@honeywell.com</B>.</i></p>

<h4><a href="../../../source/2001/nov01/ring.zip"></a></h4>

</blockquote></body></html>
