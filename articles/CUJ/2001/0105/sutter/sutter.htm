<HTML>   
     <HEAD>
<TITLE>May 2001/Sutter&#146;s Mill</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../tocmay.htm"></A><FONT COLOR="#FF0000">   C/C++ Contributing Editors</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">Sutter&#146;s Mill: To New, Perchance To Throw (Part 2 of 2)</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Herb Sutter</FONT></H3>

<BLOCKQUOTE>
<p>Conventional wisdom says always check for and respond to allocation failure &#151; but oftentimes you just can&#146;t do either.</p>
</BLOCKQUOTE>

<HR>
<BLOCKQUOTE>

<p>Copyright &copy; 2001 by Herb Sutter</p>
<p>In my previous column <a href="#1">[1]</a>, I illustrated and justified the following coding guideline:</p>
<UL><LI>   Any class that provides its own class-specific <B>operator new</B>, or <B>operator new[]</B>, should also provide corresponding class-specific versions of plain <B>new</B>, in-place <B>new</B>, and <B>nothrow new</B>. Doing otherwise can cause needless problems for people trying to use your class.</LI></UL>
<p>This time, we&#146;ll delve deeper into the question of what <B>operator new</B> failures mean, and how best to detect and handle them:</p>
<UL><LI>   Avoid using <B>new(nothrow)</B>, and make sure that when you&#146;re checking for <B>new</B> failure you&#146;re really checking what you think you&#146;re checking.</LI></UL>
<p>The first part may be mildly surprising advice, but it&#146;s the latter that is likely to raise even more eyebrows &#151; because on certain popular real-world platforms, memory allocation failures usually don&#146;t even manifest in the way the Standard says they must.</p>
<p>Like last time, for simplicity, I&#146;m not going to mention the array forms of <B>new</B> specifically. What&#146;s said about the single-object forms applies correspondingly to the array forms.</p>

<H3><FONT COLOR="#000080">Exceptions, Errors, and new(nothrow)</FONT></H3>

<p>First, a recap of what we all know: whereas most forms of <B>new</B> report failure by throwing a <B>bad_alloc</B> exception, nothrow <B>new</B> reports failure the time-honored <B>malloc</B> way, namely by returning a null pointer. This guarantees that nothrow <B>new</B> will never throw, as indeed its name implies.</p>
<p>The question is whether this really buys us anything. Some people have had the mistaken idea that nothrow <B>new</B> enhances exception safety because it prevents some exceptions from occurring. So here&#146;s the $64,000 motivating question: does using nothrow <B>new</B> enhance program safety in general, or exception safety in particular? The (possibly surprising) answer is: no, not really. Error reporting and error handling are orthogonal issues. &#147;It&#146;s just syntax after all.&#148; <a href="#2">[2]</a></p>
<p>The choice between throwing <B>bad_alloc</B> and returning null is just a choice between two equivalent ways of reporting an error. Therefore, detecting and handling the failure is just a choice between checking for an exception and checking for the null. To a calling program that checks for the error, the difference is just syntactic. That means that it can be written with exactly equivalent program safety, and exception safety, in either case &#151; the syntax by which the error happens to be detected isn&#146;t relevant to safety, because it is just syntax, and leads to only minor variations in the calling function&#146;s structure (e.g., something like <B>if (null) { HandleError(); throw MyOwnException; }</B> vs. something like <B>catch(bad_alloc) { HandleError(); throw MyOwnException; }</B>). Neither way of <B>new</B> error reporting provides additional information or additional inherent safety, and so neither way inherently makes programs somehow &#147;safer&#148; or &#147;able to be more correct,&#148; assuming of course that the handling is coded accurately.</p>
<p>But what&#146;s the difference to a calling program that doesn&#146;t check for errors? In that case, the only difference is that the eventual failure will be different in mode but not severity. Either an uncaught <B>bad_alloc</B> will unceremoniously unwind the program stack all the way back to <B>main</B>, or an unchecked null pointer will cause a memory violation and immediate halt when it&#146;s later dereferenced. Both failure modes are fairly catastrophic, but there&#146;s some advantage to the uncaught exception: it will make an attempt to destroy at least some objects, and therefore release some resources, and if some of those objects are things like a <B>TextEditor</B> object that automatically saves recovery information when prematurely destroyed, then not all the program state need be lost if the program is carefully written. (Caveat: when memory really is exhausted, it&#146;s harder than it appears to write code that will correctly unwind and back out, without trying to use a teensy bit more memory.) An abrupt halt due to use of a bad pointer, on the other hand, is far less likely to be healthy.</p>
<p>From this we derive Moral #1:</p>

<H4><FONT COLOR="#000080">Moral #1: Avoid Nothrow New</FONT></H4>

<p>Nothrow <B>new</B> does not inherently benefit program correctness or exception safety. For some failures, namely failures that are ignored, it&#146;s worse than an exception because at least an exception would get a chance to do some recovery via unwinding. As pointed out in the previous column, if classes provide their own <B>new</B> but forget to provide nothrow <B>new</B> too, nothrow <B>new</B> will be hidden and won&#146;t even work. In most cases nothrow <B>new</B> offers no benefit, and for all these reasons it should be avoided.</p>
<p>I can think of two corner cases where nothrow <B>new</B> can be beneficial. The first case is one that these days is getting pretty hoary with age: when you&#146;re migrating a lot of legacy <I>really</I>-old-style C++ application code written before the mid-1990s that still assumes that (and checks whether) <B>new </B>returns null to report failure, then it can be easier to globally replace &#147;<B>new</B>&#148; with &#147;<B>new (nothrow)</B>&#148; in those files &#151; but it&#146;s been a long time now since unadorned <B>new</B> behaved the old way! The amount of such hoary old code that&#146;s still sitting around and hasn&#146;t yet been migrated (or recompiled on a modern compiler) yet is dwindling fast. The second case for using nothrow <B>new</B> is if <B>new</B> is being used a lot in a time-critical function or inner loop, and the function is being compiled using a weaker compiler that generates inefficient exception handling code overhead, and this produces a measurable run-time difference in this time-critical function between using normal <B>new</B> and using nothrow <B>new</B>. Note that when I say &#147;measurably&#148; I mean that we&#146;ve actually written a test harness that includes at least the entire piece of time-critical code (not just a toy example of <B>new</B> by itself), and timed two versions, one with <B>new</B> and one with <B>new(nothrow)</B>. If after all that we&#146;ve proved that it makes a difference to the performance of the time-critical code, we might consider <B>new(nothrow)</B> &#151; and should at the same time consider other ways to improve the allocation performance, including the option of writing a custom <B>new</B> using a fixed-size allocator or other fast memory arena <a href="#3">[3]</a>.</p>
<p>This brings us to Moral #2:</p>

<H4><FONT COLOR="#000080">Moral #2: There&#146;s Often Little Point in Checking for new Failure Anyway</FONT></H4>

<p>This statement might horrify some people. &#147;How can you suggest that we <I>not</I> check for <B>new</B> failure, or that checking failures is not important?&#148; some might say, righteously indignant. &#147;Checking failures is a cornerstone of robust programming!&#148; That&#146;s very true in general, but &#151; alas &#151; it often isn&#146;t as meaningful for <B>new</B>, for reasons which are unique to memory allocation, as opposed to other kinds of operations whose failure should indeed be checked and handled.</p>
<p>Here are some reasons why checking for <B>new</B> failure isn&#146;t as important as one might think:</p>
<p>1)  Checking <B>new</B> failure is useless on systems that don&#146;t commit memory until the memory is used.</p>
<p>On some operating systems, including specifically Linux <a href="#4">[4]</a>, memory allocation always succeeds. Full stop. How can allocation always succeed, even when the requested memory really isn&#146;t available? The reason is that the allocation itself merely <I>records a request</I> for the memory; under the covers, the (physical or virtual) memory is not actually committed to the requesting process, with real backing store, until the memory is actually used. Even when the memory is used, often real (physical or virtual) memory is only actually committed as each page of the allocated buffer is touched, and so it can be that only the parts of the block that have actually been touched get committed.</p>
<p>Note that if <B>new</B> uses the operating system&#146;s facilities directly, then <B>new</B> will always succeed but any later innocent code like <B>buf[100] = &#146;c&#146;;</B> can throw or fail or halt. From a Standard C++ point of view, both effects are nonconforming, because the C++ Standard requires that if <B>new</B> can&#146;t commit enough memory it must fail (this doesn&#146;t), and that code like <B>buf[100] = &#146;c&#146;</B> shouldn&#146;t throw an exception or otherwise fail (this might).</p>
<p>Background: why do some operating systems do this kind of &#147;lazy allocation?&#148; There&#146;s a noble and pragmatic idea behind this scheme, namely that a given process that requests memory might not actually immediately need all of said memory &#151; the process might never use all of it, or it might not use it right away and in the meantime maybe the memory can be usefully &#147;lent&#148; to a second process which may need it only briefly. Why immediately commit all the memory a process demands, when it may not really need it right away? So this scheme does have some potential advantages.</p>
<p>The main problem with this approach, besides that it makes C++ standards conformance difficult, is that it makes program correctness in general difficult, because <I>any</I> access to successfully-allocated dynamic memory might cause the program to halt. That&#146;s just not good. If allocation fails up front, the program knows that there&#146;s not enough memory to complete an operation, and then the program has the choice of doing something about it, such as trying to allocate a smaller buffer size or giving up on only that particular operation, or at least attempting to clean up some things by unwinding the stack. But if there&#146;s no way to know whether the allocation really worked, then any attempt to read or write the memory may cause a halt &#151; and that halt can&#146;t be predicted, because it might happen on the first attempt to use part of the buffer, or on the millionth attempt after lots of successful operations have used other parts of the buffer.</p>
<p>On the surface, it would appear that our only way to defend against this is to immediately write to (or read from) the entire block of memory to force it to really exist. For example:</p>

<pre>
// Example 1: Manual initialization
//
// Deliberately go and touch each byte.
//
char* p = new char[1000000000];
memset( p, 0, 1000000000 );
</pre>

<p>If the type being allocated happens to be a non-POD <a href="#5">[5]</a> class type, the memory is in fact touched automatically for you:</p>

<pre>
// Example 2: Default initialization
//
// If T is a non-POD, this code initializes
// all the T objects immediately and will
// touch every (significant, non-padding) byte
//
T* p = new T[1000000000];
</pre>

<p>If <B>T</B> is a non-POD, each object is default-initialized, which means that all the significant bytes of each object are written to, and so the memory has to be accessed <a href="#6">[6]</a>.</p>
<p>You might think that&#146;s helpful. It&#146;s not. Sure, if we successfully complete the <B>memset</B> in Example 1, or the <B>new</B> in Example 2, we do in fact know that the memory has really been allocated and committed. But if accessing the memory fails, the twist is that the failure won&#146;t be what we might naively expect it to be: we won&#146;t get a null return or a nice <B>bad_alloc</B> exception, but rather we&#146;ll get an access violation and a program halt, none of which the code can do anything about (unless it can use some platform-specific way to trap the violation). That may be marginally better and safer than just allocating without writing and pressing on regardless, hoping that the memory really will be there when we need it and that all will be well, but not by much.</p>
<p>This brings us back to the issue of standards conformance, because it may be possible for the compiler-supplied <B>::operator new</B> and <B>::operator new[]</B> themselves to do better with the above approach to working around operating system quirks than we as programmers could do easily. In particular, the compiler implementer may be able to use knowledge of the operating system to intercept the access violation and therefore prevent a program halt. That is, it may be possible for a C++ implementer to do all the above work: allocate, and then confirm the allocation by making sure each byte is written to, or at least to each page; catch any failure in a platform-specific way; and convert it to a standard <B>bad_alloc</B> exception (or a null return, in the case of a nothrow <B>new</B>). I doubt that any implementer would go to this trouble, though, for two reasons: first, it means a performance hit, and probably a big one to incur for all cases; and second, <B>new</B> failure is pretty rare in real life anyway... which happens to lead us nicely to the next point:</p>
<p>2)  In the real world, <B>new</B> failure is a rare beast, made nearly extinct by the thrashing beast.</p>
<p>As a practical matter, many modern server-based programs rarely encounter memory exhaustion.</p>
<p>On a virtual memory system, most real-world server-based software performs work in various parts of memory while other processes are actively doing the same in their own parts of memory; this causes increasing paging as the amount of memory in use grows, and often the processes never reach <B>new</B> failure. Rather, long before memory can be fully exhausted, the system performance will hit the thrash wall and just grind ever more unusably slowly as pages of virtual memory are swapped in and out from disk, and the sysadmin will start killing processes. </p>
<p>I caveat this with words like &#147;most&#148; because it is possible to create a program that allocates more and more memory but doesn&#146;t actively use much of it. That&#146;s possible, but unusual, at least in my own experience. This also of course does not apply to systems without virtual memory, such as many embedded and real-time systems; some of these are so failure-intolerant that they won&#146;t even use any kind of dynamic memory at all, never mind virtual memory.</p>
<p>3)  There&#146;s not always much you can do when you detect new failure.</p>
<p>As Andy Koenig pointed out in his 1996 article &#147;When Memory Runs Low,&#148; <a href="#7">[7]</a> the default behavior of halting the program on <B>new</B> failure (usually with at least an attempt to unwind the stack) is actually the best option in most situations, especially during testing.</p>
<p>Sometimes when <B>new</B> fails there are a few things you can do, of course: if you want to record some diagnostic info, the <B>new</B> handler is a nice hook for doing logging. It is sometimes possible to apply the strategy of keeping a reserve &#147;emergency memory&#148; buffer; although anyone who does this should know what they are doing, and actually carefully test the failure case on their target platforms, because this doesn&#146;t necessarily work the way people think. Finally, if memory really is exhausted, you can&#146;t necessarily rely on being able to throw a nontrivial (e.g., non-builtin) exception; even <B>throw string("failed");</B> will usually attempt a dynamic allocation using <B>new</B>, depending on how highly optimized your implementation of <B>string</B> happens to be.</p>
<p>So yes, sometimes there are useful things you can do to cope with specific kinds of <B>new</B> failure, but often it&#146;s not worth it beyond letting stack unwinding and the <B>new</B> handler (including perhaps some logging) do their thing.</p>

<H3><FONT COLOR="#000080">What Should You Check?</FONT></H3>

<p>There are special cases for which checking for memory exhaustion, and trying to recover from it, do make sense. Andy mentions some in his article referenced above. For example, you could choose to allocate (and if necessary write to) all the memory you&#146;re ever going to use up front, at the beginning of your program, and then manage it yourself; if your program crashes at all, it will crash right away before actually doing work. This approach requires extra work and is only an option if you know the memory requirements in advance.</p>
<p>The main category of recoverable <B>new</B> failure error I&#146;ve seen in production systems has to do with creating buffers whose size is externally supplied from some input. For example, consider a communications application where each transmitted packet is prepended with the packet length, and the first thing the receiver does with each packet is to read the length and then allocate a buffer big enough to store the rest of the packet. In just such situations, I&#146;ve seen attempts to allocate monstrously large buffers, especially when data stream corruption or programming errors cause the length bytes to get garbled. In this case, the application <I>should</I> be checking for this kind of corruption (and, better still, designing the protocol to prevent it from happening in the first place if possible) and aborting on invalid data and unreasonable buffer sizes, because the program can often continue doing something sensible, such as retrying the transmission with a smaller packet size or even just abandoning that particular operation and going on with other work. After all, the program is <I>not</I> really &#145;out of memory&#146; when an attempt to allocate 2 GB failed but there&#146;s still 1GB of virtual memory left <a href="#8">[8]</a>!</p>
<p>Another special case where <B>new</B> failure recovery can make sense is when your program optimistically tries to allocate a huge working buffer, and on failure just keeps retrying a smaller one until it gets something that fits. This assumes that the program as a whole is designed to adjust to the actual buffer size and does chunking as necessary.</p>

<H3><FONT COLOR="#000080">Summary</FONT></H3>
<p>Avoid using nothrow <B>new</B>, because it offers no significant advantages these days and usually has worse failure characteristics than plain throwing <B>new</B>. Remember that there&#146;s often little point in checking for <B>new</B> failure anyway, for several reasons. If you are rightly concerned about memory exhaustion then be sure that you&#146;re checking what you think you&#146;re checking, because: checking <B>new</B> failure is typically useless on systems that don&#146;t commit memory until the memory is used; in virtual memory systems <B>new</B> failure is encountered rarely or never because long before virtual memory can be exhausted the system typically thrashes and a sysadmin begins to kill processes; and, except for special cases, even when you detect <B>new</B> failure there&#146;s not always much you can do if there really is no memory left.</p>

<H3><FONT COLOR="#000080">Notes and References</FONT></H3>

<p><a name="1"></a>[1]  Herb Sutter. &#147;To New, Perchance to Throw, Part 1 of 2,&#148; <I>C/C++ Users Journal,</I> March 2001.</p>
<p><a name="2"></a>[2]  Can be sung to the tune of &#147;It&#146;s a Small World (After All).&#148;</p>
<p><a name="3"></a>[3]  See also Herb Sutter, &#147;Containers in Memory: How Big is Big?&#148; <I>C/C++ Users Journal,</I> January 2001 and Herb Sutter, <I>Exceptional C++</I> (Addison-Wesley, 2000).</p>
<p><a name="4"></a>[4]  This is what I&#146;ve been told by Linux folks in a discussion about this on comp.std.c++. Lazy-commit is also a configurable feature on some other operating systems.</p>
<p><a name="5"></a>[5]  POD stands for &#147;plain old data.&#148; Informally, a POD means any type that&#146;s just a bundle of plain data, though possibly with user-defined member functions just for convenience. More formally, a POD is a class or union that has no user-defined constructor or copy assignment operator or destructor, and no (non-static) data member that is a reference, pointer to member, or non-POD.</p>
<p><a name="6"></a>[6]  This ignores the pathological case of a <B>T</B> whose constructor doesn&#146;t actually initialize the object&#146;s data.</p>
<p><a name="7"></a>[7]  Andrew Koenig. &#147;When Memory Runs Low,&#148; <I>C++ Report,</I> June 1996.</p>
<p><a name="8"></a>[8]  Interestingly, allocating buffers whose size is externally specified is a classic security vulnerability. Attacks by malicious users or programs specifically trying to cause buffer problems is a classic, and still favorite, security exploit to bring down a system. Note that trying to crash the program by causing allocation to fail is a denial-of-service attack, not an attempt to actually gain access to the system; the related, but distinct, overrun-a-fixed-length-buffer attack is also a perennial favorite in the hacker community, and it&#146;s amazing just how many people still use <B>strcpy</B> and other unchecked calls and thereby leave themselves wide open to this sort of abuse.</p>

<p><i><B>Herb Sutter</B> (<B>hsutter@peerdirect.com</B>) is chief technology officer of PeerDirect Inc. and the architect of their heterogeneous database replication products. He is secretary of the ISO/ANSI C++ standards committees, a member of the ANSI SQL committee, and author of the book </I>Exceptional C++<I> (Addison-Wesley).</p>

</blockquote></body></html>
