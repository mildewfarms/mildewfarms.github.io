<html>
<head>
<title>Efficient Variable Automatic Buffers</title>

</head>

<body>
<h2>Efficient Variable Automatic Buffers</h2> 

<h3>By Matthew Wilson </h3>


<p>Automatic variables, including arrays, are substantially quicker than dynamically allocating storage from the heap. However, since automatic variables actually occupy space in the program stack reserved by the compiler, they are of a fixed size. Though some compilers and platforms provide facilities for dynamically allocating stack space by adjusting the stack pointer, this support is neither standard nor universal, and is quite limited. </p>
<p>In this article, I present <b>auto_buffer</b>, a simple, platform-independent STLSoft template class (<a href="http://stlsoft.org/">http://stlsoft.org/</a>) that attempts to fulfill dynamically sized allocations from an encapsulated array; otherwise, it allocates space from its parameterizing allocator. Because <b>auto_buffer</b> lets programmers specify the size of the encapsulated memory as a parameter to the template, this scheme gives you more control over memory allocation. Such control is especially significant in circumstances of limited stack or when stack allocations over system page size are undesirable. You can also use instances of <b>auto_buffer</b> in object composition as well as with variable automatic variables.</p>



<h3>Memory Allocation</h3>


<p>Stack variables are allocated from the executing thread's stack, in the scope of the local block within which they are defined, by adjustment of the stack pointer. Global variables are fixed in program global memory and allocated by reservation of space in the global memory area. In <A HREF="0312wilsonl1.htm" target="_BLANK">Listing 1</A>, <b>m1</b> is a global variable, <b>m2</b> a stack variable, and <b>m3</b> (a stack variable that points to) heap memory. Here, I focus on the implications of the use of stack memory, although some of the issues I discuss also apply to global memory.</p>


<p>Since the allocation of memory for global and stack variables is carried out at compile time, there is no actual "allocation" &#151; simply a manipulation of pointers or addresses. Disregarding virtual-memory issues, the memory already exists. Consequently, this form of memory allocation is extremely efficient; in fact, it is the most efficient form of memory allocation. An additional advantage is that you can determine, at compile time, the size of the available memory via the <b>sizeof</b> operator.</p>

<p>However, since the allocation is carried out at compile time, it can only be of fixed, predetermined size. This is often perfectly acceptable such as when, for example, dealing with filesystem entity names, which have a fixed maximum on most platforms. When writing such code, you can simply declare a buffer of the maximum potential size, confident that passing it to any function will not result in buffer overwrites. However, when dealing with APIs whose functions may use buffers of any length (the Windows Registry API, for instance), you can never guarantee to have a fixed buffer of sufficient size (though most of us have written <b>RegXxx()</b> code passing buffers of <b>_MAX_PATH </b>size!).</p>

<p>Another disadvantage of stack variables is that variables within a single function having a combined size above a certain (operating system) threshold can cause compilers to insert stack-checking code, thereby binding the executable to the C-Runtime Library. </p>

<p>You have a number of alternatives to fixed-size stack memory. Heap memory, the most obvious option, is the opposite of frame memory. It is obtained from the heap, or from one of a set of heaps, at runtime. Every heap API requires a function to allocate memory (<b>malloc()</b>) and, except where using garbage collection, a corresponding function to return it (<b>free()</b>).The advantage of heap memory is that the size of the buffer can be any practical size within the limitations of the runtime system (although some older memory APIs restrict the maximum size of individual buffers).</p>

<p>The main disadvantage of heap memory is that heap allocations are considerably slower than stack/global allocations (due to the complexity of implementing the memory-allocation schemes to reclaim and defragment freed memory). Additionally, it is possible that the request may not actually be satisfiable at runtime, requiring client code to manage that eventuality (whether through exception handling or through testing the returned value against NULL). Finally, if you forget to free allocated chunks, your process will likely die a slow death through memory exhaustion.</p><p>

<b>alloca()</b> (and its vendor-specific variants) is an attempt to merge the speed of stack memory with the flexibility of heap memory. The <b>alloca()</b> function allocates memory, whose size is determined at runtime, from the stack rather than from the heap by simply adjusting the stack pointer. The memory is automatically freed when the enclosing scope exits. This is a useful facility and, where available and applicable, provides the best solution for most cases of automatic buffers. Unfortunately, <b>alloca()</b> has a number of disadvantages. </p>

<ul>

  <li>It cannot give compile-time size and requires a local variable to keep track of memory size.

  <li>It cannot reallocate memory in the same manner that <b>realloc()</b> (and its platform-specific analogs) can, so in that way, it cannot be a direct replacement for heap memory APIs.

  <li>It is nonstandard, so it is not guaranteed to be available with all platforms and compilers.

  <li>It has varied failure behavior: On Linux, it returns NULL if it fails to allocate the requested memory, on Win32 a machine-dependent exception is generated. 


</ul>

<p><b>alloca()</b> also requires stack-checking code and the context for its use is restricted. For instance, the Microsoft Win32 C-runtime<b> _alloca() </b>cannot be used from within certain exception-handling contexts since it can cause the system to crash unpredictably.</p>

<p>Two other disadvantages of <b>alloca()</b> are even more significant:</p>

<ul>

  <li>By virtue of its mechanism, <b>alloca()</b> cannot be used to hold memory outside the current execution context, so it cannot be used for allocating variable-sized blocks inside object instances, templates, or other functions (i.e., it is not possible to write a <b>local_strdup()</b> unless you use macros).

  <li>Finally, and most seriously, because of its adjusting of stack pointers, it can easily cause stack exhaustion when used in a function that performs a number of allocation/deallocation cycles. Indeed, the functions I used to test performance for this article quickly failed on both Linux and Win32 because of stack exhaustion. If it is not possible or practicable to move the <b>alloca()</b> call and processing of the memory it returns into a separate function called within a loop &#151; which may hurt performance too much to be useful anyway &#151; you are better off avoiding <b>alloca()</b> for looping code. 


</ul>

<p>The C-language C99 enhancements offer another memory allocation alternative in the form of Variable Length Arrays (VLAs). VLAs provide a great deal of power and (syntactically at least) address the issue of dynamically sized stack array variables [1]. Though a few compilers support them in C++, VLAs are not a part of Standard C++. (Randy Meyers tells me that, although there is no factor hindering VLA adoption into C++, the <b>valarray</b> class is the generally recommended alternative.) Hence, the main disadvantage of VLAs in C++ is lack of portability.</p>

<p>Where implemented, VLAs will likely be layered on top of either <b>alloca()</b> (or a similar technique) or heap memory. Whether they support all types, or just Plain Old Data (POD) types &#151; fundamental and simple aggregate types &#151; is implementation dependent. So, although the syntax of the language will be clearer than current solutions based on either <b>alloca()</b> or heap memory, the implications for performance, robustness, and availability are largely the same. Digital Mars C/C++ uses <b>alloca()</b>, and thus does not work with nonPOD types. Comeau C++ uses whatever VLA support is provided by the backend compiler. (Comeau Computing is considering directly implementing VLA in terms of <b>alloca()</b>, which would widen the support considerably.) Impressively, GCC supports nonPOD types in its VLAs, and does not experience the stack exhaustion that it does with <b>alloca()</b>.</p>



<h3><b>auto_buffer</b> Class</h3>


<p>The purpose of <b>auto_buffer</b> is to emulate the syntax and semantics of a standard array variable. The requirements are:</p>


<ul>

  <li>To attempt to allocate requested memory from its internal buffer, otherwise allocating from the heap

  <li>To be as compatible as is possible with normal raw array syntax in the access to and manipulation of its allocated storage, in particular facilitating the use of array arithmetic and index operators [2].

  <li>To provide basic STL container methods <b>empty()</b>, <b>size()</b>, <b>begin()</b>, and <b>end()</b>. Note that <b>begin() </b>and<b> end() </b>are added purely as a programmatic convenience and should not be taken to mean that <b>auto_buffer</b> is a full STL container. It is not, in part because it is not (currently) able to allocate and manage instances of nonPOD types.


</ul>

<p><b>auto_buffer</b>'s constructor performs only the allocation of memory, not in-place construction of elements. Storing arrays of nonPOD type objects would cause problems, since the construction and/or destruction of these objects would not be carried out. While the presence of the <b>m_internal</b> array member prevents compilation of parameterizations of <b>auto_buffer</b> with classes that do not provide publicly accessible default constructors, default-constructible types could still cause problems. Hence, the presence of the constraint in the constructor, which uses the <b>type_is_non_class_or_trivial_class</b> constraint class (see <A HREF="0312wilsonl2.htm" target="_BLANK">Listing 2</A>) to prevent compilation for nonPOD types. The constraint is simply a union containing the type, thereby preventing an instance being created when parameterized with a nonPOD type. (This check can be omitted by specification of a preprocessor symbol, but you're responsible for ensuring correctness if you use it.)</p>

<p>The class itself is comprised of a single constructor, destructor, <b>data()</b>,<b> size()</b>, and <b>empty() </b>methods, <b>const</b> and <b>non-const</b> sequence methods (<b>begin()</b> and <b>end()</b>), and <b>const</b> and <b>non-const</b> conversion operators [2]; see <A HREF="0312wilsonl3.htm" target="_BLANK">Listing 3</A> [3].</p>

<p>The implementation of the class is straightforward. Almost all the action is in the constructor. It takes a single argument: the requested number of elements in the array. This is tested against the size of the internal buffer and, if not larger, the <b>m_buffer</b> member is set to point to <b>m_internal</b>, the internal array. (This is fairly similar to the small string optimization; see <i><i>Effective STL</i></i>, by Scott Meyers, Addison-Wesley, 2002). If the requested size is larger, a request is made to the allocator, setting <b>m_buffer</b> to the allocated block. (All accessor methods refer to <b>m_buffer</b>, in either case.) </p>

<p>In the latter case, it is possible for the allocation to fail. Because an important requirement for the class (and the STLSoft libraries as a whole) is to be as widely compatible as possible, the constructor is written to work correctly both in situations where allocation failures result in an exception, and in those cases where the <b>allocate()</b> method returns NULL. When exceptions are thrown, they are propagated to the caller of the <b>auto_buffer</b> constructor and the instance of the <b>auto_buffer</b> is not constructed. </p>

<p>Some allocators do not throw exceptions when they fail to secure enough memory for the requested allocation, instead returning NULL (for example, the STL that ships with Visual C++ 5 and 6; see my article "Generating Out-Of-Memory Exceptions," <i><i>Windows Developer's Journal,</i></i> May 2001). Also, if you are creating a small program, you may not want to compile/link in exception-handling mechanisms and may deliberately plug in a NULL-on-failure allocator. In such circumstances, it is a good idea to leave the <b>auto_buffer</b> in a coherent state; therefore, the initializing condition for <b>m_cItems</b> discriminates on whether <b>m_buffer</b> is nonNULL. In the case where NULL is returned, the remaining construction of the <b>auto_buffer</b> instance results in initialization of the <b>m_cItems</b> member to <b>0</b>, thereby providing correct behavior for the use of this empty instance; namely, that <b>begin() == end(), empty() </b>returns <b>true</b> and <b>size()</b> returns <b>0.</b> </p>

<p>This scheme uses the fragile technique of relying on the relative member (initialization) order of <b>m_buffer</b> and <b>m_cItems</b>. The constructor includes assertions (static/compile-time and dynamic/runtime) to guard against any maintenance edits that are not mindful of this requirement and might change this ordering, resulting in the discrimination against the value of <b>m_buffer</b> testing garbage and the classic undefined results: a crash. Such member ordering dependencies are not generally a good idea, but I chose to use the technique here because it allows me to declare <b>m_cItems</b> as <b>const</b>, and the assertions ensure that all is well [4].</p>

<p>In keeping with good practice, the constructor is explicit (though it's hard to conceive of an implicit conversion scenario against which to guard). The destructor has a straightforward implementation. By testing <b>m_cItems</b> against the size of the internal buffer, it determines whether <b>m_buffer</b> points to <b>m_internal</b>; if not, it frees the heap memory by calling the allocator's <b>deallocate()</b> method.</p>

<p>The remainder of the method implementations are trivial. Pointer conversions (<b>operator value_type *()</b> and <b>const</b> version) are used rather than index operators (<b>value_type &amp;operator []()</b> and <b>const</b> version), since the pointer conversion approach supports both conversion to pointer and (implicit) array indexing, whereas the index operator approach on its own allows only (explicit) array indexing [5].</p>

<p>Finally, the <b>auto_buffer</b> class can be used in composition within other classes.</p>



<h3>Parameterizing the Template</h3>


<p>Using the template requires parameterization of the element type, allocator type, and size of the internal array (<b>m_internal</b>). Again, the class does not require a particular memory-allocation scheme to support its semantics. As long as the allocator supports the STL Allocator (<a href="http://sgi.com/tech/stl/Allocators.html">http://sgi.com/tech/stl/Allocators.html</a>) concept, you may use any compliant allocator. This approach offers several advantages in addition to the benefit of conforming to STL practices, particularly when the code is to work with Visual C++ and you want to avoid linkage to the C-Runtime Library. The size of <b>m_internal</b>, measured in number of elements rather than bytes, is given by the third parameter, which defaults to 512. Client code can specify any size here, to best match the most common required array size, for maximum performance benefit.</p>


<p>For all compilers other than Borland's, the <b>auto_buffer</b> derives from the allocator and takes advantage of the Empty Base Optimization (EBO) (<a href="http://www.gotw.ca/publications/mill07.htm">http://www.gotw.ca/publications/mill07.htm</a>) when appropriate. However, with Borland 5.5 and 5.6, this causes such significant performance degradation (actually worse than the <b>malloc()/free()</b> scenario in all cases) that an allocator is, instead, shared between all instances [6].</p>



<h3>Performance</h3>


<p>I measured <b>auto_buffer</b>'s performance via a test program on both Linux (single-processor desktop 800 MHz, 128 MB) and Win32 (single-processor desktop 2 GHz, 512 MB). It was tested against memory-allocation types such as: stack variables; heap using against <b>malloc()/free()</b>; heap using operator <b>new/delete</b>; dynamic stack allocation using <b>alloca()/alloca()</b>; VLAS; and <b>std::vector</b>.</p>


<p>For each allocation type, the program allocates a block, the size determined by the first program parameter, accesses a byte within it (to prevent the compiler optimizing away the loop), and then deallocates it. The operation is repeated for a given number of times determined by the second program parameter. Since the program's <b>auto_buffer</b> class is parameterized with the default 512 for the internal buffer size, the two sizes tested are above and below this; specifically, 100 and 1000 bytes repeated 10 million times. (Tests of 10 and 100 bytes with a 64-byte buffer size yielded virtually identical relative results for all compilers.)</p><p>

<b>auto_buffer</b> does a test (comparing the size of its internal buffer against the requested buffer size) before making any heap allocation. In circumstances where it must allocate from the heap, performance is less than going straight to the heap. Therefore, the purpose of the performance test is to quantify the presumed superiority where it uses its internal buffer, and the presumed inferiority where it allocates from the heap.</p>

<p>The resultant times were normalized with respect to the time taken for<b> malloc()/free()</b>, and are expressed in percentage terms; see <A HREF="0312wilsont1.htm" target="_BLANK">Table 1</A>. Except where marked, they are on the Win32 platform. For figures marked by an asterisk (*), it proved impossible (on both Linux and Win32) to obtain meaningful performance figures due to stack exhaustion crashes on any useful level of loop repeats. This was the case for all <b>alloca()</b> implementations as well as VLAs with Digital Mars (which is expected given its implementation over <b>alloca()</b>). The surprising thing is that GCC's VLA did not suffer stack exhaustion (as its <b>alloca()</b> did), which implies it uses a different technique. Though about five times slower than stack memory, it was twice as fast as <b>auto_buffer</b>. (My suspicion, therefore, is that it adjusts the stack pointer back when the scope, rather than the function, is exited &#151; thus avoiding the exhaustion.)</p>

<p>When the requested size of the buffer causes heap allocation, the cost is less than 113 percent for all compilers, except Borland, where it is 213 percent. This cost is not in any way detrimental, since the wise programmer will specify the template size parameter such that the vast majority of allocations will fit within that size. The overall effect in such circumstances will be a significant net performance gain.</p>

<p>The vector performance is 100-370 percent and 129-2600 percent for the two scenarios, so there's no doubt that <b>auto_buffer</b> is vastly more efficient than <b>vector</b>. But remember that <b>auto_buffer</b> is not resizable, nor does it (currently) support nonPOD types. Furthermore, it does not initialize the memory it acquires (for POD types). All of this is by design &#151; I wanted one-time allocation and wanted it fast &#151; and so it's fair to draw a comparison with <b>vector</b>, but only in these circumstances. <b>auto_buffer</b> is not a replacement for <b>vector</b>, and it is not intended to be.</p>



<h3><b>auto_buffer</b> Disadvantages</h3>


<p><b>auto_buffer</b> has two disadvantages. The first is that the current implementation &#151; designed for maximum efficiency &#151; cannot be used with nonPOD types. However, the reason for the existence of the class, and where it has found its main use thus far, is as dynamic local character (ANSI or Unicode) arrays, so it fully satisfies its requirements. Hence, though there are plans to expand its capabilities to work with nonPOD types &#151; using traits/policies to efficiently discriminate any needed (un)initialization &#151; they are not yet a high priority. Indeed, before releasing such an updated version, I would want to ensure that reimplementation to policy-based initialization (or lack thereof) would not have a deleterious effect on performance for currently supported types. <b>auto_buffer</b> would also need to be significantly faster than <b>std::vector</b> for nonPOD class types; if it weren't, then there'd not be much point!</p>


<p>The second disadvantage is more significant, subtle, and potentially expensive. Although it can be advantageous to use <b>auto_buffer</b> as a member of other classes, you should only do so in cases where the most common allocation size is exactly, or close to, the internal buffer size. If the typical size is larger than the internal buffer, most instances of the class will not use their internal buffer at all. If the typical size is much less than the internal buffer, most instances of the class will not use most of their buffer. In either case, when the containing class is on the heap, the <b>auto_buffer</b> and its internal buffer will also exist on the heap, potentially wasting a large amount of memory.</p>



<h3>Conclusion</h3>


<p><A HREF="0312wilsont2.htm" target="_BLANK">Table 2</A> lists the relative merits of the various memory-allocation schemes. The advantages over the other schemes are efficiency, dynamic size, platform independence, compiler independence, flexibility of allocation scheme, and the avoidance of stack-checking code. Clearly, when used wisely, <b>auto_buffer</b> has the best mix of features of all the schemes for automatic arrays of fundamental and POD types. In addition, <b>auto_buffer</b> can be used in composition, although care must be exercised. </p>




<h3>Acknowledgments</h3>


<p>Thanks to Randy Meyers for enlightening me about VLAs, in particular, the cross-language issues. Thanks also to Greg Comeau of Comeau Computing, and Walter Bright of Digital Mars, for lending their expertise here and there.</p>




<h3>References</h3>


<p>[1] Randy Meyers, "The New C: Why Variable Length Arrays?" <i><i>C/C++ Users Journal</i></i>, October 2001.</p>

<p>[2] I don't like implicit conversion operators as a rule, but in this case, they're required to achieve array-like behavior, emulating built-in arrays' names being synonyms for the address of their (first) elements.</p>

<p>[3] Please forgive the nonstandard preprocessor symbols beginning with underscores. This is a bad habit I'm gradually shedding. From STLSoft v1.7.1 onwards, they will not appear.</p>

<p>[4] Practice wins out over theory here. In principle, <b>offsetof()</b> is undefined for nonPOD types, so applying it here is not strictly valid. However, for all compilers supported by STLSoft, it does what's required, so it is used. If STLSoft is ported to a compiler that lays out classes such that this would not apply, then<b> auto_buffer</b>, or the macro, will be rewritten accordingly.</p>

<p>[5] Though index operators would afford the possibility of some simple index validation (for example, assert or even exception), I felt that this still does not make up for the concomitant restriction of client code expressions. </p>

<p>[6] Thankfully, allocators are not allowed to act as if they have per-instance data, and most actually do not; but if they did, there's a slight possibility of a multithreading race condition here, which is not pleasant. </p>

<hr><I>
<b>Matthew</b> is a software development consultant for Synesis Software, author of the STLSoft libraries, and author of </i>Imperfect C++<i> (to be published by Addison-Wesley, 2004). He can be contacted via <a href="mailto:matthew@synesis.com.au">matthew@synesis.com.au</a> or <a href="http://stlsoft.org/">http://stlsoft.org/</a>.</I>

<hr>

</body>
</html>
