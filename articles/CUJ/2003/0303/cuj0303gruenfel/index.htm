<HTML><HEAD><TITLE>Easy Analog Data Compression</TITLE></HEAD><body bgcolor="#FFFFFF" text="#000000"><H2>Easy Analog Data Compression</h2><h3>Stephan Gr&uuml;nfelder</h3><i>A low-footprint compression technique for when efficiency really matters.</i>When I needed to implement a lossless data compressor for 16-bit ECGs (electrocardiograms), EEGs (electroencephalograms), and other medical signals in C on a 16-bit integer processor, I discovered how little value the majority of compression techniques were for my problem. Nearly all of the techniques I found on the Internet and in computer science literature rely on two fundamental techniques. The first technique consists of repetitive patterns (known as the Lempel-Ziv family of algorithms, such as <B>gzip</B>) that can be encoded in an efficient way. The second technique uses a family of approaches (known as entropy coding or Huffman coding) that exploit the fact that the individual numbers/letters found in the raw data are usually not equally probable. Both techniques make use of look-up tables -- the larger the tables, the better the chance for good compression ratios. That was another challenge in my project. My algorithm should use less than 2KB of data memory for up to four independent channels and should not need more than 1,000 instruction words. I needed something very simple but effective.<P> To the untrained human eye, an ECG appears to be extremely repetitive (see   <a href="cuj0303gruenfel_f1.htm">Figure 1</a>). Each heartbeat yields a waveform   that looks very much like the preceding heartbeat, but the data values differ   slightly. The barely visible differences of the high-resolution signals make   the compression with standard techniques nearly impossible. In an experiment,   I assessed that a standard version of <B>gzip</B> reduces the size of my ECG   by barely five percent although I consider myself quite healthy. An EEG curve   (see <a href="cuj0303gruenfel_f2.htm">Figure 2</a>) is so irregular that <B>zip</B>,   <B>pack</B>, and others do an even worse job.<P>However, I know something that Lempel, Ziv, and Huffman do not. My data consists of digitized signals originating from an analog source. These signals are known to exhibit relatively few big jumps in their signal amplitude. For signals that change slightly over time, "delta encoding" (i.e., the coding of the signed differences between two sample values instead of the sample values themselves), saves a lot of memory. Suppose, for example, you know that the absolute difference between two adjacent samples of the 16-bit signal never exceeds 1,023; you can code the signal as one absolute value of 16 bits followed by the delta values of 11 bits each.<P><h3>Adaptive Delta Encoding</h3>The problem with the signals shown in Figures 1 and 2 is that the delta values (i.e., the differences between two adjacent samples) vary comparatively slowly, but vary a lot. Sometimes as few as 4 or 5 bits suffice to code the delta values; sometimes 12 to 14 bits are needed. In order to increase compression power, I made the bit width used by the delta code adaptive. The encoder algorithm detects when the delta values need fewer bits than the current bit width of the delta code. If that is the case, it reduces the number of bits used to code the delta values. In my program, I do this by checking if the two most significant bits in each of the last <B>BIT_REDUCTION_LENGTH</B>-produced codes are identical (see function <B>storeDelta16</B> in <a href="cuj0303gruenfel_l1.htm">Listing 1</a>). If this is the case, the bit width of the delta code, <B>iBitWidthOfCode</B>, is decreased by one. The decoder does exactly the same, and thus it is not necessary to mark the place where <B>iBitWidthOfCode</B> is reduced.<P>If a new delta value cannot be represented by the current bit width, then <B>iBitWidthOfCode</B> must be increased. This time the decoder cannot anticipate the change and must be informed. The encoder does this by storing a reserved letter <B>piOVFLCODE[]</B> that exists for every possible delta bit width. When the decoder encounters such a reserved letter, it knows it must increase its <B>iBitWidthOfCode</B> by one. To reduce excessive coding of <B>piOVFLCODE[]</B> when <B>iBitWidthOfCode</B> gets so small that merely signal noise is coded, I have introduced the lower bound <B>MIN_BIT_WIDTH</B>.<P>I also have reserved two other letters: <B>piALIGNCODE[]</B> indicates that the next code is aligned with the next word boundary, an indication I need to implement flushing; <B>piABSCODE[]</B> indicates that an absolute code (a raw sample value) follows, aligned with word boundaries.<P>These reserved letters are chosen to flank the largest and smallest possible two's complement numbers of the delta code. If a delta value matches a reserved letter, the bit width must be increased before the value can be encoded without ambiguities. Hence, I use the reserved letters in <B>storeDelta16</B> to check if <B>iBitWidthOfCode</B> must be increased. If this is the case, then the appropriate <B>piOVFLCODE[]</B> is stored, and <B>storeDelta16()</B> is called recursively with an increased bit width.<P>The function <B>storeX</B>, called in <B>storeDelta16</B>, concatenates the codes of variable bit lengths that are passed as parameters. As soon as a <B>WORD</B> (16 bits) is complete, it passes it to <B>writeCode</B>. Any code fraction that is cut off when completing a <B>WORD</B> is kept in the buffer <B>iXcode</B>. The function <B>writeCode</B> represents the interface of a device driver, which is supposed to accept 16-bit-entities only. In my application, data needs to be transmitted via a serial interface and written to flash memory. An additional requirement is to be able to flush the buffer <B>iXcode</B> at any time. This is possible by calling <B>flushX</B>, which checks if there is anything buffered in <B>iXcode</B>. If this is the case, it will add the aforementioned <B>piALIGNCODE[]</B> to the output and pass the content of <B>iXcode</B> to <B>writeCode</B>, although not all of its 16 bits are meaningful. When the decoding algorithm reads the reserved letter <B>piALIGNCODE[]</B>, it knows that any undecoded remaining bits of the <B>WORD</B> at hand can be dismissed, and the next delta code will be found in the next <B>WORD</B>.<P><h3>Prediction</h3>The functions I have described so far code the delta values with an adaptive bit width, but do not yet compute the values themselves. The function <B>encoder_storeADEPT</B> does this. In the simplest form, this function would just compute the difference of the sample passed as parameter and the sample received previously and use this difference as parameter for a call to <B>storeDelta16</B>. This already yields nice compression ratios for many signal types.<P> However, if you can extrapolate the signal curve and predict the new sample   value with reasonable accuracy, then you may encode the difference between the   prediction and the actual sample value to further reduce the size of the delta   code. The function <B>encoder_storeADEPT</B> shown in <a href="cuj0303gruenfel_l1.htm">Listing   1</a> performs these compression steps.<P>I call the compression technique ADEPT (adaptive delta encoding with prediction). The prediction step itself may be chosen according to a model of the signal, heuristics, similarities to other signals recorded simultaneously, etc. Signal prediction is a complicated matter that could fill entire books or at least book chapters [1]. I decided to implement a simple linear extrapolator instead of spending the rest of my days trying to understand those books. The predictor given here records the history of the last three samples, tries to fit a line through them, and extrapolates linearly.<P>As you can see in <B>encoder_storeADEPT</B>, I have arranged the computations of the predictor such that first I do the division and then the multiplication. This makes an integer overflow less likely in case you rewrite the function to deal with 32-bit data. If the prediction fails completely, it may happen that the generated delta code cannot be represented as a 16-bit number anymore. Then the algorithm resets the predictor, writes the reserved letter <B>piABSCODE[]</B>, and finally writes the sample directly, word aligned, without delta encoding. If no absolute code is needed, <B>encoder_storeADEPT</B> passes the delta value to <B>storeDelta16</B>.<P>Note that the simple predictor presented above assumes a rather continuous signal. It improves the compression for EEG signals or breathing curves, but performs very poorly for ECGs. Having no prediction step at all is sometimes better than having an inappropriate one: heartbeats of a very high amplitude may irritate the presented predictor so much that pure adaptive delta encoding outperforms adaptive delta encoding with prediction.<P><h3>Decompression</h3>Since I needed to run the ADEPT decompressor on a PC, I opted to implement it in C++. The class <B>TDecompressor</B> shown in <a href="cuj0303gruenfel_l2.htm">Listing 2</a> resembles the compressor. The constructor takes a reference to a vector as an argument that holds the decompressed values. The decompressor is fed by passing the codes as parameters to the member function <B>Decode</B> in the order they are received or read. This function checks first if the code passed previously indicates that an absolute code would follow. In this case, the value of <B>miAbsValueCoded</B> would be one, and the passed code may be directly appended to the vector of decompressed values. In the more probable case, the passed code must be concatenated with the remaining fractions of the last code and split into the respective delta codes to be able to reconstruct the input values of the encoder.<P><B>Decode</B> does this division into delta values in a <B>while</B> loop. As a bitmask for this job, it uses <B>mpiABSCODE[]</B>. The obtained delta code <B>iSubword</B> might represent a negative number, but it might be a positive 32-bit two's complement number and therefore must be sign extended. Think of the number <B>0xFF</B>. In an 8-bit two's complement representation, it is interpreted as -1. It must be negative because the MSB (most significant bit) is 1. In a 32-bit system, this is interpreted as the positive number 255. Its MSB is 0. To interpret the number correctly in the 32-bit system, the bits to the left of the sign bit of the small-scale representation must hold a copy of this sign bit. In our example, <B>0xFF</B> will be converted to <B>0xFFFFFFFF</B>, which is interpreted as -1. This technique is called sign extension.<P>After the sign extension of <B>iSubword</B>, <B>Decode</B> checks if the obtained code is a reserved letter. If so, it changes the object's state to be able to react appropriately; else it reconstructs the original data by calling the member function <B>DeltaDecode</B> and stores the result in the vector named by the constructor.<P> <a href="cuj0303gruenfel_l3.htm">Listing 3</a> shows an example program that   uses ADEPT compression and decompression.<P><h3>Performance and Application</h3>In general, ADEPT may be used for any kind of data. However, due to its nature, it will perform badly on sample data from a very irregular signal with many jumps and big delta values. A worst-case scenario is a random signal that covers the full 16-bit range. The predictor is hopelessly lost and many absolute values must be coded; the encoded data is about 130% of the size of the raw data.<P>ADEPT performs well if the difference between two subsequent samples is small and/or the signal form is fairly linear. In the ideal case (i.e., when a strictly linear signal is compressed), ADEPT exhibits a compression ratio of <B>16:MIN_BIT_WIDTH</B>. With a reasonable value for the minimal bit width, it is obvious that the best compression ratio that can be achieved with ADEPT is between 16:3 and 16:4, or 18 and 25percent, respectively.<P>Obviously a noise-free, repetitive signal can be more compressed by <B>zip</B> and <B>pack</B> than by ADEPT, which cannot go beyond the mentioned threshold. However, signals coming from natural sources are rarely exactly repetitive, therefore the aforementioned bad performance of <B>gzip</B> on an ECG. However, a comparison of ADEPT to <B>gzip</B> is unfair. Each of the algorithms has a very different application domain. Hence the attempt to compress an EEG with <B>gzip</B> must fail, and you should compare ADEPT to other techniques.<P>The strengths of ADEPT are most of all simplicity, no need for floating-point arithmetic, small memory requirements, and zero delay. The latter needs explanation: compression techniques based on so-called FIR filters inherently introduce a time delay. This may be very annoying when real-time performance is desired.<P> An obvious weakness of the technique is its dependency on scale (i.e., on   the gain factor of the analog/digital converter); a higher gain value will lead   to larger delta values for the digitized data and thus needs a wider code. Simplicity   pays its price when you compare ADEPT to specialized algorithms, as shown in   <a href="cuj0303gruenfel_t1.htm">Table 1</a>. Note that such comparisons are   very difficult, because they are hardly ever objective. This is especially true   when different input data is used to assess the performance of the respective   methods, which is the case here.<P> Changes of the signal scale may increase or decrease the compression ratio   of ADEPT given in <a href="cuj0303gruenfel_t1.htm">Table 1</a> by about five   percent. Most likely, the same is true for the two reference ECG compressors,   but no figures were given in [2]. The EEG compressor in [3] is probably the   most powerful known, but it is so specialized that it cannot encode arbitrary   signals without losses.<P>A nice feature of the algorithms ALZ77 and ADPCM Subband Coding is a smooth transition from lossless to lossy compression, a step that is difficult with DPCM, ADEPT, or entropy coding. When tuning ADEPT, you should note the comparatively little compression gain when improving the prediction because of the logarithmic dependency between the delta code width and the prediction error.<P><h3>Conclusion</h3>The ADEPT algorithm is a variant of the Differential Pulse Code Modulation [4]. When comparing ADEPT to state-of-the-art ECG and EEG compressors, you will find that its compression power comes close to their levels. Its advantages are versatility, easy implementation on any integer processor, speed, and a very small memory footprint. The downloadable source code (available at &lt;www.cuj.com/code&gt;) provides you with an easy-to-use tool that allows you to efficiently compress digitized data from continuous signals on virtually any hardware for which a C compiler is available.<P><h3>Further Reading</h3>[1]  John G. Proakis and Dimitris G. Manolakis. <I>Digital Signal Processing: Principles, Algorithms, and Applications</I>, 3rd Edition (Prentice Hall, 1996), Chapter 11.<P>[2]  R. Nigel Horspool and Warren J. Windels. "An LZ Approach to ECG Compression," Proceedings of the 1994 IEEE 7th Symposium on Computer-Based Medical Systems. See also &lt;www.csr.uvic.ca/~nigelh/Publications/ECG-compression.pdf&gt;.<P>[3]  Zlatko Sijercic, Gyan Agarwal, and Charles Anderson. "EEG Signal Compression with ADPCM Subband Coding," Proceedings of the 39th Midwest Symposium on Circuits and Systems, Aug 1996. See also &lt;www.cs.colostate.edu/~anderson/res/eeg/&gt;.<P>[4]  John G. Proakis. <I>Digital Communications</I>, 3rd Edition (McGraw-Hill, 1996), Chapter 3.<p><h3>About the Author</h3>Stephan Gr&uuml;nfelder studied computer science at the Vienna University of Technology and has been developing software for embedded systems for seven years. His interests include signal &amp; image processing, rock climbing, skiing, and sheep farming. He can be reached at <B>stephan.gruenfelder@gmx.at</B>.<p></BODY></HTML>