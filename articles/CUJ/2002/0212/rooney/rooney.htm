<HTML>   
     <HEAD>
<TITLE>December 2002/A Templated Library for Fast Multiplexing</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../tocdec.htm"></A><FONT COLOR="#FF0000">   Internet &amp; Network Programming</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">A Templated Library for Fast Multiplexing</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Christopher Rooney</FONT></H3>

<BLOCKQUOTE>
<p>If the number of your network connections hits the stratosphere, then you need this.</p>
</BLOCKQUOTE>

<HR>
<BLOCKQUOTE>

<H3><FONT COLOR="#000080">Introduction</FONT></H3>

<p>As the Internet continues to grow, the traditional Unix network programming paradigms show more and more signs of age. What functioned perfectly well for 30 or 50 simultaneous connections fails spectacularly when asked to handle 10 or 15 thousand. The recognized (and portable) way to achieve better scaling behavior is to use standard multiplexing I/O (i.e., <B>poll</B>). Even multiplexing applications, though, can show considerable signs of strain on servers that demand high performance while scaling to thousands of concurrent connections. Fortunately, Unix platform vendors have recognized this failing and responded with more scalable alternatives to <B>poll</B>. Of course, Unix vendors being Unix vendors, their responses are platform dependent. Since the various schemes necessarily deliver at least as much functionality as standard <B>poll</B>, it is possible to abstract their behavior into a well-behaved class that provides a simple interface for a polling engine. This article presents a class that uses a templated delegate pattern instead of subclassing to achieve platform independence. The class provides implementations for <B>/dev/poll</B>, which is standard in Solaris 8 and available as a patch for Linux; the <B>kqueue</B>/<B>kevent</B> model, standard in FreeBSD and OpenBSD; and traditional <B>poll</B>.</p>

<H3><FONT COLOR="#000080">The Dark Ages</FONT></H3>

<p>Traditional approaches to Unix network programming involve using a separate thread or process to handle each concurrent client connection. The first problem here is that context switching costs, especially switching between contexts that are blocked in I/O calls. It&#146;s easy to overlook the high price of context switching, since the behind-the-scenes processing it necessitates is transparent to the programmer, but the costs are serious: stack frames have to be saved and restored, memory pages swapped, and registers reset. Preforking (or prespawning) and allowing contexts to serve connections in sequence will amortize the cost of forking, but not of context switching. Unfortunately, context switching is not the only or even most serious problem with an approach that uses a separate thread of execution per client. If you want to handle several thousand connections simultaneously, you need an equivalent number of threads or processes, and current operating systems are just not prepared to handle that many.</p>
<p>Take Apache as an example: say you compile a fairly lean Apache with shared modules. Each instance goes about 2 MB, and you have 1 GB of free RAM (ignore disk swap for this example). The theoretical maximum number of simultaneous connections is 500, as each connection requires an <B>httpd</B> instance. Servers that use threads instead of processes may be able to squeeze more connections into the same amount of memory, but are obviously still constrained to some relatively low upper limit. The presented polling engine allows and encourages single-threaded programming.</p>

<H3><FONT COLOR="#000080">The Way of the Forefathers</FONT></H3>

<p>Traditionally, multiplexing I/O uses the <B>poll</B> system call:</p>

<pre>
int poll(struct pollfd *fds, int nfds, int timeout);
</pre>

<p>It accepts an array of <B>pollfd struct</B>s, which contains fields for the descriptor, a <B>short</B> integer representation of events in which the caller is interested, and another <B>short</B> for the ready events returned from the kernel. There may be holes in the array, signified by a negative descriptor, which are examined and then ignored by the kernel. Polling in this fashion necessarily scales linearly with the number of <B>pollfd</B>s to be checked, even if these <B>struct</B>s are in holes (have descriptors of -1). When the number of descriptors to be checked gets into the thousands, performance degrades noticeably. In addition, calling code must loop through the entire array, or at least until it discovers as many ready descriptors as the <B>poll</B> call reports. So if there are ready descriptors only in the first and hundredth indices of the <B>pollfd</B> array, both the kernel and client code must inspect and test 98 unready indices to find 2 ready ones. These failings of traditional <B>poll</B> generated much comment on mailing lists and newsgroups, and the kernel implementers heard. And in grand Unix tradition, they had similar, yet incompatible answers.</p>

<H3><FONT COLOR="#000080">The Way of the Sun</FONT></H3>

<p>Sun introduced the first response to the polling problem as a patch to Solaris 7, which became a standard feature in Solaris 8. Their innovation was <B>/dev/poll</B>, a polling &#147;driver&#148; (their description), which a caller <B>open</B>s, <B>write</B>s <B>pollfd</B> <B>structs</B> to, and <B>close</B>s. Since there is a nice orthogonal <B>close</B> to match <B>open</B>, then the opposite of <B>write</B> must be <B>read</B>, right? Of course not. Reading is done with <B>ioctl</B>, to which a caller passes the <B>/dev/poll</B> descriptor, the integer constant <B>DP_POLL</B>, and a <B>dvpoll struct</B>. This <B>dvpoll struct</B> is a hack to conform to the <B>ioctl</B> interface, which contains a <B>pollfd</B> pointer, the number of <B>pollfd</B>s represented by the pointer, and an integer timeout. This is almost irredeemably ugly, but the performance improvement over <B>poll</B> is not to be sneezed at and grows as the number of descriptors increases. The benchmark at <a href="#1">[1]</a> shows a 7 to 1 speed advantage (over traditional <B>poll</B>) at 100 descriptors growing to nearly 400 to 1 at 10,000 descriptors. In my parish, those kinds of numbers redeem a gang of inelegance.</p>

<H3><FONT COLOR="#000080">The Way of the Free</FONT></H3>

<p>Jonathan Lemon, who implemented kernel queues in FreeBSD 4.1, also used the idea of opening and reading from a descriptor. But his vision was a little broader than his nameless counterpart at Sun, as the <B>kqueue</B> mechanism is intended not only for sockets and descriptors but also process events (exits, forks, etc.), signals, <B>vnode</B> events (deletions, writes, appends, etc.), and others. To use a kernel queue, you get a descriptor from a call to <B>kqueue</B>. Then you call <B>kevent</B> to have the kernel read from an array of <B>kevent</B>s and fill another with ready descriptors. Subsequent calls to <B>kevent</B> only need to pass new and changed interest info, while the kernel keeps track of which descriptors are ready. This is a pretty neat and elegant system. Its aims at genericity dictate that it uses the more expansive <B>struct kevent</B> instead of the familiar <B>pollfd</B>. A small problem for C++ coders, though, is that the <B>kevent struct</B> and system call are identically named so that variables must always be declared to be of type <B>&#147;struct kevent&#148;</B> to avoid having calls to <B>kevent</B> interpreted as constructors and the attendant hilarity of error messages that appear to make no sense.</p>
<p>The <B>kqueue</B> system and <B>/dev/poll</B> both rely on a kernel technique called &#147;hinting,&#148; in which files (sockets, in this case), upon becoming ready for an event, signify that readiness to the kernel. If any user code has specified interest in the event, the kernel stores the readiness indicator until asked.</p>

<H3><FONT COLOR="#000080">But, But, assert(LINUX==UNIX) Fails...</FONT></H3>

<p>Which of these will make its way into the Linux kernel? Looks like neither. There exist unofficial patches to get <B>/dev/poll</B> <a href="#2">[2]</a> under Linux, but Linus himself has said <I>scathing</I> things about both <B>kqueue</B> and <B>/dev/poll</B>, although what he suggests instead is not completely dissimilar to <B>kqueue</B>, at least to the user-space coder <a href="#3">[3]</a>. Various suggestions, of varying elegance, on alternate interfaces for faster polling have been suggested on the Linux lists and newsgroups. What exists now for Linux, though, and around what, so help me, a lot of people seem to be writing code, are the POSIX real-time signals. The technique of applying real-time signals to multiplexing is so ugly it offends delicate sensibilities, so I will only describe its fatal flaw: when there are many concurrent connections, the signal queue can overflow and you must revert to using <B>poll</B>. So you either have to maintain a hopefully unnecessary <B>pollfd</B> array or construct one just when you are being creamed with client requests. And <B>poll</B>, of course, remains less than ideal.</p>
<p>If you are determined to use Linux for high concurrency TCP servers, you can either get the best <B>/dev/poll</B> patch you can find or set yourself up for heartache.</p>

<H3><FONT COLOR="#000080">Enough Hooha, Let&#146;s See Some Code</FONT></H3>

<p>The poller engine&#146;s interface presented here is an extremely simple one (<a href="list1.htm">Listing 1</a>). It presents only three functions (and one overload) for adding, clearing, and polling the descriptors and events of interest. With <B>add</B>, you register a callback, which is simply a function object whose <B>operator()</B> returns either <B>KEEP</B> or <B>REMOVE</B>. <B>poll</B> determines which descriptors are ready to read and write and executes the callbacks associated with the ready ones. If the callback returns <B>REMOVE</B>, the descriptor and its associated callback are cleared. Generally, if the socket is not closed, it is expected that a callback <B>add</B>s another before returning <B>REMOVE</B>. In this way, the engine is self-managing; <B>clear</B> may never be needed.</p>
<p>The steps for accepting a connection hardly ever vary; code accepts a connection and gets a descriptor for the new socket, sets the descriptor to non-blocking, and registers a <B>read</B> callback for it. The library provides this standard <B>accept_callback</B> parameterized over a reader <B>callback</B>. Given that, the usual structure of a program using the polling engine is:</p>

<pre>
define at least one reader callback
define at least one writer callback
create a poller
open a listening socket
register an accept_callback&lt;reader_callback&gt; for the listening socket
while (1)
  poller::poll()
  unrelated processing (if necessary)
end while
</pre>

<p>The callback pointers used to register with the polling engine are wrapped in a smart pointer based on the Boost library&#146;s implementation <a href="#4">[4]</a>, which means you allocate a callback on the heap and pass in its address, and the polling engine takes responsibility for deleting it. It may seem a little odd at first, <B>new</B>ing objects and not deleting them, but hey, you could be writing Java and feel odd all the time.</p>
<p>The callbacks are implemented as function objects to allow them to maintain state of which the polling engine is ignorant. Function object callbacks can efficiently hold as much state as desired because only (smart) pointers are ever copied by the polling engine. Scott Meyers describes this technique for storing stateful function objects in standard containers in <I>Effective STL</I>, Item 38 <a href="#5">[5]</a>. These callbacks can therefore be much more full featured than conventional function-based callbacks. In particular, function object callbacks can have knowledge of other callback instances. A reader could know which writer is associated with its socket and append strings to a FIFO member of the writer. Alternatively, a callback could do all the reading and writing for a connection. Included with the source code (available for download from &lt;www.cuj.com/code&gt;) is a simple server that defines a reader that exists for the life of a connection, creating a new writer for every response. These stateful callbacks can do pretty much whatever you want, so go ahead; <B>theWorld == you.getOyster()</B>.</p>

<H3><FONT COLOR="#000080">Delegates</FONT></H3>

<p>As <a href="list1.htm">Listing 1</a> shows, the poller class delegates almost all work to the class over which it is parameterized. This pattern is similar to the use of traits classes, but the emphasis is reversed: a traits class generally defines <B>typedef</B>s and provides low-level functionality, while all the logic and algorithmic complexity is in the parameterized class. In the Delegate pattern <a href="#6">[6]</a> used by this polling engine, the parameterized class is merely an interface, similar to a pure virtual base class, except that none of the run-time performance costs of polymorphism are incurred. Note that in this instance we still need to do a little <B>autoconf</B> magic to compile conditionally, because no current system has <B>/dev/poll</B> and <B>kqueue</B> headers, but this is not inherent in the pattern.</p>
<p>All three delegates have similar logic. <B>poll</B> is called; it checks its descriptors and processes the ready ones. During this processing, additional descriptors may be added (when an <B>accept</B> socket is ready), and callbacks may signify intent to modify or purge themselves. Changes are stored in a delegate-specific way and processed again the next time <B>poll</B> is called. A cute optimization is that callbacks are executed before adding, as frequently they can be immediately removed and avoid further processing. For example, client sockets are usually writable when their requests are sent, well before their requests are received, read, and parsed.</p>
<p>Standard library containers (and the <B>hash_map</B> extension <a href="#7">[7]</a>) are used exclusively in the delegates. Even the arrays needed by system calls are maintained as vectors and zeroth element references passed as pointer addresses.</p>
<p>The <B>kqueue</B> and <B>/dev/poll</B> delegates actually have simpler implementations, because their underlying operating system constructs maintain state. A look at two delegates&#146; implementations of <B>remove</B> (<a href="list2.htm">Listing 2</a>) shows some of the additional complexity the older <B>poll</B> requires. The newer, stateful approaches are able to aggregate the events that they have been asked to monitor. For the stateless <B>poll</B>, you have to do the aggregation yourself, or pay the price of evaluating the same descriptor multiple times. Therefore, the member data structure and the manipulation code are considerably more complicated. Additionally, a priority queue (<B>available_</B>) is used by the generic poller to store the lowest available index, as you want to keep the <B>pollfd</B> array as small and densely populated as possible, because of the linear scaling behavior mentioned above.</p>

<H3><FONT COLOR="#000080">Exceptions and Expectations</FONT></H3>

<p>The library purposefully only throws a small number of exceptions and expects clients to ignore most of them. Theoretically, setting a socket to non-blocking mode could fail, because it uses <B>fcntl</B>, which could return -1, although it is hard to imagine a circumstance in which changing the blocking flags for a valid socket will fail. Because the socket is always set to <B>non_blocking</B> immediately after creation, it will never be invalid. The convenience function <B>set_non_blocking</B> throws an exception if it can&#146;t execute, but client code can in good conscience ignore it, as does the provided <B>accept_callback</B>.</p>
<p>Initial setup is another matter, however. If the delegate-specific poller cannot be created (i.e., the call to open(/dev/poll) or kqueue fails), an exception will be thrown that should be caught. The convenience function get_listen_socket combines five related system calls into one function that returns the descriptor on which the program listens for incoming connections. If this function throws, steps should be taken. Probably, the only possible response to one of these circumstances is a log message and graceful exit. This exception-handling code, though, can be kept outside the main polling loop.</p>
<p>The imaginatively titled exception class adds <B>which</B> and <B>where</B> to the <B>what</B> it inherits from <B>std::exception</B>. The complete implementation is shown in <a href="list3.htm">Listing 3</a> since it is generally useful as a translation layer between the POSIX <B>errno</B> framework and C++ exception handling and might be of interest to programmers writing non-networking code. Did you know that <B>errno</B> might be implemented as a macro and mysteriously break all kinds of things under Linux that work without problems on four other Unix versions? Consider reading a magazine the easy way of finding out.</p>

<H3><FONT COLOR="#000080">Caveats</FONT></H3>

<p>Is that really all there is to it? Can you be up and running and serving super high traffic on multiple platforms with just three method calls and a couple function objects? Well, not exactly. First you have to configure your system to allow a suitably large amount of descriptors per process, and possibly do other system tuning. Such explanation is beyond the scope of this article, but full information on necessary tweaks (and much, much more) can be found on Dan Kegel&#146;s excellent C10K page <a href="#8">[8]</a>.</p>
<p>The Solaris header file for <B>/dev/poll</B> has an interesting little quirk. A <B>}</B> near the end of the file needs to be commented out to compile with g++. This is interesting as it is within an <B>#ifdef __cplusplus</B> condition. Presumably it works with Sun&#146;s toolset.</p>
<p>The <B>kevent</B> <B>syscall</B> waits the entire timeout, even if there are ready descriptors. This is different than <B>poll</B> and <B>/dev/</B>poll, and frankly, I am hard pressed to think of a situation in which this is desirable behavior. For portable speed, you probably want to pass zero timeouts.</p>
<p>The main development environment for this polling engine was OpenBSD 2.9 (Intel) <a href="#9">[9]</a>. The <B>/dev/poll</B> delegate was written on Solaris 8 (Intel). The <B>kqueue</B> system was also tested on FreeBSD 4.3 (Intel). The generic <B>poll</B> piece was compiled on Red Hat Linux 7.1 (Intel) and NetBSD 1.5.1 (AMD).</p>

<H3><FONT COLOR="#000080">Conclusion</FONT></H3>

<p>While money dribbles out of the Internet, consumer demand keeps streaming in. Consequently, there exists a need to be able to network more efficiently, to increase capability without purchasing new hardware. The currently much ballyhooed peer-to-peer models, in particular, will require maximal per-node network efficiency for commodity machines. The polling engine presented here provides a simple paradigm for writing high-powered, highly scalable Unix networking applications.</p>

<H3><FONT COLOR="#000080">Notes</FONT></H3>

<p><a name="1"></a>[1]  &lt;www.kegel.com/dkftpbench/Poller_bench.html&gt;</p>
<p><a name="2"></a>[2]  I have just become aware of a new <B>/dev/poll</B> patch (actually called <B>/dev/epoll</B>) for Linux that is supposed to perform wonderfully. I have not examined it. See &lt;www.xmailserver.org/linux-patches/nio-improve.html&gt;.</p>
<p><a name="3"></a>[3] &lt; http://groups.google.com/groups?hl=en&amp;safe=off&amp;th=3c85e39303c716a2,62&amp;seekm=fa.fb3h7jv.90o42k%40ifi.uio.no&gt;. Daunting URL, I know. Search google groups for &#147;linus /dev/poll interface _hell_&#148;. It should be the only thread that returns.</p>
<p><a name="4"></a>[4]  &lt;www.boost.org/libs/smart_ptr/&gt;</p>
<p><a name="5"></a>[5]  Scott Meyers. <I>Effective STL: 50 Specific Ways to Improve Your Use of the Standard Template Library</I> (Addison-Wesley, 2001).</p>
<p><a name="6"></a>[6]  Since writing this article and code, I have read Andrei Alexandrescu&#146;s excellent <I>Modern C++ Design</I> (&lt;www.moderncppdesign.com&gt;) and recognize what I call Delegates as examples of what Andrei calls policy-based design. I will defer to Andrei and the inevitable and assume the pattern will be called the Policy Pattern, although I think Delegate remains a better name for it. </p>
<p><a name="7"></a>[7]  Only the SGI (g++, STLPort) style of <B>hash_map</B> is used, as I have no access to the DinkumWare STL. If someone sends me a patch, I will apply it.</p>
<p><a name="8"></a>[8]  &lt;www.kegel.com/c10k.html&gt;</p>
<p><a name="9"></a>[9]  Thanks, Theo.</p>

<p><i><B>Christopher Rooney</B> (&lt;http://crooney.com&gt;) is cofounder of Fistful of Digits (&lt;http://fodny.com&gt;), a software-consulting firm in New York City.</i></p>

<h4><a href="../../../source/2002/dec02/rooney.zip"></a></h4>
</blockquote></body></html>
