<HTML>   
     <HEAD>
<TITLE>October 2002 C++ Experts Forum/Generic&lt;Programming&gt;</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../../20.10/tococt.htm"></A><FONT COLOR="#FF0000">   C++ Experts Forum</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">Generic&lt;Programming&gt;: Efficient Generic Sorting and Searching in C++ (I): In Search of a Better Search</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Andrei Alexandrescu</FONT></H3>

<HR>
<BLOCKQUOTE>

<p>As I told a friend &quot;You know, I plan on writing an article on efficient sorting and searching in C++,&quot; he replied with badly dissimulated disappointment, &quot;What? Weren't these issues put to rest a long time ago?&quot;</p>
<p>And boy does he have a point. After all the seminal work of Hoare <a href="#1">[1]</a>, after Donald Knuth's fantastic work in <i>The Art of Computer Programming</i> <a href="#2">[2]</a>, after Stepanov and Musser's contribution (specifically applied to C++) <a href="#3">[3]</a>, after Plauger's refinements <a href="#4">[4]</a>, and after all the work done by so many other people on the subject of sorting and searching, it would appear that there's really nothing new to say on sorting and searching in general, or on generic incarnations thereof in C++.</p>
<p>The interest in the subject is not incidental. It is estimated that programs spend 80% of their time on searching (save for I/O). This means that applying improved methods for searching data and sorting it to make it easier to search is always welcome.</p>
<p>After five years of experience with implementing the standard library, you would think that every possible bit of efficiency has been squeezed out by today's world-class implementations of <b>std::sort</b>, <b>std::find</b>, <b>std::lower_bound</b>, <b>std::binary_search</b>, and such.</p>
<p>Not quite. This article offers some tips, accompanied by measurements, on improving generic sorting and searching in C++. Wishful thinking leads me to also consider it an open suggestion for the few standard library implementers out there. The numbers are quite interesting: under certain circumstances, you can get as much as <i>double</i> mileage over current implementations of the Standard. That, however, doesn't happen always and with all kinds of data. </p> 
<p>So if you are itching for faster sorting and searching, you are in the right place. If you enjoy generic programming, you are also in the right place. If you enjoy expanding your horizons with new ideas that don't have anything to do with sorting and searching, again you are in the right place. That covers pretty much everyone, so by now you should feel really compelled to read, and guilty not to. </p>
<p>Guess I could sell used cars.</p>

<H3><FONT COLOR="#000080">How to Send a Christmas Card to Yourself</FONT></H3>

<p>Let's start with the easiest routine of all: linear search.</p>
<p>In a popular British TV comic routine, a comedian sends himself a Christmas card. In the case that no one sends him a card, he at least has received one. </p>
<p>If you use an instant messaging program (i.e., Yahoo Messenger, MSN Messenger), you can add your own ID as a friend. Whenever you log on, you know there's at least one friend there online for you. (Yeah, I actually tried that.) Never feel lonely again!</p>
<p>How about applying the same principle to linear searching? You plant the element that you are looking for directly in the sequence to be searched. If there was none before, there is one now.</p>
<p>Why would one choose to do this? Let's review the classic generic linear search algorithm, the one that you can write with your toes while sleeping:</p>

<pre>
template &lt;class Iter, class ValueType&gt;
Iter find(Iter from, const Iter to, const ValueType&amp; val)
{
    for (; from != to &amp;&amp; !(*from == value); ++from) 
    {
    }
    return from;
}
</pre>

<p>The above code is identical to most of today's STL implementations of <b>std::find</b>. We will use it as a reference for evaluating alternative implementations. That code is as simple as it gets, partly because of the clever STL iteration style that we all know and love: ranges are closed to the left and open to the right &#151; <b>[begin, end)</b> &#151; so there are no contortions needed whatsoever. You iterate while the condition is met, and then you can tell how far you went.</p> 
<p>Given that things are so clear and simple, you ask how could it possibly be any better? Well, the answer is &#151; send yourself a Christmas card, so you'll at least receive <i>one</i>! Look at the condition. On each iteration, you test for two things: &quot;end of range&quot; and &quot;found.&quot; The &quot;end of range&quot; test means, eek, what if the whole range was spanned without finding anything? But if you know there is at least one, you don't have to test for end of range because you are confident you'll find at least one match. </p>
<p>If comparing iterators is not much cheaper than comparing values, then your savings can be significant. For example, on most machines, comparing two pointers costs exactly as much as comparing two integers. This means if you're searching for integers, you could slash the search time by two.</p> 
<p>So, an idea would be to plant the value you are looking for instead of the last element of the sequence. Save that last element of the sequence in a temporary. Search with only one comparison per iteration, because you are sure to never exhaust the whole range without finding at least one occurrence. When the search ends, you can sort things out.</p>
<p>We'll put all the sample code (also available for download) in namespace <b>Achilles</b> (named after the fast-running star of ancient Greece. In spite of his speed, ironically, he'd be unable to beat a turtle given only a 100-step handicap <a href="#5">[5]</a>). For completeness, there's even a function <b>Heel</b> in the <b>Achilles</b> namespace, so if you call <b>Achilles::Heel</b>, you know what to expect. Anyhow, <b>Achilles</b> is a bit of a misnomer. The code for this article comes straight from YASLI (Yet Another Standard Library Implementation), the legendary portable STL implementation that puts absolutely all other implementations on welfare by being overwhelmingly faster.</p>

<pre>
// Inside namespace Achilles, actually yasli
template &lt;class Iter, class ValueType&gt;
Iter mutative_find(Iter b, Iter e, const ValueType&amp; v)
{
    using std::swap;
    if (b == e) return e;
    std::iterator_traits&lt;Iter&gt;::value_type tmp(v);
    --e;
    swap(*e, tmp);
    for (; !(*b == v); ++b)
    {
    }
    swap(*e, tmp);
    if (b == e &amp;&amp; !(v == *b)) ++b;
    return b;
}
</pre>

<p>The name of the new routine is <b>mutative_find</b>, because it mutates the sequence being searched. Besides, &quot;mutative&quot; lends a Hollywood-scary-movie atmosphere to an, oh, so geeky article. </p>
<p>So here's how it all works: first off, empty sequences are handled as a degenerate case. Then, a temporary is created holding the value you're looking for and then swapped with the last element in the sequence. Effectively, the temporary holds the old last value, and the last value of the sequence holds the element you're looking for. Now you can search with only one test per iteration. When the search ends (necessarily), you have to swap back the last element in place (so you don't leave the sequence mutated). Finally, one more little test distinguishes between a false positive and a genuine match for the last element. That's it!</p>
<p>The routine above is predictably faster than the run-of-the-mill <b>find</b> implementation, but it also imposes quite a few more requirements on the types on which it operates. Let's enumerate them:</p>

<UL>
<LI>The iterators must be bi-directional. Decrementing <b>e</b> is instrumental to the algorithm's functioning. So, input and forward iterators are out of the question.</LI>
<LI>The sequence search must be mutable. Oops. That is, if you search through a range of constant elements (like, if you deal with <b>const_iterator</b>s), <b>Achilles::mutative_find</b> won't compile.</LI>
<LI>The type iterated by <b>Iter</b> must accept a non-throwing <b>swap</b>. If <b>swap</b> could throw, a very unpleasant thing might happen: before you search, you mutate the last element of the sequence, and after the search, as you try to put the toothpaste back in the tube, the last call to <b>swap</b> throws an exception. So, you end up with a failing function, which is pretty bad, and with unpredictably changed data (what caller would think searching through a range would change that range?), which is, put in mild terms, a catastrophe of epic proportions.</LI>
<LI>Compared to the reference <b>find</b> implementation, there's one more copy in sight. <b>Achilles</b>' routine makes a copy of the searched object so it can swap it with the last element of the sequence. That shouldn't be a problem in and of itself, but if the copy operation might throw an exception, <b>Achilles::mutative_find</b> is not Standard-compliant anymore. That's not as bad as the apocalyptic scenario above, because if <b>Achilles::mutative_find</b> fails, at least it didn't change your data. Still, it's easy to imagine situations in which you have data that you'd enjoy searching through, but that's extremely awkward to copy unwittingly. Think, for example, of searching for a <b>File</b> in a vector of <b>File</b>s, or a <b>DatabaseConnection</b> in a... you get my drift.</LI></UL>

<p>In conclusion, <b>Achilles::mutative_find</b> is fast, but not universal. On the other hand, you don't want to put everyone at a disadvantage for the sake of some. To exploit speed when possible, while still keeping generality, the algorithm we're looking at is:</p>

<pre>
if (<i>Iter is bidirectional_or_random</i> &amp;&amp; 
        <i>range is mutable</i> &amp;&amp; 
        <i>Iter's value type supports non-throwing swap</i> &amp;&amp;
        <i>Iter's value type's copy constructor doesn't throw</i>)
{
    ... cool, do it the fast way ...
}
else
{
    ... alright, do it the conservative way ...
}
</pre>

<p>Now, honestly, all that business with the non-throwing <b>swap</b> and the copy constructor that won't throw is just too vague to be convincing. Ladies and gentlemen, I'd like to make a motion to reduce this notion to the coarser, yet clearer, <i><b>Iter</b>'s value type must be a POD (Plain Old Data) type</i>. PODs don't throw at all, so creating temporaries is okay, plus the default <b>std::swap</b> will work for them just fine.</p>
<p>Seems like there are quite a few situations where <b>Achilles</b> isn't quite that fast, so we should be looking at ways to improve the &quot;conservative&quot; way.</p>

<H3><FONT COLOR="#000080">Duff Hits Again</FONT></H3>

<p>A past installment of &quot;Generic&lt;Programming&gt;&quot; <a href="#6">[6]</a> uses Duff's device to improve the speed of filling and copying memory, with excellent results. If you're not familiar with Duff's device, you may want to skim that article for a description. Now is the time to put the same device to work for linear searching. </p>

<pre>
// Inside namespace Achilles
template &lt;class Iter, class ValueType&lt;
Iter duff_find(Iter b, const Iter e, const ValueType&amp; v)
{
    switch ((e - b) &amp; 7u)
    {
    case 0:
        while (b != e)
        {
            if (*b == v) return b; ++b;
    case 7: if (*b == v) return b; ++b;
    case 6: if (*b == v) return b; ++b;
    case 5: if (*b == v) return b; ++b;
    case 4: if (*b == v) return b; ++b;
    case 3: if (*b == v) return b; ++b;
    case 2: if (*b == v) return b; ++b;
    case 1: if (*b == v) return b; ++b;
        }
    }
    return b;
}</pre>

<p>Duff's device is a loop unrolling technique that can be applied to a variety of tasks, including, well, linear searching. The initial <b>switch</b> statement places execution flow inside the loop at the exact point that will smoothly handle the termination condition.</p>
<p>From an efficiency viewpoint, <b>Achilles::duff_find</b> performs eight times fewer iterator comparisons than the reference <b>find</b> implementation. This places its efficiency somewhere in between <b>Achilles::mutative_find</b> (which does no repeated iterator comparison) and the reference <b>find</b>.</p>
<p>Let's analyze what strain <b>duff_find</b> puts on <b>Iter</b> and its corresponding value type. Very nicely, <b>duff_find</b> doesn't ask for extra copies, swaps, or any other tricks. However, the expression <b>e - b</b> reveals that the difference between two iterators must be computed presto &#151; so <b>Iter</b> must be random, period. </p> 
<p>Ultimately, this leaves us with the following outline of the best-of-the-breed <b>find</b> implementation:</p>

<pre>
if (<i>Iter is bidirectional_or_random</i> &amp;&amp; 
        <i>range is mutable</i> &amp;&amp; 
        <i>Iter's value type is a POD</i>)
{
    ... cool, do it the fast way ...
}
else
{
    if (<i>Iter is random</i>)
    {
        ... still cool, do it the Duff way ...
    }
    else
    {
        ... alright, do it the conservative way ...
    }
}
</pre>

<p>That's a definite improvement. I'm sure there are more techniques that would apply to other kinds of sequences, but this sounds like a great exercise for the reader. For now, let's see how to dispatch to one of the three algorithm implementations we now have.</p>

<H3><FONT COLOR="#000080">Choosing Algorithms at Compile Time</FONT></H3>

<p>Only a couple of years ago, this section on type manipulation would have been the meat of the article. &quot;Wow, you can detect that a type is <b>const</b>, how cool!&quot; Nowadays, however, there are known techniques and libraries that explain all the magic, so all we have to do is put it to work.</p>
<p>We detect <b>Iter</b>'s category by looking at <b>std::iterator_traits&lt;Iter&gt;::iterator_category</b>, which can be one of <b>std::input_iterator_tag</b>, <b>std::forward_iterator_tag</b>, <b>std::bidirectional_iterator_tag</b>, or <b>std::random_access_iterator_tag</b>. </p> 
<p>We could use Loki's type traits facility to play hide-and-seek with types <a href="#7">[7]</a>, but quite frankly, I prefer Boost's <a href="#8">[8]</a>. Boost's type traits are more complete and better designed. (So in correlation with the &quot;Not Invented Here&quot; syndrome, yours truly must be suffering from the &quot;Invented Here&quot; syndrome.) </p>
<p>With Boost's type traits, we can detect whether, for example, <b>Iter</b> is random by checking the compile-time constant:</p>

<pre>
boost::is_same&lt;
    std::iterator_traits&lt;Iter&gt;::iterator_category, 
    std::random_access_iterator_tag&gt;::value
</pre>

<p>To detect whether the sequence pointed to by <b>Iter</b> is mutable, we can look at <b>std::iterator_traits&lt;Iter&gt;::pointer_type</b>. If that <b>pointer_type</b> is <i>not</i> a pointer to <b>const</b>, then the sequence is mutable. We detect <b>const-</b>ness like this (say <b>PointerType</b> is the pointer type):</p>

<pre>
boost::is_const&lt;
    boost::remove_pointer&lt;PointerType&gt;::type&gt;::value
</pre>

<p>Finally, Boost also provides a mechanism for POD detection by writing:</p>

<pre>
boost::is_POD&lt;
    std::iterator_traits&lt;Iter&gt;::value_type&gt;::value
</pre>

<p>Now that we have all the information in Boolean form, we're halfway there. Simple <b>if</b> statements wouldn't work for selecting the appropriate algorithm, because the compiler will try to compile both branches of the <b>if</b> statement, even though one branch will never be executed. We need to use a different idiom, which uses templates combined with overloading, to select the appropriate algorithm. Refer to <a href="#9">[9]</a> for details. </p>
<p>In essence, what we'll do is compute a compile-time integral constant. That constant is <b>0</b> for the straight algorithm, <b>1</b> for the Duff algorithm, or <b>2</b> for the mutative algorithm. Then, we'll invoke the appropriate algorithm by using the <b>Loki::Int2Type</b> template class.</p> 
<p>So here's what the dispatcher looks like:</p>

<pre>
template &lt;class Iter, class ValueType&gt;
Iter find(Iter from, Iter to, const ValueType&amp; val)
{
    using namespace std;
    typedef iterator_traits&lt;Iter&gt;::iterator_category Category;
    enum { isBidir = boost::is_same&lt;
        Category, bidirectional_iterator_tag&gt;::value };
    enum { isRand = boost::is_same&lt;
        Category, random_access_iterator_tag&gt;::value };
    typedef iterator_traits&lt;Iter&gt;::pointer PointerType;
    typedef boost::remove_pointer&lt;PointerType&gt;::type
    IteratedType;
    enum { isMutableSeq = !boost::is_const&lt;
        IteratedType&gt;::value };
    typedef iterator_traits&lt;Iter&gt;::value_type ValueType;
    enum { isPod = boost::is_POD&lt;ValueType&gt;::value };
    enum { selector =
        (isBidir || isRand) &amp;&amp; isPod &amp;&amp; isMutableSeq ? 2 :
        (isRand ? 1 : 0) };
    Loki::Int2Type&lt;selector&gt; sel;
    return find_impl(from, to, val, sel);
}
</pre>

<p>I know that there's some clutter in there. But this is the way it is; funny how only the last line actually does something; all the rest just sets up the stage. </p>
<p>There are three overloads of <b>find_impl</b>. Here's a synopsis of them:</p>

<pre>
// Inside namespace Achilles
template &lt;class Iter, class ValueType&gt;
Iter find_impl(Iter from, const Iter to, const ValueType&amp; val,
    Loki::Int2Type&lt;0&gt;)
{
    ... conservative find implementation ...
}
template &lt;class Iter, class ValueType&gt;
Iter find_impl(Iter from, const Iter to, const ValueType&amp; val, 
    Loki::Int2Type&lt;1&gt;)
{
    ... Duff-based find implementation ...
}
template &lt;class Iter, class ValueType&gt;
Iter find_impl(Iter from, Iter to, const ValueType&amp; val, 
    Loki::Int2Type&lt;2&gt;)
{
    ... mutative find implementation ...
}
</pre>

<p>Well, so much about the implementation. On to the next step.</p>

<H3><FONT COLOR="#000080">Measurements</FONT></H3>

<p>Making good speed measurements is hard. Without much pretense, here are some results that I obtained with some simple test code. The code runs a repeated number of calls to <b>Achilles::find</b> or <b>std::find</b> over an array of integers and reports the average time spent per element searched.</p>

<table border=1 cellspacing=1 cellpadding=2>
 <tr>
  <td>Elements searched</td>
  <td colspan=2><b>std</b> (ns/element)</td>
  <td colspan=2><b>Achilles</b> (ns/element)</td>
  <td colspan=2>Improvement (%)</td>
 </tr>
 <tr>
  <td>&nbsp;</td>
  <td>MSVC</td>
  <td>MWCW</td>
  <td>MSVC</td>
  <td>MWCW</td>
  <td>MSVC</td>
  <td>MWCW</td>
 </tr>
 <tr>
  <td>128 <b>int</b></td>
  <td>1.640</td>
  <td>1.637</td>
  <td>0.910</td>
  <td>1.189</td>
  <td>80.22</td>
  <td>27.37</td>
 </tr>
 <tr>
  <td>512 </td>
  <td>1.570</td>
  <td>1.554</td>
  <td>0.753</td>
  <td>1.063</td>
  <td>108.50</td>
  <td>31.60</td>
 </tr>
 <tr>
  <td>2,048 </td>
  <td>1.550</td>
  <td>1.535</td>
  <td>0.715</td>
  <td>1.031</td>
  <td>116.78</td>
  <td>32.83</td>
 </tr>
 <tr>
  <td>8,192 </td>
  <td>1.548</td>
  <td>1.530</td>
  <td>0.755</td>
  <td>1.024</td>
  <td>105.03</td>
  <td>33.07</td>
 </tr>
 <tr>
  <td>32,768</td>
  <td>1.557</td>
  <td>1.536</td>
  <td>0.760</td>
  <td>1.024</td>
  <td>104.87</td>
  <td>33.33</td>
 </tr>
 <tr>
  <td>131,072</td>
  <td>1.662</td>
  <td>1.988</td>
  <td>0.885</td>
  <td>1.248</td>
  <td>87.80</td>
  <td>37.22</td>
 </tr>
 <tr>
  <td>524,288</td>
  <td>2.884</td>
  <td>2.874</td>
  <td>2.884</td>
  <td>2.874</td>
  <td>0.00</td>
  <td>0.00</td>
 </tr>
 <tr>
  <td>128 <b>const int</b></td>
  <td>1.640</td>
  <td>1.637</td>
  <td>1.234</td>
  <td>1.225</td>
  <td>32.90</td>
  <td>25.17</td>
 </tr>
 <tr>
  <td>512</td>
  <td>1.570</td>
  <td>1.554</td>
  <td>1.099</td>
  <td>1.084</td>
  <td>42.86</td>
  <td>30.24</td>
 </tr>
 <tr>
  <td>2,048 </td>
  <td>1.550</td>
  <td>1.535</td>
  <td>1.067</td>
  <td>1.060</td>
  <td>45.27</td>
  <td>30.94</td>
 </tr>
 <tr>
  <td>8,192 </td>
  <td>1.548</td>
  <td>1.530</td>
  <td>1.153</td>
  <td>1.157</td>
  <td>34.26</td>
  <td>24.38</td>
 </tr>
 <tr>
  <td>32,768</td>
  <td>1.557</td>
  <td>1.536</td>
  <td>1.158</td>
  <td>1.170</td>
  <td>34.46</td>
  <td>23.83</td>
 </tr>
 <tr>
  <td>131,072</td>
  <td>1.662</td>
  <td>1.988</td>
  <td>1.290</td>
  <td>1.472</td>
  <td>28.84</td>
  <td>25.96</td>
 </tr>
 <tr>
  <td>524,288</td>
  <td>2.884</td>
  <td>2.874</td>
  <td>2.880</td>
  <td>2.880</td>
  <td>0.14</td>
  <td>-0.21</td>
 </tr>
</table>

<p>The test was performed with two compilers: Microsoft Visual C++ .NET 7.1 alpha (&quot;For evaluation purpose only, not for production use&quot;) and Metrowerks CodeWarrior 7.0. The elements searched were <b>int</b>s (exercising the mutative algorithm) and <b>const int</b>s (exercising the implementation based on Duff's device).</p> 
<p>For large amounts of data, there is no improvement at all. This effect is discussed in <a href="#6">[6]</a>: when operating on very large ranges of memory, the speed is limited by the memory bandwidth &#151; not by the actual operations that are going on inside the processor. Your data is much bigger than your cache, so you end up hitting that memory bus hard. Because of the slow speed of that bus compared to the processor and the cache, large-block memory operations get more of an I/O feel. So if your task is to occasionally search over huge amounts of data, the techniques presented in this article won't help much.</p>
<p>However, for repeated searches on small and medium amounts of data, our effort wasn't in vain; the table shows consistent improvements of <b>Achilles</b> over <b>std</b> when searching integers. The savings, however, are not as significant for all types of data. Don't forget that what we save is iterator comparisons. If the value types are much more expensive to compare than the iterators, then the savings will be insignificant. If, however, you have fast value-type comparisons and/or costly iterator comparisons, then you'll enjoy considerable benefits. </p> 

<H3><FONT COLOR="#000080">Random Thoughts</FONT></H3>

<p>Was it worth it? The answer, as always, is &#151; it depends. If you are searching through a vector of matrices, you won't notice any improvement at all: the price of comparing matrices will eclipse the price of comparing iterators. Or, if you write code for a machine with limited program memory, the increased size of <b>Achilles::find</b> might play against you. </p>
<p>Wasn't it Donald Knuth who said <i>radix omnia malorum prematurae optimisatia est</i>? Well, I couldn't agree more. But when it comes to libraries, especially the standard library, I have a slightly different view.</p>
<p>Often, a library writer doesn't know exactly how that library will be used. The library might be used for mild tasks or in a speed-hungry context. While a fast library can be used for mellow stuff, a slow library can't be used in inner loops. My opinion in the matter is: bottom line, a library writer's duty is to offer as good a set of tradeoffs as possible.</p> 
<p>Such thoughts apply all the more when it comes to the <i>standard</i> library. The C++ Standard library is likely to be used by <i>all</i> C++ applications. In addition, the standard library often doesn't have to worry about portability either. It's expected to find in a world-class implementation of the standard library &quot;surprising&quot; little nuggets of magic that mere application programmers wouldn't have thought of. The standard library should be the ideal towards which we all enviously gravitate.</p>
<p>So far, only one <b>std</b> implementation passes that litmus test: the legendary YASLI.</p>
<p>But wait until the next column, which discusses generic sorting. We'll talk about a new algorithmic technique, and we'll implement together a sorting algorithm. We'll compare that with a couple of current <b>std::sort</b> implementations.</p>

<H3><FONT COLOR="#000080">Acknowledgments</FONT></H3>

<p>Many thanks are due to Florin Trofin, Dang-Thao Truong, and Attila Feher, who provided thorough reviews.</p>

<H3><FONT COLOR="#000080">Bibliography and Notes</FONT></H3>

<p><a name="1"></a>[1] C. A. R. Hoare. &quot;Quicksort,&quot; <i>Computer Journal</i>, 1962, 5, 10-15.</p>
<p><a name="2"></a>[2] D.E. Knuth. <i>The Art of Computer Programming, Vol. 3: Sorting and Searching</i> (Addison-Wesley Longman, 1998).</p>
<p><a name="3"></a>[3] D. Musser. <i>The Standard Template Library</i> (Rensselaer Polytechnic Institute, 1994).</p>
<p><a name="4"></a>[4] P.J. Plauger. &quot;Standard C/C++: A Better Sort,&quot; <i>C/C++ Users Journal</i>, October 1999.</p>
<p><a name="5"></a>[5] &lt;http://web.mit.edu/18.02-esg/www/notes/zeno.pdf&gt;</p>
<p><a name="6"></a>[6] A. Alexandrescu. &quot;Generic&lt;Programming&gt;: Typed Buffers (II),&quot; <i>C/C++ Users Journal C++ Experts Forum</i>, October 2001, &lt;www.cuj.com/experts/1910/alexandr.htm&gt;.</p>
<p><a name="7"></a>[7] &lt;www.moderncppdesign.com&gt;</p>
<p><a name="8"></a>[8] &lt;www.boost.org&gt;. Check the file <b>type_traits.h</b>.</p>
<p><a name="9"></a>[9] A. Alexandrescu. &quot;Generic&lt;Programming&gt;: Mappings between Types and Values,&quot; <i>C/C++ Users Journal C++ Experts Forum</i>, October 2000, &lt;www.cuj.com/experts/1810/alexandr.htm&gt;.</p>


<p><i><b>Andrei Alexandrescu</b> is a Ph.D. student at University of Washington in Seattle, and author of the acclaimed book </I>Modern C++ Design<I>. He may be contacted at <b>andrei@metalanguage.com</b>. Andrei is also one of the featured instructors of The C++ Seminar (&lt;http://thecppseminar.com&gt;).</i></p>

<h4><a href="../../../source/2002/oct02/alexandr.zip"></a></h4>


</BLOCKQUOTE></BODY></HTML>
